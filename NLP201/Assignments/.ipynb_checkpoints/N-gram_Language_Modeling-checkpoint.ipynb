{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec906e57-4d6f-4965-813d-6b9698ea01f2",
   "metadata": {},
   "source": [
    "# N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79eba2-a868-47b5-afda-8a50e5d1e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "print(os.getcwd())  # 這會顯示目前的工作目錄\n",
    "# 讀取數據\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "    return [line.strip().split() for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9621c11-f7b6-4866-8002-f111f5a787f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP201\\Assignments\n",
      "Unigram Model - Train Perplexity: 1696.1368477170352\n",
      "Unigram Model - Development Perplexity: 1720.9324942060468\n"
     ]
    }
   ],
   "source": [
    "# 構建 unigram 模型\n",
    "def build_unigram_model(data):\n",
    "    word_counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            word_counts[word] += 1\n",
    "            total_count += 1\n",
    "    # 計算機率\n",
    "    unigram_prob = {word: count / total_count for word, count in word_counts.items()}\n",
    "    return unigram_prob\n",
    "\n",
    "# 計算困惑度\n",
    "def calculate_perplexity(model, data):\n",
    "    perplexity = 0\n",
    "    total_words = 0\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            prob = model.get(word, model.get(\"<UNK>\", 1e-6))  # 使用 <UNK> 來處理未見詞\n",
    "            perplexity += -math.log2(prob)\n",
    "            total_words += 1\n",
    "    return 2 ** (perplexity / total_words)\n",
    "\n",
    "# 測試函數\n",
    "train_data = load_data('HW2/A2-Data/1b_benchmark.train.tokens')\n",
    "dev_data = load_data('HW2/A2-Data/1b_benchmark.dev.tokens')\n",
    "\n",
    "\n",
    "unigram_model = build_unigram_model(train_data)\n",
    "train_perplexity = calculate_perplexity(unigram_model, train_data)\n",
    "dev_perplexity = calculate_perplexity(unigram_model, dev_data)\n",
    "\n",
    "print(f\"Unigram Model - Train Perplexity: {train_perplexity}\")\n",
    "print(f\"Unigram Model - Development Perplexity: {dev_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda0ea16-9122-4838-ba52-a8b392b7c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model - Train Perplexity: 82.29197494110008\n",
      "Bigram Model - Development Perplexity: 1531.3391546889097\n"
     ]
    }
   ],
   "source": [
    "def build_bigram_model(data):\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    unigram_counts = defaultdict(int)\n",
    "    for sentence in data:\n",
    "        for i in range(1, len(sentence)):\n",
    "            unigram_counts[sentence[i - 1]] += 1\n",
    "            bigram_counts[sentence[i - 1]][sentence[i]] += 1\n",
    "        unigram_counts[sentence[-1]] += 1  # for the last word\n",
    "    # 計算條件機率\n",
    "    bigram_prob = {w1: {w2: count / unigram_counts[w1] for w2, count in w2_dict.items()} \n",
    "                   for w1, w2_dict in bigram_counts.items()}\n",
    "    return bigram_prob\n",
    "def calculate_bigram_perplexity(model, data):\n",
    "    perplexity = 0\n",
    "    total_words = 0\n",
    "    for sentence in data:\n",
    "        for i in range(1, len(sentence)):\n",
    "            prob = model.get(sentence[i - 1], {}).get(sentence[i], model.get(\"<UNK>\", 1e-6))\n",
    "            perplexity += -math.log2(prob)\n",
    "            total_words += 1\n",
    "    return 2 ** (perplexity / total_words)\n",
    "    \n",
    "bigram_model = build_bigram_model(train_data)\n",
    "bigram_train_perplexity = calculate_bigram_perplexity(bigram_model, train_data)\n",
    "bigram_dev_perplexity = calculate_bigram_perplexity(bigram_model, dev_data)\n",
    "\n",
    "print(f\"Bigram Model - Train Perplexity: {bigram_train_perplexity}\")\n",
    "print(f\"Bigram Model - Development Perplexity: {bigram_dev_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa97cbd-de48-40fc-85f7-e6e548961f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Model - Train Perplexity: 5.53204747614129\n",
      "Trigram Model - Development Perplexity: 42533.384050602035\n"
     ]
    }
   ],
   "source": [
    "def build_trigram_model(data):\n",
    "    trigram_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # 計算 trigram 和 bigram 的詞頻\n",
    "    for sentence in data:\n",
    "        if len(sentence) < 2:\n",
    "            continue  # 如果句子少於兩個詞，跳過該句\n",
    "        for i in range(2, len(sentence)):\n",
    "            bigram_counts[sentence[i - 2]][sentence[i - 1]] += 1\n",
    "            trigram_counts[sentence[i - 2]][sentence[i - 1]][sentence[i]] += 1\n",
    "        # 更新最後兩個單詞的 bigram 次數\n",
    "        bigram_counts[sentence[-2]][sentence[-1]] += 1 \n",
    "    \n",
    "    # 計算條件機率\n",
    "    trigram_prob = {w1: {w2: {w3: count / bigram_counts[w1][w2] \n",
    "                              for w3, count in w3_dict.items()} \n",
    "                         for w2, w3_dict in w2_dict.items()} \n",
    "                    for w1, w2_dict in trigram_counts.items()}\n",
    "    return trigram_prob\n",
    "\n",
    "    \n",
    "def calculate_trigram_perplexity(model, data):\n",
    "    perplexity = 0\n",
    "    total_words = 0\n",
    "    for sentence in data:\n",
    "        for i in range(2, len(sentence)):\n",
    "            # 取得三連詞的機率，如果不存在則使用 `<UNK>` 的低機率\n",
    "            prob = model.get(sentence[i - 2], {}).get(sentence[i - 1], {}).get(sentence[i], 1e-6)\n",
    "            perplexity += -math.log2(prob)\n",
    "            total_words += 1\n",
    "    return 2 ** (perplexity / total_words)\n",
    "    \n",
    "trigram_model = build_trigram_model(train_data)\n",
    "trigram_train_perplexity = calculate_trigram_perplexity(trigram_model, train_data)\n",
    "trigram_dev_perplexity = calculate_trigram_perplexity(trigram_model, dev_data)\n",
    "\n",
    "print(f\"Trigram Model - Train Perplexity: {trigram_train_perplexity}\")\n",
    "print(f\"Trigram Model - Development Perplexity: {trigram_dev_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b0c2e-f2fc-4936-959a-80e678f49ffc",
   "metadata": {},
   "source": [
    "# Smoothing with Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cadc988-39fb-4dce-9144-9084ccec8037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda values (λ1=0.3, λ2=0.3, λ3=0.4) - Train Perplexity: 11.393113747399937\n",
      "Lambda values (λ1=0.3, λ2=0.3, λ3=0.4) - Development Perplexity: 617.7934957112209\n",
      "Lambda values (λ1=0.1, λ2=0.3, λ3=0.6) - Train Perplexity: 8.129643791859909\n",
      "Lambda values (λ1=0.1, λ2=0.3, λ3=0.6) - Development Perplexity: 783.0750529916\n",
      "Lambda values (λ1=0.2, λ2=0.4, λ3=0.4) - Train Perplexity: 11.007429969369507\n",
      "Lambda values (λ1=0.2, λ2=0.4, λ3=0.4) - Development Perplexity: 623.2613123314627\n",
      "Lambda values (λ1=0.5, λ2=0.3, λ3=0.2) - Train Perplexity: 19.445779031405287\n",
      "Lambda values (λ1=0.5, λ2=0.3, λ3=0.2) - Development Perplexity: 580.2746066391761\n",
      "Lambda values (λ1=0.4, λ2=0.4, λ3=0.2) - Train Perplexity: 18.47001454471425\n",
      "Lambda values (λ1=0.4, λ2=0.4, λ3=0.2) - Development Perplexity: 560.8472677479118\n",
      "Best Lambda values: λ1=0.4, λ2=0.4, λ3=0.2\n",
      "Best Development Perplexity: 560.8472677479118\n"
     ]
    }
   ],
   "source": [
    "# 使用線性插值平滑計算困惑度\n",
    "def calculate_interpolated_perplexity(unigram_model, bigram_model, trigram_model, data, lambda1, lambda2, lambda3):\n",
    "    perplexity = 0\n",
    "    total_words = 0\n",
    "    for sentence in data:\n",
    "        for i in range(2, len(sentence)):\n",
    "            unigram_prob = unigram_model.get(sentence[i], unigram_model.get(\"<UNK>\", 1e-6))\n",
    "            bigram_prob = bigram_model.get(sentence[i - 1], {}).get(sentence[i], 1e-6)\n",
    "            trigram_prob = trigram_model.get(sentence[i - 2], {}).get(sentence[i - 1], {}).get(sentence[i], 1e-6)\n",
    "            \n",
    "            # 經過平滑後的機率\n",
    "            interpolated_prob = lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
    "            perplexity += -math.log2(interpolated_prob)\n",
    "            total_words += 1\n",
    "    return 2 ** (perplexity / total_words)\n",
    "\n",
    "# 定義不同的 lambda 組合來進行實驗\n",
    "lambda_combinations = [\n",
    "    (0.3, 0.3, 0.4),  # 作業指定的組合\n",
    "    (0.1, 0.3, 0.6),\n",
    "    (0.2, 0.4, 0.4),\n",
    "    (0.5, 0.3, 0.2),\n",
    "    (0.4, 0.4, 0.2)\n",
    "]\n",
    "\n",
    "\n",
    "# 儲存最佳困惑度和對應的 lambda 值\n",
    "best_dev_perplexity = float('inf')\n",
    "best_lambda_combination = None\n",
    "\n",
    "# 進行實驗，計算每組 lambda 值的困惑度\n",
    "for lambda1, lambda2, lambda3 in lambda_combinations:\n",
    "    train_perplexity = calculate_interpolated_perplexity(\n",
    "        unigram_model, bigram_model, trigram_model, train_data, lambda1, lambda2, lambda3\n",
    "    )\n",
    "    dev_perplexity = calculate_interpolated_perplexity(\n",
    "        unigram_model, bigram_model, trigram_model, dev_data, lambda1, lambda2, lambda3\n",
    "    )\n",
    "    print(f\"Lambda values (λ1={lambda1}, λ2={lambda2}, λ3={lambda3}) - Train Perplexity: {train_perplexity}\")\n",
    "    print(f\"Lambda values (λ1={lambda1}, λ2={lambda2}, λ3={lambda3}) - Development Perplexity: {dev_perplexity}\")\n",
    "    \n",
    "    # 更新最佳困惑度和 lambda 組合\n",
    "    if dev_perplexity < best_dev_perplexity:\n",
    "        best_dev_perplexity = dev_perplexity\n",
    "        best_lambda_combination = (lambda1, lambda2, lambda3)\n",
    "\n",
    "# 輸出最佳結果\n",
    "print(f\"Best Lambda values: λ1={best_lambda_combination[0]}, λ2={best_lambda_combination[1]}, λ3={best_lambda_combination[2]}\")\n",
    "print(f\"Best Development Perplexity: {best_dev_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577656f8-04d7-4658-9948-72306178223a",
   "metadata": {},
   "source": [
    "# Experiments with GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e9273-02f2-498f-870d-259e48b7c290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13b3ab-9f2a-4069-bac5-b548188861ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
