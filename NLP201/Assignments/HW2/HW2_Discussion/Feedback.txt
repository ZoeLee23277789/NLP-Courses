Feedback for Soren

    Your implementation of Unigram, Bigram, and Trigram models is logically sound, with a clear presentation of data preprocessing steps and the growth of n-gram counts. The detailed experiments, such as evaluating the impact of <UNK> thresholds and reduced training data, are commendable and add depth to the analysis. However, it would be helpful to provide more explanation regarding the selection of lambda values, including their impact on randomness in the data and the reasoning behind choosing these specific lambda combinations. This would further strengthen the theoretical foundation of your results.

Feedback for Sid

  Your experiments with reduced training data and <UNK> thresholds are well done; they provide valuable insights into the model's generalization capabilities. Your implementation of the Unigram, Bigram, and Trigram models appears correct, as the reported perplexities (658, 63.7, and 39.5, respectively) align with theoretical expectations. However, the smoothed test set perplexity of 10,794.57 is unusually high, indicating potential overfitting or issues in the interpolation formula. I recommend verifying the Î» values and ensuring they are optimized using the development set. Additionally, test with different training set sizes to confirm if data sparsity affects results. Lastly, double-check the implementation of the smoothing formula to ensure probabilities are combined correctly.  