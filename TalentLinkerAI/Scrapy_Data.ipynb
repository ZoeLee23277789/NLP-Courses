{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16355,"status":"ok","timestamp":1738275046968,"user":{"displayName":"李柔儀","userId":"09368646958181700978"},"user_tz":480},"id":"rrZKdEHWatFf","outputId":"a949b694-0868-48bb-e520-a460155ebf49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scrapy\n","  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n","Collecting Twisted\u003e=21.7.0 (from scrapy)\n","  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: cryptography\u003e=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n","Collecting cssselect\u003e=0.9.1 (from scrapy)\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n","Collecting itemloaders\u003e=1.0.1 (from scrapy)\n","  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n","Collecting parsel\u003e=1.5.0 (from scrapy)\n","  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: pyOpenSSL\u003e=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n","Collecting queuelib\u003e=1.4.2 (from scrapy)\n","  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n","Collecting service-identity\u003e=18.1.0 (from scrapy)\n","  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n","Collecting w3lib\u003e=1.17.0 (from scrapy)\n","  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting zope.interface\u003e=5.1.0 (from scrapy)\n","  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting protego\u003e=0.1.15 (from scrapy)\n","  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n","Collecting itemadapter\u003e=0.1.0 (from scrapy)\n","  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n","Collecting tldextract (from scrapy)\n","  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: lxml\u003e=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n","Requirement already satisfied: defusedxml\u003e=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n","Collecting PyDispatcher\u003e=2.0.5 (from scrapy)\n","  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography\u003e=37.0.0-\u003escrapy) (1.17.1)\n","Collecting jmespath\u003e=0.9.5 (from itemloaders\u003e=1.0.1-\u003escrapy)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: attrs\u003e=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (25.1.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (0.4.1)\n","Collecting automat\u003e=24.8.0 (from Twisted\u003e=21.7.0-\u003escrapy)\n","  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n","Collecting constantly\u003e=15.1 (from Twisted\u003e=21.7.0-\u003escrapy)\n","  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n","Collecting hyperlink\u003e=17.1.1 (from Twisted\u003e=21.7.0-\u003escrapy)\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting incremental\u003e=24.7.0 (from Twisted\u003e=21.7.0-\u003escrapy)\n","  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n","Requirement already satisfied: typing-extensions\u003e=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (4.12.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface\u003e=5.1.0-\u003escrapy) (75.1.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (3.10)\n","Requirement already satisfied: requests\u003e=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (2.32.3)\n","Collecting requests-file\u003e=1.4 (from tldextract-\u003escrapy)\n","  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: filelock\u003e=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (3.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi\u003e=1.12-\u003ecryptography\u003e=37.0.0-\u003escrapy) (2.22)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (3.4.1)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (2.3.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (2024.12.14)\n","Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n","Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n","Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n","Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n","Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n","Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n","Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n","Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n","Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n","Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n","Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n","Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n","Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n","Requirement already satisfied: twisted in /usr/local/lib/python3.11/dist-packages (24.11.0)\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (25.1.0)\n","Requirement already satisfied: automat\u003e=24.8.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (24.8.1)\n","Requirement already satisfied: constantly\u003e=15.1 in /usr/local/lib/python3.11/dist-packages (from twisted) (23.10.4)\n","Requirement already satisfied: hyperlink\u003e=17.1.1 in /usr/local/lib/python3.11/dist-packages (from twisted) (21.0.0)\n","Requirement already satisfied: incremental\u003e=24.7.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (24.7.2)\n","Requirement already satisfied: typing-extensions\u003e=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted) (4.12.2)\n","Requirement already satisfied: zope-interface\u003e=5 in /usr/local/lib/python3.11/dist-packages (from twisted) (7.2)\n","Requirement already satisfied: idna\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from hyperlink\u003e=17.1.1-\u003etwisted) (3.10)\n","Requirement already satisfied: setuptools\u003e=61.0 in /usr/local/lib/python3.11/dist-packages (from incremental\u003e=24.7.0-\u003etwisted) (75.1.0)\n","Requirement already satisfied: service_identity in /usr/local/lib/python3.11/dist-packages (24.2.0)\n","Requirement already satisfied: attrs\u003e=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service_identity) (25.1.0)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from service_identity) (43.0.3)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service_identity) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service_identity) (0.4.1)\n","Requirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography-\u003eservice_identity) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi\u003e=1.12-\u003ecryptography-\u003eservice_identity) (2.22)\n","Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (2.3.1)\n","Requirement already satisfied: scrapy in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Collecting scrapy-user-agents\n","  Downloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: Twisted\u003e=21.7.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.11.0)\n","Requirement already satisfied: cryptography\u003e=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n","Requirement already satisfied: cssselect\u003e=0.9.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.2.0)\n","Requirement already satisfied: itemloaders\u003e=1.0.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.3.2)\n","Requirement already satisfied: parsel\u003e=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.10.0)\n","Requirement already satisfied: pyOpenSSL\u003e=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n","Requirement already satisfied: queuelib\u003e=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scrapy) (1.7.0)\n","Requirement already satisfied: service-identity\u003e=18.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.0)\n","Requirement already satisfied: w3lib\u003e=1.17.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.3.1)\n","Requirement already satisfied: zope.interface\u003e=5.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (7.2)\n","Requirement already satisfied: protego\u003e=0.1.15 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.4.0)\n","Requirement already satisfied: itemadapter\u003e=0.1.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.1.3)\n","Requirement already satisfied: lxml\u003e=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n","Requirement already satisfied: defusedxml\u003e=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n","Requirement already satisfied: PyDispatcher\u003e=2.0.5 in /usr/local/lib/python3.11/dist-packages (from scrapy) (2.0.7)\n","Collecting user-agents (from scrapy-user-agents)\n","  Downloading user_agents-2.2.0-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography\u003e=37.0.0-\u003escrapy) (1.17.1)\n","Requirement already satisfied: jmespath\u003e=0.9.5 in /usr/local/lib/python3.11/dist-packages (from itemloaders\u003e=1.0.1-\u003escrapy) (1.0.1)\n","Requirement already satisfied: attrs\u003e=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (25.1.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity\u003e=18.1.0-\u003escrapy) (0.4.1)\n","Requirement already satisfied: automat\u003e=24.8.0 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (24.8.1)\n","Requirement already satisfied: constantly\u003e=15.1 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (23.10.4)\n","Requirement already satisfied: hyperlink\u003e=17.1.1 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (21.0.0)\n","Requirement already satisfied: incremental\u003e=24.7.0 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (24.7.2)\n","Requirement already satisfied: typing-extensions\u003e=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted\u003e=21.7.0-\u003escrapy) (4.12.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface\u003e=5.1.0-\u003escrapy) (75.1.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (3.10)\n","Requirement already satisfied: requests\u003e=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (2.32.3)\n","Requirement already satisfied: requests-file\u003e=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (2.1.0)\n","Requirement already satisfied: filelock\u003e=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract-\u003escrapy) (3.17.0)\n","Collecting ua-parser\u003e=0.10.0 (from user-agents-\u003escrapy-user-agents)\n","  Downloading ua_parser-1.0.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi\u003e=1.12-\u003ecryptography\u003e=37.0.0-\u003escrapy) (2.22)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (3.4.1)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (2.3.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.1.0-\u003etldextract-\u003escrapy) (2024.12.14)\n","Collecting ua-parser-builtins (from ua-parser\u003e=0.10.0-\u003euser-agents-\u003escrapy-user-agents)\n","  Downloading ua_parser_builtins-0.18.0.post1-py3-none-any.whl.metadata (1.4 kB)\n","Downloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl (27 kB)\n","Downloading user_agents-2.2.0-py3-none-any.whl (9.6 kB)\n","Downloading ua_parser-1.0.0-py3-none-any.whl (31 kB)\n","Downloading ua_parser_builtins-0.18.0.post1-py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ua-parser-builtins, ua-parser, user-agents, scrapy-user-agents\n","Successfully installed scrapy-user-agents-0.1.1 ua-parser-1.0.0 ua-parser-builtins-0.18.0.post1 user-agents-2.2.0\n"]}],"source":["!pip install scrapy\n","!pip install twisted\n","!pip install service_identity\n","!pip install w3lib\n","!pip install scrapy scrapy-user-agents\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af7WX8CTaym6"},"outputs":[],"source":["# import scrapy\n","# from scrapy.crawler import CrawlerProcess\n","# import pandas as pd\n","\n","\n","# class ResumeSpider(scrapy.Spider):\n","#     name = 'resumes'\n","#     allowed_domains = ['hireitpeople.com']\n","#     start_urls = ['https://www.hireitpeople.com/resume-database/']\n","\n","#     custom_settings = {\n","#         'FEEDS': {\n","#             'resumes.json': {'format': 'json'},\n","#         }\n","#     }\n","\n","#     def parse(self, response):\n","#         # 提取主分類的名稱和連結\n","#         rows = response.css('table.hit-table tr')\n","#         for row in rows:\n","#             name = row.css('h4 a::text').get()\n","#             link = row.css('h4 a::attr(href)').get()\n","\n","#             if name and link:\n","#                 # 跟隨分類連結，處理分頁和子連結\n","#                 yield response.follow(link, self.parse_category, meta={\n","#                     'category_name': name.strip(),\n","#                     'category_link': response.urljoin(link),\n","#                 })\n","\n","#     def parse_category(self, response):\n","#         # 抓取每個分頁中的所有子連結\n","#         category_name = response.meta['category_name']\n","#         category_link = response.meta['category_link']\n","\n","#         sublinks = response.css('table.hit-table h4 a')\n","#         for sublink in sublinks:\n","#             sub_name = sublink.css('::text').get()\n","#             sub_link = sublink.css('::attr(href)').get()\n","\n","#             yield {\n","#                 'Category': category_name,\n","#                 'Category Link': category_link,\n","#                 'Resume Name': sub_name.strip() if sub_name else 'N/A',\n","#                 'Resume Link': response.urljoin(sub_link) if sub_link else 'N/A'\n","#             }\n","\n","#         # 正確處理分頁邏輯\n","#         next_page = response.css('ul.pagination-custom li a::attr(href)').getall()\n","#         for page_link in next_page:\n","#             if \"page\" in page_link:  # 確保是分頁連結\n","#                 next_page_url = response.urljoin(page_link)\n","#                 self.logger.info(f\"Following next page: {next_page_url}\")  # Log 下一頁\n","#                 yield response.follow(next_page_url, self.parse_category, meta={\n","#                     'category_name': category_name,\n","#                     'category_link': category_link,\n","#                 })\n","\n","\n","# # 啟動 Scrapy\n","# if __name__ == '__main__':\n","#     process = CrawlerProcess()\n","#     process.crawl(ResumeSpider)\n","#     process.start()\n","\n","#     # 將結果轉換為 DataFrame 並保存為 CSV\n","#     df = pd.read_json('resumes.json')\n","#     df.to_csv('resumes.csv', index=False)\n","#     print(df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1kRWqVMMjGC8PD1VqvwA24dQcNhbONMlY"},"id":"VdFNS7fZDlHu","outputId":"f91af7c0-cc3e-4eb4-a709-eddc4d90f3b8"},"outputs":[],"source":["import scrapy\n","from scrapy.crawler import CrawlerProcess\n","import pandas as pd\n","\n","\n","class ResumeSpider(scrapy.Spider):\n","    name = 'resumes'\n","    allowed_domains = ['hireitpeople.com']\n","    start_urls = ['https://www.hireitpeople.com/resume-database/']\n","\n","    custom_settings = {\n","        'FEEDS': {\n","            'resumes_details.json': {'format': 'json'},\n","        },\n","        'DOWNLOAD_DELAY': 2,  # 延遲 2 秒\n","        'CONCURRENT_REQUESTS': 1,  # 降低並發數量\n","        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n","        'AUTOTHROTTLE_ENABLED': True,\n","        'AUTOTHROTTLE_START_DELAY': 1,  # 初始延遲 1 秒\n","        'AUTOTHROTTLE_MAX_DELAY': 5,  # 最大延遲 5 秒\n","        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,  # 目標並發為 1\n","        'RETRY_HTTP_CODES': [429],  # 重試 429 狀態碼\n","        'RETRY_TIMES': 5,  # 最大重試次數\n","        'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n","        'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,  # 隨機 User-Agent\n","    }\n","\n","\n","    def parse(self, response):\n","        # 提取主分類的名稱和連結\n","        rows = response.css('table.hit-table tr')\n","        for row in rows:\n","            name = row.css('h4 a::text').get()\n","            link = row.css('h4 a::attr(href)').get()\n","\n","            if name and link:\n","                # 跟隨分類連結，處理分頁和子連結\n","                yield response.follow(link, self.parse_category, meta={\n","                    'category_name': name.strip(),\n","                    'category_link': response.urljoin(link),\n","                    'page_number': 1  # 初始化頁碼\n","                })\n","\n","    def parse_category(self, response):\n","        # 抓取每個分頁中的所有子連結\n","        category_name = response.meta['category_name']\n","        category_link = response.meta['category_link']\n","        page_number = response.meta['page_number']\n","\n","        # 抓取當前頁面的所有子連結\n","        sublinks = response.css('table.hit-table h4 a')\n","        for sublink in sublinks:\n","            sub_name = sublink.css('::text').get()\n","            sub_link = sublink.css('::attr(href)').get()\n","\n","            # 跟隨子連結，提取詳細資料\n","            if sub_link:\n","                yield response.follow(sub_link, self.parse_resume_detail, meta={\n","                    'category_name': category_name,\n","                    'category_link': category_link,\n","                    'resume_name': sub_name.strip() if sub_name else 'N/A',\n","                    'resume_link': response.urljoin(sub_link),\n","                })\n","\n","        # 處理分頁邏輯，僅抓取前兩頁\n","        if page_number \u003c 2:  # 限制爬取頁數\n","            next_page = response.css('ul.pagination-custom li a::attr(href)').getall()\n","            for page_link in next_page:\n","                if \"page\" in page_link:\n","                    next_page_url = response.urljoin(page_link)\n","                    yield response.follow(next_page_url, self.parse_category, meta={\n","                        'category_name': category_name,\n","                        'category_link': category_link,\n","                        'page_number': page_number + 1  # 增加頁碼\n","                    })\n","                    break  # 僅選擇第一個下一頁連結\n","\n","    def parse_resume_detail(self, response):\n","        # 提取詳細資料\n","        category_name = response.meta['category_name']\n","        category_link = response.meta['category_link']\n","        resume_name = response.meta['resume_name']\n","        resume_link = response.meta['resume_link']\n","\n","        # 提取 `div.cell-sm-9` 中的所有文字內容\n","        detailed_info = response.css('div.cell-sm-9 *::text').getall()\n","        detailed_info_cleaned = \" \".join([text.strip() for text in detailed_info if text.strip()])\n","\n","        yield {\n","            'Category': category_name,\n","            'Category Link': category_link,\n","            'Resume Name': resume_name,\n","            'Resume Link': resume_link,\n","            'Detailed Info': detailed_info_cleaned if detailed_info_cleaned else 'N/A',\n","        }\n","\n","\n","# 啟動 Scrapy\n","if __name__ == '__main__':\n","    process = CrawlerProcess()\n","    process.crawl(ResumeSpider)\n","    process.start()\n","\n","    # 將結果轉換為 DataFrame 並保存為 CSV\n","    df = pd.read_json('resumes_details.json')\n","    df.to_csv('resumes_details.csv', index=False)\n","    print(df.head())\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMg3WKJgDXS1bCUZenJSE49","gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}