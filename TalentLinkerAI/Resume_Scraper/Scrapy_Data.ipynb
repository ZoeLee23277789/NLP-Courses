{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrZKdEHWatFf",
    "outputId": "51bb6b97-f7ba-4bd5-b864-a97f992e8327",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (44.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (1.9.1)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (24.3.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (2.2.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (0.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (24.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (5.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.3.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: automat>=24.8.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from tldextract->scrapy) (3.16.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.12.14)\n",
      "Requirement already satisfied: twisted in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (24.11.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (24.3.0)\n",
      "Requirement already satisfied: automat>=24.8.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (4.12.2)\n",
      "Requirement already satisfied: zope-interface>=5 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from twisted) (7.2)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from hyperlink>=17.1.1->twisted) (3.10)\n",
      "Requirement already satisfied: setuptools>=61.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from incremental>=24.7.0->twisted) (75.1.0)\n",
      "Requirement already satisfied: tomli in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from incremental>=24.7.0->twisted) (2.2.1)\n",
      "Requirement already satisfied: service_identity in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (24.2.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service_identity) (24.3.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service_identity) (44.0.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service_identity) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from service_identity) (0.4.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from cryptography->service_identity) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (from cffi>=1.12->cryptography->service_identity) (2.22)\n",
      "Requirement already satisfied: w3lib in c:\\users\\user\\anaconda3\\envs\\scapy\\lib\\site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install twisted\n",
    "!pip install service_identity\n",
    "!pip install w3lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "af7WX8CTaym6",
    "outputId": "a57d5824-1cd6-4777-b13b-d6550dea53ce",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerProcess\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ResumeSpider(scrapy.Spider):\n",
    "    name = 'resumes'\n",
    "    allowed_domains = ['hireitpeople.com']\n",
    "    start_urls = ['https://www.hireitpeople.com/resume-database/']\n",
    "\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'resumes.json': {'format': 'json'},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 提取主分類的名稱和連結\n",
    "        rows = response.css('table.hit-table tr')\n",
    "        for row in rows:\n",
    "            name = row.css('h4 a::text').get()\n",
    "            link = row.css('h4 a::attr(href)').get()\n",
    "\n",
    "            if name and link:\n",
    "                # 跟隨分類連結，處理分頁和子連結\n",
    "                yield response.follow(link, self.parse_category, meta={\n",
    "                    'category_name': name.strip(),\n",
    "                    'category_link': response.urljoin(link),\n",
    "                })\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        # 抓取每個分頁中的所有子連結\n",
    "        category_name = response.meta['category_name']\n",
    "        category_link = response.meta['category_link']\n",
    "\n",
    "        sublinks = response.css('table.hit-table h4 a')\n",
    "        for sublink in sublinks:\n",
    "            sub_name = sublink.css('::text').get()\n",
    "            sub_link = sublink.css('::attr(href)').get()\n",
    "\n",
    "            yield {\n",
    "                'Category': category_name,\n",
    "                'Category Link': category_link,\n",
    "                'Resume Name': sub_name.strip() if sub_name else 'N/A',\n",
    "                'Resume Link': response.urljoin(sub_link) if sub_link else 'N/A'\n",
    "            }\n",
    "\n",
    "        # 正確處理分頁邏輯\n",
    "        next_page = response.css('ul.pagination-custom li a::attr(href)').getall()\n",
    "        for page_link in next_page:\n",
    "            if \"page\" in page_link:  # 確保是分頁連結\n",
    "                next_page_url = response.urljoin(page_link)\n",
    "                self.logger.info(f\"Following next page: {next_page_url}\")  # Log 下一頁\n",
    "                yield response.follow(next_page_url, self.parse_category, meta={\n",
    "                    'category_name': category_name,\n",
    "                    'category_link': category_link,\n",
    "                })\n",
    "\n",
    "\n",
    "# 啟動 Scrapy\n",
    "if __name__ == '__main__':\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(ResumeSpider)\n",
    "    process.start()\n",
    "\n",
    "    # 將結果轉換為 DataFrame 並保存為 CSV\n",
    "    df = pd.read_json('resumes.json')\n",
    "    df.to_csv('resumes.csv', index=False)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
