{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e785273-c8e6-4d0b-9b1e-2c3dfe340872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "# from nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # # Load the dataset\n",
    "# # file_path = './semeval-2017-train.csv'\n",
    "# # data = pd.read_csv(file_path, sep='\\t', header=None, names=['label_text'], quoting=3)\n",
    "# # # Reload the dataset with tab-separated values\n",
    "# file_path = './semeval-2017-train.csv'\n",
    "# data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "\n",
    "# # Display the first few rows to verify the splitting\n",
    "# data.head()\n",
    "\n",
    "# # Inspect and clean data with proper splitting\n",
    "# def clean_and_split(row):\n",
    "#     parts = row.split('\\t', 1)  # Split into label and text\n",
    "#     if len(parts) == 2:\n",
    "#         return parts\n",
    "#     return [None, None]  # Handle rows that don't split cleanly\n",
    "\n",
    "# data[['label', 'text']] = data['label_text'].apply(clean_and_split).apply(pd.Series)\n",
    "\n",
    "# # Drop rows where label or text is missing\n",
    "# data = data.dropna()\n",
    "# print(data)\n",
    "# # Convert label to integer, ignore invalid entries\n",
    "# try:\n",
    "#     data['label'] = data['label'].astype(int)\n",
    "# except ValueError as e:\n",
    "#     print(f\"Error converting labels to integers: {e}\")\n",
    "\n",
    "# # # Separate positive and negative samples\n",
    "# # positive_samples = data[data['label'] > 0]['text'].tolist()\n",
    "# # negative_samples = data[data['label'] < 0]['text'].tolist()\n",
    "\n",
    "# # # Tokenize text samples\n",
    "# # positive_tokens = [word_tokenize(text.lower()) for text in positive_samples]\n",
    "# # negative_tokens = [word_tokenize(text.lower()) for text in negative_samples]\n",
    "\n",
    "# # # Flatten token lists\n",
    "# # positive_tokens_flat = [token for sublist in positive_tokens for token in sublist]\n",
    "# # negative_tokens_flat = [token for sublist in negative_tokens for token in sublist]\n",
    "\n",
    "# # # Find significant collocations using PMI\n",
    "# # bigram_measures = BigramAssocMeasures()\n",
    "# # trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# # # Positive sentiment\n",
    "# # positive_bigram_finder = BigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "# # positive_trigram_finder = TrigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "\n",
    "# # positive_bigrams = positive_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "# # positive_trigrams = positive_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# # # Negative sentiment\n",
    "# # negative_bigram_finder = BigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "# # negative_trigram_finder = TrigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "\n",
    "# # negative_bigrams = negative_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "# # negative_trigrams = negative_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# # # Output results\n",
    "# # print(\"Top-10 Positive Sentiment Bigrams:\", positive_bigrams)\n",
    "# # print(\"Top-10 Positive Sentiment Trigrams:\", positive_trigrams)\n",
    "# # print(\"Top-10 Negative Sentiment Bigrams:\", negative_bigrams)\n",
    "# # print(\"Top-10 Negative Sentiment Trigrams:\", negative_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47a60c42-94d9-4d57-ac6e-bf951d269723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>One Night like In Vegas I make dat Nigga Famous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Walking through Chelsea at this time of day is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>And on the very first play of the night, Aaron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Drove the bike today, about 40 miles. Felt lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>looking at the temp outside....hpw did it get ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1    One Night like In Vegas I make dat Nigga Famous\n",
       "1      1  Walking through Chelsea at this time of day is...\n",
       "2      0  And on the very first play of the night, Aaron...\n",
       "3      0  Drove the bike today, about 40 miles. Felt lik...\n",
       "4     -1  looking at the temp outside....hpw did it get ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the dataset with tab-separated values\n",
    "file_path = './semeval-2017-train.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "\n",
    "# Display the first few rows to verify the splitting\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bf2518a-bf4b-4c3e-baf1-3421f3f46f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Top-10 Positive Sentiment Bigrams: [(\"'goodbye\", 'leeteuk'), (\"'in\", 'desolate'), (\"'merica\", '-obama'), (\"'percy\", 'porker'), (',2011', 'gmwa'), ('-bobby', 'kimball'), ('-plan', 'rosy'), ('...............', 'beacause'), ('.3', 'slower'), ('//t.co/000vvuis', 'postters')]\n",
      "------------------------------------------------------\n",
      "Top-10 Positive Sentiment Trigrams: [('//t.co/88zz0t7tui', 'arnold', 'schwarzenegger'), ('//t.co/guz69pkm', 'ooshma', 'garg'), ('//t.co/hokhyievoz', 'simultaneously', 'exhilarated'), ('//t.co/j4kauvch', 'chiara', 'atik'), ('//t.co/suickldnlz', 'sportsnet', '960'), ('//t.co/vkkiryt0', 'joaquim', 'rodriquez'), ('//t.co/vn7wyrzx', 'rasheed', 'wallace'), ('//t.co/wwrjxvlv', 'karina', 'smirnoff'), ('//t.co/xdgvxemp7b', 'hisashi', 'iwakuma'), ('//t.co/zhfu3g0j', 'bynum', 'hollywoodcrush')]\n",
      "------------------------------------------------------\n",
      "Top-10 Negative Sentiment Bigrams: [(\"'ba\", 'turenci'), (\"'boston\", 'globe'), (\"'fruitcakes\", 'loonies'), (\"'is\", \"'blows\"), (\"'organ\", 'harvesting'), (\"'paul\", 'haggis'), (\"'shamed\", 'starlet'), ('+border', 'terrier'), ('-mike', 'shannon'), ('-miss', 'adu')]\n",
      "------------------------------------------------------\n",
      "Top-10 Negative Sentiment Trigrams: [('//october-ppropriate', 'oingo', 'boingo'), ('//t.co/14hqjmmq', '|infosecisland', 'sup'), ('//t.co/5shjoyrn', 'sherman', 'hemsley'), ('//t.co/bfmpwdcerd', 'ceasefire', 'violations'), ('//t.co/eyzxz0px54', 'anup', 'surendranath'), ('//t.co/kjsihuo1v5', 'leftist', 'communists'), ('//t.co/r8lhb4e2zp', \"'is\", \"'blows\"), ('//t.co/saub0siinw', 'elbert', 'guillory'), ('//t.co/wwzxajar7c', 'bethany', 'cosentino'), ('10th\\\\u002c', '2-2', 'count\\\\u002c')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 下載 NLTK 資源（如果尚未下載）\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 加載數據\n",
    "file_path = './semeval-2017-train.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "data.head()\n",
    "# 分離正面和負面樣本\n",
    "positive_samples = data[data['label'] > 0]['text'].tolist()\n",
    "negative_samples = data[data['label'] < 0]['text'].tolist()\n",
    "\n",
    "# Tokenize 文本樣本\n",
    "positive_tokens = [word_tokenize(text.lower()) for text in positive_samples]\n",
    "negative_tokens = [word_tokenize(text.lower()) for text in negative_samples]\n",
    "\n",
    "# 展平 tokens 列表（將所有樣本的 token 合併為一個列表）\n",
    "positive_tokens_flat = [token for sublist in positive_tokens for token in sublist]\n",
    "negative_tokens_flat = [token for sublist in negative_tokens for token in sublist]\n",
    "\n",
    "# 使用 NLTK 的 Collocation Finder 找出 bi-grams 和 tri-grams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# 正面情緒 bi-grams 和 tri-grams\n",
    "positive_bigram_finder = BigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "positive_trigram_finder = TrigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "\n",
    "positive_bigrams = positive_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "positive_trigrams = positive_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 負面情緒 bi-grams 和 tri-grams\n",
    "negative_bigram_finder = BigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "negative_trigram_finder = TrigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "\n",
    "negative_bigrams = negative_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "negative_trigrams = negative_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Top-10 Positive Sentiment Bigrams:\", positive_bigrams)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Top-10 Positive Sentiment Trigrams:\", positive_trigrams)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Top-10 Negative Sentiment Bigrams:\", negative_bigrams)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Top-10 Negative Sentiment Trigrams:\", negative_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c3c176e-db18-45ab-92bc-88900167cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Positive Sentiment Bigrams: [('0048', 'taiul'), ('01924', '290870'), ('01943', '609539'), ('090615', 'krakatoa'), ('1000s', 'queuing'), ('1005chunjiday', 'be_ggdeeryoona'), ('1007', 'cute_af'), ('100u002c000', 'reward'), ('10307pm', '26715'), ('1049virginradioca', 'imachristianbut')]\n",
      "Top-10 Positive Sentiment Trigrams: [('13h', 'capitalpride', 'ottawapride'), ('1621', 'camelsu002c', 'souksu002c'), ('1800', 'gmt1', 'betis'), ('1970u2019s', 'showroom', 'workstation'), ('1dliveupdates', 'chrisashton1', 'guttedtakin'), ('1drewhutch', 'joeybats19', 'successshawn'), ('1hazard', '2suarez', '3messi'), ('1stthen', 'jetsthen', 'eaglesget'), ('2117', 'ienever', 'nascars'), ('3045293647', 'kenova', '3044533647')]\n",
      "Top-10 Negative Sentiment Bigrams: [('062343', 'gmtegy'), ('100000000', 'bstewart84'), ('10reboundu002c', '5assist'), ('110212fri005239', 'no434329650'), ('1176', 'bbcsp'), ('11bil', 'sonotfair'), ('121pm', 'mst'), ('18009365700', 'adrian'), ('1901', 'bayford'), ('1970silver', 'plumeu002cco')]\n",
      "Top-10 Negative Sentiment Trigrams: [('1901', 'bayford', 'oyster'), ('1steveburton', 'jasonmorgan', 'stonecold'), ('2yrs', 'elder', 'den'), ('30pointu002c', '10reboundu002c', '5assist'), ('930', 'etherealx_', 'monbrielle'), ('_whiteponyjr_', 'fugazi3011', 'bensotokarass'), ('abbottag', 'ken', 'paxton'), ('abcu002c', 'cbsu002cnbcu002c', 'cnnu002c'), ('acairan', 'dealgay', 'marriagecuban'), ('adamrubinespn', 'msimonespn', 'ynscspds')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 下載 NLTK 資源（如果尚未下載）\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 加載數據\n",
    "file_path = './semeval-2017-train.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "\n",
    "# 清理文本數據\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # 移除URL\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除標點符號\n",
    "    text = text.lower()  # 轉為小寫\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# 分離正面和負面樣本\n",
    "positive_samples = data[data['label'] > 0]['text'].tolist()\n",
    "negative_samples = data[data['label'] < 0]['text'].tolist()\n",
    "\n",
    "# Tokenize 文本樣本\n",
    "positive_tokens = [word_tokenize(text) for text in positive_samples]\n",
    "negative_tokens = [word_tokenize(text) for text in negative_samples]\n",
    "\n",
    "# 展平 tokens 列表（將所有樣本的 token 合併為一個列表）\n",
    "positive_tokens_flat = [token for sublist in positive_tokens for token in sublist]\n",
    "negative_tokens_flat = [token for sublist in negative_tokens for token in sublist]\n",
    "\n",
    "# 使用 NLTK 的 Collocation Finder 找出 bi-grams 和 tri-grams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# 正面情緒 bi-grams 和 tri-grams\n",
    "positive_bigram_finder = BigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "positive_trigram_finder = TrigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "\n",
    "positive_bigrams = positive_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "positive_trigrams = positive_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 負面情緒 bi-grams 和 tri-grams\n",
    "negative_bigram_finder = BigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "negative_trigram_finder = TrigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "\n",
    "negative_bigrams = negative_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "negative_trigrams = negative_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"Top-10 Positive Sentiment Bigrams:\", positive_bigrams)\n",
    "print(\"Top-10 Positive Sentiment Trigrams:\", positive_trigrams)\n",
    "print(\"Top-10 Negative Sentiment Bigrams:\", negative_bigrams)\n",
    "print(\"Top-10 Negative Sentiment Trigrams:\", negative_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "154ff7fd-9d66-4184-95e5-2a74b14be0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Positive Sentiment Bigrams: [('1000s', 'queuing'), ('1005chunjiday', 'be_ggdeeryoona'), ('100u002c000', 'reward'), ('1049virginradioca', 'imachristianbut'), ('10am4pmorlandofarmersmarket', 'mia_noel'), ('10nov', 'quozhappens'), ('1112th', 'gavans'), ('1114u002c', 'jmcurleybar'), ('11speedcassette', 'kerryvdublin'), ('128gb', 'variants')]\n",
      "Top-10 Positive Sentiment Trigrams: [('13h', 'capitalpride', 'ottawapride'), ('1970u2019s', 'showroom', 'workstation'), ('1dliveupdates', 'chrisashton1', 'guttedtakin'), ('1drewhutch', 'joeybats19', 'successshawn'), ('1hazard', '2suarez', '3messi'), ('1stthen', 'jetsthen', 'eaglesget'), ('3overpaints', 'withyoung', 'caterina'), ('456th', 'squadron', 'b24u002c'), ('4year', '38m', 'extension'), ('6th13th', 'cr', 'mamiself')]\n",
      "Top-10 Negative Sentiment Bigrams: [('10reboundu002c', '5assist'), ('10thu002c', 'countu002c'), ('110212fri005239', 'no434329650'), ('11bil', 'sonotfair'), ('121pm', 'mst'), ('1970silver', 'plumeu002cco'), ('1971third', 'udr'), ('1n', 'dustin_longnj'), ('1steveburton', 'jasonmorgan'), ('215sfreshprince', 'mirskiimir')]\n",
      "Top-10 Negative Sentiment Trigrams: [('1steveburton', 'jasonmorgan', 'stonecold'), ('2yrs', 'elder', 'den'), ('30pointu002c', '10reboundu002c', '5assist'), ('_whiteponyjr_', 'fugazi3011', 'bensotokarass'), ('abbottag', 'ken', 'paxton'), ('abcu002c', 'cbsu002cnbcu002c', 'cnnu002c'), ('acairan', 'dealgay', 'marriagecuban'), ('adamrubinespn', 'msimonespn', 'ynscspds'), ('afsaneh', 'moqadam', 'hardcover'), ('ageu2019', 'edcfc73', 'cheers')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 下載 NLTK 資源（如果尚未下載）\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 加載數據\n",
    "file_path = './semeval-2017-train.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "\n",
    "# 清理文本數據\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # 移除URL\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除標點符號\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # 移除單獨的數字\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 移除多餘的空格\n",
    "    text = text.lower()  # 轉為小寫\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# 分離正面和負面樣本\n",
    "positive_samples = data[data['label'] > 0]['text'].tolist()\n",
    "negative_samples = data[data['label'] < 0]['text'].tolist()\n",
    "\n",
    "# Tokenize 文本樣本\n",
    "positive_tokens = [word_tokenize(text) for text in positive_samples]\n",
    "negative_tokens = [word_tokenize(text) for text in negative_samples]\n",
    "\n",
    "# 展平 tokens 列表（將所有樣本的 token 合併為一個列表）\n",
    "positive_tokens_flat = [token for sublist in positive_tokens for token in sublist]\n",
    "negative_tokens_flat = [token for sublist in negative_tokens for token in sublist]\n",
    "\n",
    "# 使用 NLTK 的 Collocation Finder 找出 bi-grams 和 tri-grams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# 正面情緒 bi-grams 和 tri-grams\n",
    "positive_bigram_finder = BigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "positive_trigram_finder = TrigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "\n",
    "positive_bigrams = positive_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "positive_trigrams = positive_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 負面情緒 bi-grams 和 tri-grams\n",
    "negative_bigram_finder = BigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "negative_trigram_finder = TrigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "\n",
    "negative_bigrams = negative_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "negative_trigrams = negative_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"Top-10 Positive Sentiment Bigrams:\", positive_bigrams)\n",
    "print(\"Top-10 Positive Sentiment Trigrams:\", positive_trigrams)\n",
    "print(\"Top-10 Negative Sentiment Bigrams:\", negative_bigrams)\n",
    "print(\"Top-10 Negative Sentiment Trigrams:\", negative_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d0bcc06-57c5-4515-9496-892a637244ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sub() missing 2 required positional arguments: 'repl' and 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# 轉為小寫\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 27\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 分離正面和負面樣本\u001b[39;00m\n\u001b[0;32m     30\u001b[0m positive_samples \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# 移除單獨的數字\u001b[39;00m\n\u001b[0;32m     22\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# 移除多餘空格\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# 轉為小寫\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: sub() missing 2 required positional arguments: 'repl' and 'string'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 下載 NLTK 資源（如果尚未下載）\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 加載數據\n",
    "file_path = './semeval-2017-train.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=0, names=['label', 'text'])\n",
    "\n",
    "# 清理文本數據\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # 移除 URL\n",
    "    text = re.sub(r'#\\w+', '', text)  # 移除 Hashtags\n",
    "    text = re.sub(r'u\\d{4}', '', text)  # 移除 Unicode 編碼（如 u002c, u2019）\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除標點符號\n",
    "    text = re.sub(r'\\b[a-zA-Z]*\\d+[a-zA-Z]*\\b', '', text)  # 移除數字與字母混合的詞\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # 移除單獨的數字\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 移除多餘空格\n",
    "    text = text.lower()  # 轉為小寫\n",
    "    return text\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# 分離正面和負面樣本\n",
    "positive_samples = data[data['label'] > 0]['text'].tolist()\n",
    "negative_samples = data[data['label'] < 0]['text'].tolist()\n",
    "\n",
    "# Tokenize 文本樣本\n",
    "positive_tokens = [word_tokenize(text) for text in positive_samples]\n",
    "negative_tokens = [word_tokenize(text) for text in negative_samples]\n",
    "\n",
    "# 展平 tokens 列表（將所有樣本的 token 合併為一個列表）\n",
    "positive_tokens_flat = [token for sublist in positive_tokens for token in sublist]\n",
    "negative_tokens_flat = [token for sublist in negative_tokens for token in sublist]\n",
    "\n",
    "# 移除短於兩個字符的詞\n",
    "positive_tokens_flat = [token for token in positive_tokens_flat if len(token) > 2]\n",
    "negative_tokens_flat = [token for token in negative_tokens_flat if len(token) > 2]\n",
    "\n",
    "# 使用 NLTK 的 Collocation Finder 找出 bi-grams 和 tri-grams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "# 正面情緒 bi-grams 和 tri-grams\n",
    "positive_bigram_finder = BigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "positive_trigram_finder = TrigramCollocationFinder.from_words(positive_tokens_flat)\n",
    "\n",
    "positive_bigrams = positive_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "positive_trigrams = positive_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 負面情緒 bi-grams 和 tri-grams\n",
    "negative_bigram_finder = BigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "negative_trigram_finder = TrigramCollocationFinder.from_words(negative_tokens_flat)\n",
    "\n",
    "negative_bigrams = negative_bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "negative_trigrams = negative_trigram_finder.nbest(trigram_measures.pmi, 10)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"Top-10 Positive Sentiment Bigrams:\", positive_bigrams)\n",
    "print(\"Top-10 Positive Sentiment Trigrams:\", positive_trigrams)\n",
    "print(\"Top-10 Negative Sentiment Bigrams:\", negative_bigrams)\n",
    "print(\"Top-10 Negative Sentiment Trigrams:\", negative_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa04f31-e9b1-4902-aad5-f6b6cd6b106c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
