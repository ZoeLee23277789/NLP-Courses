{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a540b7-d50b-4871-872d-4008e0e4c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the train_reviews.csv\n",
      "==================================================================================\n",
      "==================================================================================\n",
      "model_name =  Naive Bayes\n",
      "Naive Bayes Validation Accuracy: 0.892\n",
      "model_name =  Logistic Regression\n",
      "Logistic Regression Validation Accuracy: 0.8948\n",
      "model_name =  Decision Tree\n",
      "Decision Tree Validation Accuracy: 0.7236\n",
      "model_name =  Random Forest\n",
      "Random Forest Validation Accuracy: 0.8392\n",
      "model_name =  KNN\n",
      "KNN Validation Accuracy: 0.8008\n",
      "\n",
      "Best Model: Logistic Regression\n",
      "Logistic Regression Test Accuracy: 0.88408\n",
      "==================================================================================\n",
      "======================================= Finish all the Part B ===========================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# relative path\n",
    "base_dir = os.path.join(\".\", \"aclImdb_v1\", \"aclImdb\")\n",
    "\n",
    "def load_reviews(directory, label):\n",
    "    reviews = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            review_text = file.read()\n",
    "            # The file name format is [id]_[rating].txt, for example 200_8.txt\n",
    "            id, rating = file_name.split('_')\n",
    "            rating = rating.split('.')[0]  \n",
    "            reviews.append([id, rating, review_text, label])\n",
    "    return reviews\n",
    "\n",
    "# Read the positive and negative comments from the training set and use os.path.join to dynamically build the path\n",
    "train_pos_reviews = load_reviews(os.path.join(base_dir, \"train\", \"pos\"), 1)  # label=1 pos\n",
    "train_neg_reviews = load_reviews(os.path.join(base_dir, \"train\", \"neg\"), 0)  # label =0 neg\n",
    "# comb dataset\n",
    "train_reviews = train_pos_reviews + train_neg_reviews\n",
    "\n",
    "train_df = pd.DataFrame(train_reviews, columns=['id', 'rating', 'review_text', 'label'])\n",
    "# save\n",
    "train_df.to_csv(os.path.join(base_dir, \"train\", \"train_reviews.csv\"), index=False)\n",
    "print(\"Get the train_reviews.csv\")\n",
    "\n",
    "test_pos_reviews = load_reviews(os.path.join(base_dir, \"test\", \"pos\"), 1)\n",
    "test_neg_reviews = load_reviews(os.path.join(base_dir, \"test\", \"neg\"), 0)\n",
    "\n",
    "test_reviews = test_pos_reviews + test_neg_reviews\n",
    "test_df = pd.DataFrame(test_reviews, columns=['id', 'rating', 'review_text', 'label'])\n",
    "test_df.to_csv(os.path.join(base_dir, \"test\", \"test_reviews.csv\"), index=False)\n",
    "print(\"Get the test_reviews.csv\")\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(base_dir, \"train\", \"train_reviews.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(base_dir, \"test\", \"test_reviews.csv\"))\n",
    "\n",
    "#  train_test_split \n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df['review_text'], train_df['label'], test_size=0.1, random_state=42)\n",
    "\n",
    "#used TfidfVectorizer get n-gram feature\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')  # Example: bigrams, remove stopwords\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(test_df['review_text'])\n",
    "\n",
    "print(\"==================================================================================\")\n",
    "\n",
    "# Get the best hyperparameters form Optuna\n",
    "best_params = {\n",
    "    \"Naive Bayes\": {'alpha': 0.05450760213833489},\n",
    "    \"Logistic Regression\": {'C': 89.67647856745852, 'solver': 'saga', 'max_iter': 1000},\n",
    "    \"Decision Tree\": {'max_depth': 20, 'min_samples_split': 3},\n",
    "    \"Random Forest\": {'n_estimators': 189, 'max_depth': 18, 'min_samples_split': 6},\n",
    "    \"KNN\": {'n_neighbors': 7}\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(**best_params[\"Naive Bayes\"]),\n",
    "    \"Logistic Regression\": LogisticRegression(**best_params[\"Logistic Regression\"]),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(**best_params[\"Decision Tree\"]),\n",
    "    \"Random Forest\": RandomForestClassifier(**best_params[\"Random Forest\"]),\n",
    "    \"KNN\": KNeighborsClassifier(**best_params[\"KNN\"])\n",
    "}\n",
    "print(\"==================================================================================\")\n",
    "# save the best model\n",
    "best_model_name = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# train the best model\n",
    "for model_name, model in models.items():\n",
    "    print(\"model_name = \" ,model_name)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    val_predictions = model.predict(X_val_tfidf)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    print(f\"{model_name} Validation Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    # Save the best model on the validation set\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_name = model_name\n",
    "\n",
    "# Evaluate on the test set using the best performing model on the validation set\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "best_model = models[best_model_name]\n",
    "test_predictions = best_model.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(test_df['label'], test_predictions)\n",
    "print(f\"{best_model_name} Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "print(\"==================================================================================\")\n",
    "\n",
    "# # Find the best hyperparameters using Optuna\n",
    "# # \n",
    "# print(\"Define hyperparameters for each model to optimize\")\n",
    "# # 6. KNN\n",
    "# def optimize_knn(trial):\n",
    "#     n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "#     model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "\n",
    "# # 1. Naive Bayes\n",
    "# def optimize_naive_bayes(trial):\n",
    "#     alpha = trial.suggest_loguniform('alpha', 1e-3, 1e1)\n",
    "#     model = MultinomialNB(alpha=alpha)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# # 2. SVM\n",
    "# def optimize_svm(trial):\n",
    "#     C = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "#     model = SVC(C=C, kernel=kernel)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# # 3. Logistic Regression\n",
    "# def optimize_logistic_regression(trial):\n",
    "#     C = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "#     solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "#     model = LogisticRegression(C=C, solver=solver, max_iter=1000)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# # 4. Decision Tree\n",
    "# def optimize_decision_tree(trial):\n",
    "#     max_depth = trial.suggest_int('max_depth', 2, 20)\n",
    "#     min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "#     model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# # 5. Random Forest\n",
    "# def optimize_random_forest(trial):\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "#     max_depth = trial.suggest_int('max_depth', 2, 20)\n",
    "#     min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "#     model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     val_predictions = model.predict(X_val_tfidf)\n",
    "#     return accuracy_score(y_val, val_predictions)\n",
    "\n",
    "# print(\"Optimize the hyperparameters of each model\")\n",
    "# models = {\n",
    "#     \"Naive Bayes\": optimize_naive_bayes,\n",
    "#     \"Logistic Regression\": optimize_logistic_regression,\n",
    "#     \"Decision Tree\": optimize_decision_tree,\n",
    "#     \"Random Forest\": optimize_random_forest,\n",
    "#     \"KNN\": optimize_knn,\n",
    "# }\n",
    "\n",
    "# best_models = {}\n",
    "# for model_name, objective in models.items():\n",
    "#     print(f\"Optimizing {model_name}...\")\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "#     print(f\"Best {model_name} parameters: {study.best_trial.params}\")\n",
    "#     print(f\"Best validation accuracy: {study.best_trial.value}\")\n",
    "#     best_models[model_name] = study.best_trial.params\n",
    "\n",
    "# print(\"Use the best hyperparameters for each model on the test set\")\n",
    "# for model_name, best_params in best_models.items():\n",
    "#     print(f\"\\nTesting best {model_name} model on test set:\")\n",
    "    \n",
    "#     if model_name == \"Naive Bayes\":\n",
    "#         model = MultinomialNB(**best_params)\n",
    "#     elif model_name == \"Logistic Regression\":\n",
    "#         model = LogisticRegression(**best_params)\n",
    "#     elif model_name == \"Decision Tree\":\n",
    "#         model = DecisionTreeClassifier(**best_params)\n",
    "#     elif model_name == \"Random Forest\":\n",
    "#         model = RandomForestClassifier(**best_params)\n",
    "#     elif model_name == \"KNN\":\n",
    "#         model = KNN(**best_params)\n",
    "#     model.fit(X_train_tfidf, y_train)\n",
    "#     test_predictions = model.predict(X_test_tfidf)\n",
    "#     test_accuracy = accuracy_score(test_df['label'], test_predictions)\n",
    "#     print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "print(\"======================================= Finish all the Part B ===========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252e2d6-ffe6-4ca7-8c47-c52adb9d56af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
