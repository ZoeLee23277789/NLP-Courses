{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef721-358f-4589-b1d1-a5bb73ac1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
      "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
      "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
      "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
      "4  Background-Foreground Segmentation for Interio...   \n",
      "\n",
      "                                            abstract                 labels  \n",
      "0  Stereo matching is one of the widely used tech...         [cs.CV, cs.LG]  \n",
      "1  The recent advancements in artificial intellig...  [cs.CV, cs.AI, cs.LG]  \n",
      "2  In this paper, we proposed a novel mutual cons...         [cs.CV, cs.AI]  \n",
      "3  Consistency training has proven to be an advan...                [cs.CV]  \n",
      "4  To ensure safety in automated driving, the cor...         [cs.CV, cs.LG]  \n",
      "Training set size: 36239\n",
      "Validation set size: 7768\n",
      "Testing set size: 7767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting Validation F1 Score: 0.7322\n",
      "Hist Gradient Boosting Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.25      0.07         4\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.14      0.30      0.19        10\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.69      0.20      0.31      1150\n",
      "           5       0.05      0.40      0.09         5\n",
      "           6       0.03      0.25      0.06         4\n",
      "           7       0.00      0.00      0.00         9\n",
      "           8       1.00      0.53      0.69        19\n",
      "           9       0.89      0.53      0.66       238\n",
      "          10       0.58      0.30      0.40       103\n",
      "          11       0.94      0.93      0.93      4526\n",
      "          12       0.67      0.20      0.31        30\n",
      "          13       0.62      0.22      0.32        23\n",
      "          14       0.75      0.24      0.37        49\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.14      0.50      0.22         4\n",
      "          17       0.90      0.28      0.43        32\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.86      0.40      0.55        90\n",
      "          21       0.23      0.14      0.17        22\n",
      "          22       0.69      0.23      0.34        48\n",
      "          23       0.89      0.27      0.42        59\n",
      "          24       0.71      0.12      0.21        41\n",
      "          25       0.90      0.85      0.87      4393\n",
      "          26       0.11      0.22      0.15         9\n",
      "          27       0.76      0.40      0.52        55\n",
      "          28       0.86      0.23      0.37        77\n",
      "          29       0.78      0.32      0.45        22\n",
      "          30       0.93      0.15      0.26       187\n",
      "          31       0.22      0.10      0.14        20\n",
      "          32       0.05      0.10      0.07        10\n",
      "          33       0.02      0.14      0.04         7\n",
      "          34       0.75      0.24      0.36       305\n",
      "          35       0.00      0.00      0.00         4\n",
      "          36       0.91      0.38      0.54        26\n",
      "          37       0.12      0.20      0.15        15\n",
      "          38       0.88      0.52      0.66       111\n",
      "          39       0.57      0.21      0.30        77\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       1.00      0.50      0.67        28\n",
      "          43       1.00      0.12      0.21       354\n",
      "          44       0.83      0.28      0.42        85\n",
      "          45       0.67      0.21      0.32        57\n",
      "          46       0.00      0.00      0.00         3\n",
      "          47       0.00      0.00      0.00         3\n",
      "          48       0.03      0.12      0.05         8\n",
      "          49       0.00      0.00      0.00         0\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.08      0.25      0.12        12\n",
      "          52       0.00      0.00      0.00         4\n",
      "          53       0.83      0.12      0.21        41\n",
      "          54       0.77      0.45      0.57        22\n",
      "          55       0.70      0.26      0.38       120\n",
      "          56       0.50      0.11      0.18         9\n",
      "          57       0.00      0.00      0.00         4\n",
      "          58       0.22      0.09      0.13        22\n",
      "          59       0.00      0.00      0.00         6\n",
      "          60       0.00      0.00      0.00         8\n",
      "          61       0.00      0.00      0.00         6\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.54      0.79      0.64        19\n",
      "          64       0.29      0.33      0.31        18\n",
      "          65       0.86      0.40      0.55        15\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       0.05      0.33      0.09         3\n",
      "          68       0.04      0.10      0.06        10\n",
      "          69       0.05      0.33      0.08         3\n",
      "          70       0.02      0.12      0.04         8\n",
      "          71       0.23      0.55      0.32        11\n",
      "          72       0.07      0.22      0.11         9\n",
      "          73       0.00      0.00      0.00         3\n",
      "          74       0.73      0.32      0.44        25\n",
      "          75       1.00      0.34      0.51        47\n",
      "          76       0.07      0.17      0.10         6\n",
      "          77       0.00      0.00      0.00         2\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.05      0.29      0.09         7\n",
      "          81       0.04      0.33      0.07         3\n",
      "          82       0.02      0.20      0.04         5\n",
      "          83       0.11      0.02      0.03        56\n",
      "          84       0.15      0.19      0.17        16\n",
      "          85       0.62      0.25      0.36        32\n",
      "          86       0.69      0.67      0.68      2354\n",
      "          87       0.30      0.14      0.19        22\n",
      "\n",
      "   micro avg       0.78      0.69      0.73     15286\n",
      "   macro avg       0.36      0.22      0.22     15286\n",
      "weighted avg       0.83      0.69      0.72     15286\n",
      " samples avg       0.84      0.77      0.77     15286\n",
      "\n",
      "Progress: 100.00% complete\n",
      "\n",
      "\n",
      "Test Classification Report (Hist Gradient Boosting):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.03      0.11      0.05         9\n",
      "           2       0.06      0.25      0.10         4\n",
      "           3       0.04      0.12      0.06         8\n",
      "           4       0.69      0.19      0.30      1256\n",
      "           5       0.04      0.40      0.07         5\n",
      "           6       0.00      0.00      0.00         6\n",
      "           7       0.19      0.36      0.25        14\n",
      "           8       1.00      0.35      0.52        23\n",
      "           9       0.87      0.47      0.61       233\n",
      "          10       0.55      0.21      0.30       100\n",
      "          11       0.94      0.93      0.94      4535\n",
      "          12       0.75      0.30      0.43        40\n",
      "          13       0.18      0.13      0.15        15\n",
      "          14       0.75      0.23      0.35        39\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.21      0.80      0.33         5\n",
      "          17       0.50      0.36      0.42        14\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.89      0.26      0.41        95\n",
      "          21       0.14      0.11      0.12        19\n",
      "          22       0.90      0.36      0.51        50\n",
      "          23       1.00      0.24      0.39        66\n",
      "          24       1.00      0.13      0.24        45\n",
      "          25       0.89      0.85      0.87      4358\n",
      "          26       0.09      0.27      0.13        11\n",
      "          27       0.69      0.53      0.60        70\n",
      "          28       0.96      0.42      0.58        65\n",
      "          29       0.25      0.14      0.18        22\n",
      "          30       0.83      0.16      0.27       181\n",
      "          31       0.44      0.17      0.24        24\n",
      "          32       0.04      0.14      0.06         7\n",
      "          33       0.01      0.12      0.02         8\n",
      "          34       0.80      0.22      0.34       303\n",
      "          35       0.04      0.20      0.06         5\n",
      "          36       0.90      0.39      0.55        23\n",
      "          37       0.02      0.08      0.03        13\n",
      "          38       0.80      0.53      0.64       104\n",
      "          39       0.63      0.16      0.26        75\n",
      "          40       0.05      0.17      0.07         6\n",
      "          41       0.05      0.50      0.09         2\n",
      "          42       0.22      0.22      0.22        27\n",
      "          43       0.91      0.11      0.19       376\n",
      "          44       1.00      0.24      0.38        89\n",
      "          45       0.60      0.20      0.30        61\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       0.04      0.25      0.07         4\n",
      "          49       0.00      0.00      0.00         3\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         7\n",
      "          52       0.00      0.00      0.00         3\n",
      "          53       0.58      0.16      0.25        45\n",
      "          54       0.36      0.17      0.23        24\n",
      "          55       0.53      0.19      0.28       106\n",
      "          56       0.10      0.22      0.14         9\n",
      "          57       0.00      0.00      0.00         3\n",
      "          58       0.64      0.20      0.30        35\n",
      "          59       0.00      0.00      0.00         2\n",
      "          60       0.06      0.50      0.10         2\n",
      "          61       0.00      0.00      0.00         1\n",
      "          62       0.00      0.00      0.00         6\n",
      "          63       0.77      0.53      0.62        19\n",
      "          64       0.38      0.33      0.36        15\n",
      "          65       0.00      0.00      0.00         6\n",
      "          66       0.00      0.00      0.00         5\n",
      "          67       0.00      0.00      0.00         6\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.04      0.33      0.07         3\n",
      "          70       0.02      0.17      0.04         6\n",
      "          71       0.50      0.47      0.48        17\n",
      "          72       0.09      0.33      0.14         3\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.80      0.15      0.25        27\n",
      "          75       0.40      0.06      0.10        35\n",
      "          76       0.04      0.20      0.07         5\n",
      "          77       0.00      0.00      0.00        13\n",
      "          78       0.09      0.50      0.15         2\n",
      "          79       0.00      0.00      0.00         9\n",
      "          80       0.02      0.10      0.04        10\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.00      0.00      0.00         5\n",
      "          83       0.75      0.33      0.46        36\n",
      "          84       0.00      0.00      0.00        18\n",
      "          85       1.00      0.26      0.41        31\n",
      "          86       0.66      0.68      0.67      2323\n",
      "          87       0.50      0.20      0.29        35\n",
      "\n",
      "   micro avg       0.77      0.68      0.72     15320\n",
      "   macro avg       0.33      0.21      0.21     15320\n",
      "weighted avg       0.82      0.68      0.71     15320\n",
      " samples avg       0.83      0.77      0.76     15320\n",
      "\n",
      "\n",
      "Training and Inference Times:\n",
      "Hist Gradient Boosting - Training time: 117.9556 seconds, Inference time: 2.4253 seconds\n",
      "Results saved to SGB_Results.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('arxiv_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# 構建 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'title': data['titles'],\n",
    "    'abstract': data['summaries'],\n",
    "    'labels': data['terms']\n",
    "})\n",
    "\n",
    "# 查看 DataFrame 結構\n",
    "print(df.head())\n",
    "# 把 85% 的數據分割為訓練集和 15% 的數據分割為測試集\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 把訓練集再分割出 15% 作為驗證集 (即 70% 訓練集, 15% 驗證集)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1765, random_state=42)  # 0.1765 確保驗證集約佔 15% 原始數據\n",
    "\n",
    "# 查看各集的大小\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Validation set size:\", len(val_df))\n",
    "print(\"Testing set size:\", len(test_df))\n",
    "\n",
    "# 解析和視覺化標籤分佈\n",
    "# 展開標籤並統計頻次\n",
    "all_labels = [label for labels in df['labels'] for label in labels]\n",
    "label_counts = pd.Series(all_labels).value_counts()\n",
    "\n",
    "\n",
    "# 假設 df 是您處理過的 DataFrame\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 去除標點符號和停用詞\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # 去除非字母字符\n",
    "    text = text.lower()  # 全部轉小寫\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['abstract'] = df['abstract'].apply(preprocess_text)\n",
    "\n",
    "# 將多標籤轉為二值矩陣\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['labels'])\n",
    "\n",
    "# 分割訓練集、驗證集、測試集\n",
    "train_texts, test_texts, y_train, y_test = train_test_split(df['abstract'], y, test_size=0.15, random_state=42)\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(train_texts, y_train, test_size=0.1765, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 使用 TF-IDF 表示文本特徵\n",
    "tfidf = TfidfVectorizer(max_features=500)  # 限制特徵數量以提高速度\n",
    "X_train = tfidf.fit_transform(train_texts).toarray()\n",
    "X_val = tfidf.transform(val_texts).toarray()\n",
    "X_test = tfidf.transform(test_texts).toarray()\n",
    "\n",
    "\n",
    "# 定義不含 SVM 的模型列表\n",
    "models = {\n",
    "    # 'Naive Bayes': MultinomialNB(),\n",
    "    # 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingClassifier(max_iter=100),\n",
    "    # 'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=3),\n",
    "    # 'Ridge Classifier': RidgeClassifier()\n",
    "}\n",
    "\n",
    "# 儲存訓練和推理時間的字典\n",
    "train_times = {}\n",
    "inference_times = {}\n",
    "val_scores = {}\n",
    "reports = {}\n",
    "\n",
    "# 訓練每個模型並評估驗證集的表現\n",
    "total_models = len(models)\n",
    "for idx, (name, model) in enumerate(models.items(), start=1):\n",
    "    multi_target_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "    \n",
    "    # 訓練時間測量\n",
    "    start_time = time.time()\n",
    "    multi_target_model.fit(X_train, y_train)\n",
    "    train_times[name] = time.time() - start_time\n",
    "    \n",
    "    # 推理時間測量\n",
    "    start_time = time.time()\n",
    "    y_val_pred = multi_target_model.predict(X_val)\n",
    "    inference_times[name] = time.time() - start_time\n",
    "    \n",
    "    # 計算 F1 分數作為驗證指標\n",
    "    score = f1_score(y_val, y_val_pred, average='micro')  # 使用微平均\n",
    "    val_scores[name] = score\n",
    "    print(f\"{name} Validation F1 Score: {score:.4f}\")\n",
    "    \n",
    "    # 生成分類報告（驗證集）\n",
    "    reports[name] = classification_report(y_val, y_val_pred, zero_division=0)\n",
    "    print(f\"{name} Validation Classification Report:\\n{reports[name]}\")\n",
    "    \n",
    "    # 顯示進度百分比\n",
    "    progress = (idx / total_models) * 100\n",
    "    print(f\"Progress: {progress:.2f}% complete\\n\")\n",
    "\n",
    "# 選擇最佳模型進行測試報告\n",
    "best_model_name = max(val_scores, key=val_scores.get)\n",
    "best_model = MultiOutputClassifier(models[best_model_name], n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# 生成測試集的分類報告\n",
    "test_report = classification_report(y_test, y_test_pred, zero_division=0)\n",
    "print(f\"\\nTest Classification Report ({best_model_name}):\\n{test_report}\")\n",
    "\n",
    "# 顯示各分類器的訓練和推理時間\n",
    "print(\"\\nTraining and Inference Times:\")\n",
    "for name in models.keys():\n",
    "    print(f\"{name} - Training time: {train_times[name]:.4f} seconds, Inference time: {inference_times[name]:.4f} seconds\")\n",
    "\n",
    "# 假設 best_model 是您選擇的最佳模型，且已經完成訓練\n",
    "# 使用最佳模型進行預測\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# 生成分類報告\n",
    "val_report = classification_report(y_val, y_val_pred, zero_division=0)\n",
    "test_report = classification_report(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "# 將報告寫入 results.txt\n",
    "output_path = \"SGB_Results.txt\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"Validation Classification Report:\\n\")\n",
    "    f.write(val_report)\n",
    "    f.write(\"\\n\\nTest Classification Report:\\n\")\n",
    "    f.write(test_report)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508b5a1-2e76-482f-b0e0-a85954852df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-11-18 18:14:46,643] A new study created in memory with name: no-name-645866b3-047c-41f3-a2e8-c71f96bf3818\n",
      "[I 2024-11-18 18:17:43,828] Trial 0 finished with value: 0.7630790190735695 and parameters: {'max_iter': 293, 'learning_rate': 0.08090556204719598, 'max_leaf_nodes': 50, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:21:54,436] Trial 1 finished with value: 0.7583637744583891 and parameters: {'max_iter': 258, 'learning_rate': 0.02286218428721065, 'max_leaf_nodes': 54, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:23:59,162] Trial 2 finished with value: 0.7371779311301454 and parameters: {'max_iter': 89, 'learning_rate': 0.05723762710784108, 'max_leaf_nodes': 74, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:25:44,985] Trial 3 finished with value: 0.7537718972558815 and parameters: {'max_iter': 147, 'learning_rate': 0.11336541464558848, 'max_leaf_nodes': 70, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:26:40,050] Trial 4 finished with value: 0.6627132114690957 and parameters: {'max_iter': 52, 'learning_rate': 0.24246884303319438, 'max_leaf_nodes': 62, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:28:22,336] Trial 5 finished with value: 0.7218056061157627 and parameters: {'max_iter': 185, 'learning_rate': 0.04084095909687865, 'max_leaf_nodes': 15, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:29:09,891] Trial 6 finished with value: 0.698870037210255 and parameters: {'max_iter': 276, 'learning_rate': 0.15180690299875413, 'max_leaf_nodes': 11, 'min_samples_leaf': 13}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:31:29,437] Trial 7 finished with value: 0.7365077307056498 and parameters: {'max_iter': 119, 'learning_rate': 0.17541901771688118, 'max_leaf_nodes': 96, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:33:19,360] Trial 8 finished with value: 0.7109589483543812 and parameters: {'max_iter': 53, 'learning_rate': 0.1853073334673996, 'max_leaf_nodes': 78, 'min_samples_leaf': 14}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:35:30,220] Trial 9 finished with value: 0.7159141850784502 and parameters: {'max_iter': 164, 'learning_rate': 0.14288736496849846, 'max_leaf_nodes': 59, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:36:40,899] Trial 10 finished with value: 0.719863954477075 and parameters: {'max_iter': 221, 'learning_rate': 0.2931341420543594, 'max_leaf_nodes': 40, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:43:48,172] Trial 11 finished with value: 0.7512735326688814 and parameters: {'max_iter': 293, 'learning_rate': 0.016851535933450165, 'max_leaf_nodes': 40, 'min_samples_leaf': 20}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:46:04,789] Trial 12 finished with value: 0.7397995105106112 and parameters: {'max_iter': 247, 'learning_rate': 0.08371759266928738, 'max_leaf_nodes': 40, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:48:26,275] Trial 13 finished with value: 0.7322569563723806 and parameters: {'max_iter': 234, 'learning_rate': 0.0851777033512961, 'max_leaf_nodes': 29, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 18:57:48,537] Trial 14 finished with value: 0.749661010737714 and parameters: {'max_iter': 299, 'learning_rate': 0.014101445847381948, 'max_leaf_nodes': 51, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 19:04:33,676] Trial 15 finished with value: 0.7623900268351507 and parameters: {'max_iter': 204, 'learning_rate': 0.07058112951766414, 'max_leaf_nodes': 89, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 19:13:01,881] Trial 16 finished with value: 0.7598042501843536 and parameters: {'max_iter': 203, 'learning_rate': 0.1023862576726043, 'max_leaf_nodes': 95, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.7630790190735695.\n",
      "[I 2024-11-18 19:18:24,664] Trial 17 finished with value: 0.7656035193841078 and parameters: {'max_iter': 204, 'learning_rate': 0.058793518024560384, 'max_leaf_nodes': 83, 'min_samples_leaf': 5}. Best is trial 17 with value: 0.7656035193841078.\n",
      "[I 2024-11-18 19:22:41,381] Trial 18 finished with value: 0.7674071054955505 and parameters: {'max_iter': 135, 'learning_rate': 0.11674905142995552, 'max_leaf_nodes': 88, 'min_samples_leaf': 19}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:26:17,287] Trial 19 finished with value: 0.7334630350194553 and parameters: {'max_iter': 122, 'learning_rate': 0.12705799553124728, 'max_leaf_nodes': 84, 'min_samples_leaf': 5}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:30:04,144] Trial 20 finished with value: 0.7218645948945617 and parameters: {'max_iter': 145, 'learning_rate': 0.21222770903092913, 'max_leaf_nodes': 100, 'min_samples_leaf': 20}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:35:10,530] Trial 21 finished with value: 0.7466987644705634 and parameters: {'max_iter': 103, 'learning_rate': 0.050161673310202534, 'max_leaf_nodes': 67, 'min_samples_leaf': 17}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:40:00,958] Trial 22 finished with value: 0.7667503903333107 and parameters: {'max_iter': 182, 'learning_rate': 0.09827381752131162, 'max_leaf_nodes': 87, 'min_samples_leaf': 18}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:45:43,163] Trial 23 finished with value: 0.7652667883952119 and parameters: {'max_iter': 175, 'learning_rate': 0.11102096852759363, 'max_leaf_nodes': 83, 'min_samples_leaf': 19}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 19:56:19,808] Trial 24 finished with value: 0.7566364331040897 and parameters: {'max_iter': 203, 'learning_rate': 0.137224157083273, 'max_leaf_nodes': 89, 'min_samples_leaf': 18}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 20:00:38,269] Trial 25 finished with value: 0.7617110704359026 and parameters: {'max_iter': 160, 'learning_rate': 0.10033810941793471, 'max_leaf_nodes': 80, 'min_samples_leaf': 13}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 20:04:00,481] Trial 26 finished with value: 0.7171040151754663 and parameters: {'max_iter': 185, 'learning_rate': 0.1724280071568905, 'max_leaf_nodes': 90, 'min_samples_leaf': 10}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 20:10:39,957] Trial 27 finished with value: 0.7571103526734926 and parameters: {'max_iter': 135, 'learning_rate': 0.03966938002706873, 'max_leaf_nodes': 74, 'min_samples_leaf': 16}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 20:15:46,794] Trial 28 finished with value: 0.751669492405107 and parameters: {'max_iter': 88, 'learning_rate': 0.06839575130362431, 'max_leaf_nodes': 100, 'min_samples_leaf': 19}. Best is trial 18 with value: 0.7674071054955505.\n",
      "[I 2024-11-18 20:19:25,206] Trial 29 finished with value: 0.7677230323645303 and parameters: {'max_iter': 224, 'learning_rate': 0.08968149659081272, 'max_leaf_nodes': 68, 'min_samples_leaf': 1}. Best is trial 29 with value: 0.7677230323645303.\n",
      "[I 2024-11-18 20:22:58,139] Trial 30 finished with value: 0.7644530987712348 and parameters: {'max_iter': 224, 'learning_rate': 0.09149405443289879, 'max_leaf_nodes': 66, 'min_samples_leaf': 1}. Best is trial 29 with value: 0.7677230323645303.\n",
      "[I 2024-11-18 20:27:06,806] Trial 31 finished with value: 0.7526317515495359 and parameters: {'max_iter': 208, 'learning_rate': 0.1184524689719528, 'max_leaf_nodes': 86, 'min_samples_leaf': 3}. Best is trial 29 with value: 0.7677230323645303.\n",
      "[I 2024-11-18 20:31:44,841] Trial 32 finished with value: 0.771316253219466 and parameters: {'max_iter': 252, 'learning_rate': 0.07028636196733792, 'max_leaf_nodes': 75, 'min_samples_leaf': 1}. Best is trial 32 with value: 0.771316253219466.\n",
      "[I 2024-11-18 20:36:37,522] Trial 33 finished with value: 0.7694853437795488 and parameters: {'max_iter': 272, 'learning_rate': 0.07520916783552864, 'max_leaf_nodes': 76, 'min_samples_leaf': 2}. Best is trial 32 with value: 0.771316253219466.\n",
      "[I 2024-11-18 20:43:53,823] Trial 34 finished with value: 0.7714496781229113 and parameters: {'max_iter': 267, 'learning_rate': 0.03425751521105928, 'max_leaf_nodes': 77, 'min_samples_leaf': 2}. Best is trial 34 with value: 0.7714496781229113.\n",
      "[I 2024-11-18 20:51:23,391] Trial 35 finished with value: 0.7722765314016456 and parameters: {'max_iter': 265, 'learning_rate': 0.0358418056874643, 'max_leaf_nodes': 73, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.7722765314016456.\n",
      "[I 2024-11-18 20:59:53,282] Trial 36 finished with value: 0.7735915992620973 and parameters: {'max_iter': 260, 'learning_rate': 0.03187779245279722, 'max_leaf_nodes': 74, 'min_samples_leaf': 3}. Best is trial 36 with value: 0.7735915992620973.\n",
      "[I 2024-11-18 21:07:02,224] Trial 37 finished with value: 0.7658290743895514 and parameters: {'max_iter': 257, 'learning_rate': 0.03278851265358005, 'max_leaf_nodes': 60, 'min_samples_leaf': 3}. Best is trial 36 with value: 0.7735915992620973.\n",
      "[I 2024-11-18 21:14:07,741] Trial 38 finished with value: 0.759236933550812 and parameters: {'max_iter': 277, 'learning_rate': 0.029778343617160516, 'max_leaf_nodes': 55, 'min_samples_leaf': 2}. Best is trial 36 with value: 0.7735915992620973.\n",
      "[I 2024-11-18 21:20:06,081] Trial 39 finished with value: 0.7616544473647808 and parameters: {'max_iter': 247, 'learning_rate': 0.051231741285068466, 'max_leaf_nodes': 65, 'min_samples_leaf': 6}. Best is trial 36 with value: 0.7735915992620973.\n",
      "[I 2024-11-18 21:35:20,505] Trial 40 finished with value: 0.7562214507582677 and parameters: {'max_iter': 285, 'learning_rate': 0.013282383516994706, 'max_leaf_nodes': 72, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.7735915992620973.\n",
      "[I 2024-11-18 21:40:13,562] Trial 41 finished with value: 0.7739245398149089 and parameters: {'max_iter': 267, 'learning_rate': 0.06916002908822928, 'max_leaf_nodes': 76, 'min_samples_leaf': 2}. Best is trial 41 with value: 0.7739245398149089.\n",
      "[I 2024-11-18 21:45:28,164] Trial 42 finished with value: 0.7712897042716321 and parameters: {'max_iter': 263, 'learning_rate': 0.058493120222335576, 'max_leaf_nodes': 72, 'min_samples_leaf': 2}. Best is trial 41 with value: 0.7739245398149089.\n",
      "[I 2024-11-18 21:54:26,063] Trial 43 finished with value: 0.7692636224507954 and parameters: {'max_iter': 244, 'learning_rate': 0.028049912781991884, 'max_leaf_nodes': 78, 'min_samples_leaf': 4}. Best is trial 41 with value: 0.7739245398149089.\n",
      "[I 2024-11-18 22:00:09,205] Trial 44 finished with value: 0.764959944270289 and parameters: {'max_iter': 264, 'learning_rate': 0.044802580347404586, 'max_leaf_nodes': 61, 'min_samples_leaf': 2}. Best is trial 41 with value: 0.7739245398149089.\n",
      "[I 2024-11-18 22:05:32,271] Trial 45 finished with value: 0.7757406714514099 and parameters: {'max_iter': 289, 'learning_rate': 0.06515715924831363, 'max_leaf_nodes': 78, 'min_samples_leaf': 1}. Best is trial 45 with value: 0.7757406714514099.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 先前的代碼準備\n",
    "# 這裡假設您已經完成了數據加載和預處理等操作\n",
    "\n",
    "# 定義目標函數\n",
    "def objective(trial):\n",
    "    # 定義超參數搜索空間\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 50, 300)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "    max_leaf_nodes = trial.suggest_int(\"max_leaf_nodes\", 10, 100)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        max_iter=max_iter,\n",
    "        learning_rate=learning_rate,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 使用多標籤分類器封裝模型\n",
    "    multi_target_model = MultiOutputClassifier(model, n_jobs=-1)\n",
    "    \n",
    "    # 訓練模型\n",
    "    multi_target_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 驗證集評估\n",
    "    y_val_pred = multi_target_model.predict(X_val)\n",
    "    score = f1_score(y_val, y_val_pred, average='micro')  # 使用微平均計算F1分數\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 創建Optuna的研究對象，並運行優化\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 顯示最佳超參數\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best micro F1 score:\", study.best_value)\n",
    "\n",
    "# 使用最佳超參數訓練和測試模型\n",
    "best_params = study.best_params\n",
    "best_model = HistGradientBoostingClassifier(\n",
    "    max_iter=best_params[\"max_iter\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    max_leaf_nodes=best_params[\"max_leaf_nodes\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    random_state=42\n",
    ")\n",
    "best_multi_target_model = MultiOutputClassifier(best_model, n_jobs=-1)\n",
    "best_multi_target_model.fit(X_train, y_train)\n",
    "\n",
    "# 驗證集和測試集的分類報告\n",
    "y_val_pred = best_multi_target_model.predict(X_val)\n",
    "y_test_pred = best_multi_target_model.predict(X_test)\n",
    "\n",
    "val_report = classification_report(y_val, y_val_pred, zero_division=0)\n",
    "test_report = classification_report(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "# 儲存結果\n",
    "output_path = \"Optimized_HistGradientBoosting_Results.txt\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"Validation Classification Report:\\n\")\n",
    "    f.write(val_report)\n",
    "    f.write(\"\\n\\nTest Classification Report:\\n\")\n",
    "    f.write(test_report)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f65f0-de68-463c-9fd0-b0aed404dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成並保存可視化圖表\n",
    "# 1. 優化歷史 (Optimization History)\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "plt.title(\"Optimization History\")\n",
    "plt.show()\n",
    "\n",
    "# 2. 平行坐標圖 (Parallel Coordinate Plot)\n",
    "optuna.visualization.plot_parallel_coordinate(study)\n",
    "plt.title(\"Parallel Coordinate Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 3. 超參數重要性 (Hyperparameter Importance)\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "plt.title(\"Hyperparameter Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065277ee-5ff5-437e-aa4d-265bdfcf4cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
