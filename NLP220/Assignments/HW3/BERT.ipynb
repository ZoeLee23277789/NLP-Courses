{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c58fbd-d8c0-46e0-bc56-e0b2add84739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|███████████████████████████████████████████████████████████| 36239/36239 [03:05<00:00, 195.24it/s]\n",
      "Encoding texts: 100%|█████████████████████████████████████████████████████████████| 7768/7768 [00:39<00:00, 197.15it/s]\n",
      "Encoding texts: 100%|█████████████████████████████████████████████████████████████| 7767/7767 [00:39<00:00, 197.50it/s]\n",
      "Generating BERT embeddings:   0%|                                                             | 0/4530 [00:00<?, ?it/s]C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Generating BERT embeddings:  16%|███████▊                                         | 722/4530 [31:16<3:56:11,  3.72s/it]"
     ]
    }
   ],
   "source": [
    "# 匯入所需的庫\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm  # 新增 tqdm 用於顯示進度條\n",
    "\n",
    "# 檢查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 讀取資料\n",
    "with open('arxiv_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 將資料構建成 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'title': data['titles'],\n",
    "    'abstract': data['summaries'],\n",
    "    'labels': data['terms']\n",
    "})\n",
    "\n",
    "# 預處理標籤\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['labels'])\n",
    "\n",
    "# 分割資料集\n",
    "train_texts, test_texts, y_train, y_test = train_test_split(df['abstract'], y, test_size=0.15, random_state=42)\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(train_texts, y_train, test_size=0.1765, random_state=42)\n",
    "\n",
    "# 初始化 BERT Tokenizer 和 Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# 將文本轉換為 BERT 向量表示\n",
    "def encode_texts(texts):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in tqdm(texts, desc=\"Encoding texts\"):  # 使用 tqdm 顯示文本編碼進度\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0).to(device), torch.cat(attention_masks, dim=0).to(device)\n",
    "\n",
    "# 編碼訓練、驗證和測試集\n",
    "train_inputs, train_masks = encode_texts(train_texts)\n",
    "val_inputs, val_masks = encode_texts(val_texts)\n",
    "test_inputs, test_masks = encode_texts(test_texts)\n",
    "\n",
    "# 構建 DataLoader\n",
    "batch_size = 8\n",
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(y_train).to(device))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, torch.tensor(y_val).to(device))\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, torch.tensor(y_test).to(device))\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 提取 BERT 嵌入\n",
    "def get_bert_embeddings(dataloader):\n",
    "    bert_model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating BERT embeddings\"):  # 顯示 BERT 嵌入提取進度\n",
    "            input_ids, attention_mask = batch[0], batch[1]\n",
    "            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
    "            embeddings.append(cls_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# 取得訓練、驗證和測試集的嵌入\n",
    "train_embeddings = get_bert_embeddings(train_dataloader)\n",
    "val_embeddings = get_bert_embeddings(val_dataloader)\n",
    "test_embeddings = get_bert_embeddings(test_dataloader)\n",
    "\n",
    "# 將 BERT 嵌入轉為 NumPy 格式以供 Scikit-Learn 使用\n",
    "X_train = train_embeddings.cpu().numpy()\n",
    "X_val = val_embeddings.cpu().numpy()\n",
    "X_test = test_embeddings.cpu().numpy()\n",
    "\n",
    "# 定義模型（以 Logistic Regression 為例）\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "\n",
    "# 訓練模型並評估表現\n",
    "start_time = time.time()\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# 評估模型\n",
    "start_time = time.time()\n",
    "y_val_pred = model.predict(X_val)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# 計算 F1 分數和生成分類報告\n",
    "val_f1_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "val_report = classification_report(y_val, y_val_pred, zero_division=0)\n",
    "print(f\"Validation F1 Score: {val_f1_score:.4f}\")\n",
    "print(f\"Validation Classification Report:\\n{val_report}\")\n",
    "\n",
    "# 測試集評估\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_report = classification_report(y_test, y_test_pred, zero_division=0)\n",
    "print(f\"\\nTest Classification Report:\\n{test_report}\")\n",
    "\n",
    "# 顯示訓練和推理時間\n",
    "print(f\"\\nTraining time: {train_time:.4f} seconds\")\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c6c84-5d84-4763-b238-3952416c5844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
