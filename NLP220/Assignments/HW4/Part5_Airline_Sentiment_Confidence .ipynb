{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683e62dc-a001-41ca-a4b0-fd0ec450ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 7701\n",
      "Airline: Virgin America\n",
      "  Most Active User: wmrrock\n",
      "  Tweets:\n",
      "    - @VirginAmerica cool picture of another VirginAmerica plane off our wing. What a site! http://t.co/5B2agFd8c4\n",
      "    - CT\n",
      "    - positive\n",
      "\n",
      "    - @VirginAmerica seats in Row 8 don't recline should mention that on your website #soreback\n",
      "    - CT\n",
      "    - negative\n",
      "\n",
      "    - @VirginAmerica flight 404 delayed 2 hours in LA due to mechanical problems. Handle like pros but you could have tossed us a free drink.\n",
      "    - CT\n",
      "    - negative\n",
      "\n",
      "    - @VirginAmerica on VX399 from JFK to LA - dirty plane - not up to your standards.\n",
      "    - CT\n",
      "    - negative\n",
      "\n",
      "    - @VirginAmerica on flight VX399 headed to LA from JFK - dirtiest VA plane I have ever been on. Sad for a great airline.\n",
      "    - CT\n",
      "    - negative\n",
      "\n",
      "    - @VirginAmerica You should still develop an app - then you will be my favorite airline.\n",
      "    - CT\n",
      "    - neutral\n",
      "\n",
      "    - @VirginAmerica got it. All set - Thanks!\n",
      "    - CT\n",
      "    - positive\n",
      "\n",
      "    - @VirginAmerica Only thing I see on passbook is Virgin Mobile Mexico. How do I integrate?\n",
      "    - CT\n",
      "    - neutral\n",
      "\n",
      "    - @VirginAmerica how come you don't have an iPhone app? Still using and making me waste paper.\n",
      "    - CT\n",
      "    - negative\n",
      "\n",
      "Airline: United\n",
      "  Most Active User: throthra\n",
      "  Tweets:\n",
      "    - @united well, you can't fix me missing my buddies 30th bday because of negligence but you can attempt to make up for it.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united why am I to believe they will help when customer service couldn't? Like I said, I want a number to someone who can fix what you did.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united but then again, maybe the @BBBNE_SD_KS_IA would care more to hear what went on than whoever listens to issues you caused\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united I wonder if sharing all this on FB and insta would produce a number. 140 characters really limits my story telling.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united so you're telling me there is no number to call after being left in an airport because of a negligent pilot and staff?\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united again I ask, who can I call to get this fixed? Tweeting me BS questions to stall only makes things worse. Phone number please.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united well, considering every agent before claimed they were unable to help with everything else, why waste more time to hear 'call corp'\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united not just refunded, but for those of us who are on vacation to get a free room night to make up for making us sleep in DIA\n",
      "    - nan\n",
      "    - positive\n",
      "\n",
      "    - @united I received my luggage that also looked to be left in the snow when I arrived. I'm asking for all 50 people to be refunded.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united to be clear on my luggage comment, I am referencing the photo attached. Flight 6232 to JAC http://t.co/PnBajfkmHG\n",
      "    - nan\n",
      "    - neutral\n",
      "\n",
      "    - @united what's a good number to call to speak with someone about how you can fix what you did to 50 people and their luggage on Saturday?\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united why de-ice before taxing? Maybe it #makestoomuchsense ? #shouldhaveflowndelta #unitedsucks @Delta @SouthwestAir @AmericanAir\n",
      "    - nan\n",
      "    - neutral\n",
      "\n",
      "    - @united right as we think we will take off, we stop to get de-iced. Why wasn't this done when the planes were sitting out all night?\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united they let us board again but will we fly this time? Who knows! #shouldhaveflowndelta\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united apparently sleeping in B terminal wasn't the worst situation. Someone told other UA passengers they had to sleep at baggage claim.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united Hey, thanks again for helping me miss my buddies 30th bday, you guys are really a trashy company #shouldhaveflowndelta #unitedsucks\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united hey, it's 4am, guess what I'm doing? Not sleeping! Why? Cause you care so little about your customers you let them sleep in airports\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united thanks for letting me sleep at DIA to ensure you ruin as much of my vacation as possible. Wait, no, fuck you. #unitedairlinessucks\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united oh, I'll be sharing alright. Especially about sleeping in this shitty airport and getting 1hr of sleep all night because UA.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united you can read the full story when I submit a case to your team about the pilot of flight 6232 and why we are sleeping in DIA\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united an inconvenience is a weather delay. Your pilot deciding to waste enough time so the FAA wouldn't let him fly is negligence.\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united more people stranded cause you suck. Better yet, you weasel out of Flight Booking Problems rooms for people claiming weather http://t.co/Flcnnn2USD\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united look at all the delta flights that landed while your pilots claimed they couldn't fly 6232 to JAC. http://t.co/6Kp4m0R1f7\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united look at the people tryn to sleep in the airport due to your shitty company not comping rooms after cxl flight http://t.co/o1u96Xc3bo\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united I hope your corporate office is ready to deal with the rage created by your shitty service and bullshit pilots. #UnitedAirlinesSucks\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united please fire the captain of flight 6232 today. He single handedly ruined every passengers day by being a piece of shit. #unitedsucks\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "    - @united why do you hire POS pilots? Thanks for ruining my trip and not allowing me to see my buddy as he turns 30. #unitedsucksdick\n",
      "    - nan\n",
      "    - negative\n",
      "\n",
      "Airline: Southwest\n",
      "  Most Active User: scoobydoo9749\n",
      "  Tweets:\n",
      "    - @SouthwestAir arrangements to reimburse me for the rental I had to get?\n",
      "    - Tallahassee, FL\n",
      "    - neutral\n",
      "\n",
      "    - @SouthwestAir i got the call from RDU that my baggage is there except that doesn't help me bc i can't get it until I'm flying back home.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir \"Will my luggage be on that flight?\" \"No\" \"Y not\" \"bc ur on that flight n it won't end up where ur goin http://t.co/6Zj6L2ZTua\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir \"...you in the 10 hrs we were hanging out there? Oh, no I understand things get crazy n sometimes 10 hours isn't long enough.\"\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir that's gotta be a new record: 4 hrs in the air, 11 hrs waiting and 0 bags delivered to destination.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir why am I still in Baltimore?! @delta is doing laps around us and laughing about it. # ridiculous\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir a week after Valentines day..not feeling the #LUV. Going on 11 hrs @BWI and don't my bags aren't even coming w me.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir  no such thing as a free flight. Gonna be spending $100 on board rental bc #swa couldn't get my baggage to greenville w me.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir still haven't left @BWI. Maybe by the time I'm suppose to fly back to Austin on Tuesday we'll have moved.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir coming up on 10 hrs and all at the gate, not leaving and without my baggage. SWA you are my nightmare!\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir finally boarded. Looks like I'll make it to my final destination but my baggage won't # baggagefail #bagsflyfreebutnotwithme\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir 20 passengers on this plane. I should've just grabbed my baggage and gave it its own seat.\n",
      "    - Tallahassee, FL\n",
      "    - neutral\n",
      "\n",
      "    - @SouthwestAir 50min to get bag checked n ATX, miss my flight, spend all day @BWI, n not get my baggage at the end of it all.  #epitimeoffail\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir 9 hours at this airport and you can't move a bag from one plane to another! #furious\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir bags fly free..just not to where you're going.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir 2 hrs to put a tag on my bag sayin it should go to greenville instead of Raleigh?! ARE YOU KIDDING ME?!\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir Am I flying on Spirit air?\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir 9 hrs in Baltimore, still not going to get my baggage to greenville w me. This is just unbelievable.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir the ball has been dropped. My snowboard will not be making it to my destination. #totalfail #letdown\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "    - @SouthwestAir but if my bag makes it with me to Greenville tonight then all is forgiven. #HighHopes\n",
      "    - Tallahassee, FL\n",
      "    - neutral\n",
      "\n",
      "    - @SouthwestAir not at all. Rerouted to snowy BWI been trying to figure out how to get out of here. Now dealing with more baggage troubles.\n",
      "    - Tallahassee, FL\n",
      "    - negative\n",
      "\n",
      "Missing values in 'tweet_location': 4733\n",
      "Missing values in 'user_timezone': 4820\n",
      "tweet_created type: datetime64[ns, pytz.FixedOffset(-480)]\n",
      "Total Philadelphia tweets: 72\n",
      "Unique variations: ['philadelphia, pa' 'los angeles, ca (via philly)' 'phila. pa'\n",
      " 'philadelphia pa ' 'philly' 'philadelphia/cali' 'philadelphia'\n",
      " 'philadelphia, pa usa' 'philly yo' 'philly area'\n",
      " 'philly, chicago, msp, vegas' 'philly to ny/nj' 'phila, princeton, nyc. '\n",
      " 'philadelphia suburbs']\n",
      "Subset saved to: ./airline_sentiment_confidence_above_0_6.csv, Total rows: 7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25148\\2459484260.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_data_cleaned['tweet_created'] = pd.to_datetime(tweets_data_cleaned['tweet_created'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './Tweets.csv'\n",
    "tweets_data = pd.read_csv(file_path)\n",
    "\n",
    "# 1. Find number of unique users and compute top-5 words from their tweets using TF-IDF\n",
    "unique_users = tweets_data['name'].nunique()\n",
    "print(f\"Number of unique users: {unique_users}\")\n",
    "\n",
    "# Create a dictionary to store top-5 words for each user\n",
    "user_top_words = {}\n",
    "for user in tweets_data['name'].unique():\n",
    "    user_tweets = tweets_data[tweets_data['name'] == user]['text']\n",
    "    if not user_tweets.empty:\n",
    "        tfidf = TfidfVectorizer(stop_words='english', max_features=5)\n",
    "        tfidf_matrix = tfidf.fit_transform(user_tweets)\n",
    "        top_words = tfidf.get_feature_names_out()\n",
    "        user_top_words[user] = top_words\n",
    "\n",
    "# 2. For each airline, find the most active users and their details\n",
    "# 2. For each airline, find the most active users and their details\n",
    "most_active_users_per_airline = {}\n",
    "for airline in tweets_data['airline'].unique():\n",
    "    airline_data = tweets_data[tweets_data['airline'] == airline]\n",
    "    most_active_user = airline_data['name'].value_counts().idxmax()\n",
    "    user_tweets = airline_data[airline_data['name'] == most_active_user]\n",
    "    user_details = {\n",
    "        \"tweets\": user_tweets['text'].tolist(),\n",
    "        \"locations\": user_tweets['tweet_location'].tolist(),\n",
    "        \"sentiments\": user_tweets['airline_sentiment'].tolist()\n",
    "    }\n",
    "    most_active_users_per_airline[airline] = {\n",
    "        \"user\": most_active_user,\n",
    "        \"details\": user_details\n",
    "    }\n",
    "\n",
    "# Display a sample result in a specific format\n",
    "most_active_users_sample = {airline: details for airline, details in list(most_active_users_per_airline.items())[:3]}\n",
    "\n",
    "# Print in the required format\n",
    "for airline, info in most_active_users_sample.items():\n",
    "    print(f\"Airline: {airline}\")\n",
    "    print(f\"  Most Active User: {info['user']}\")\n",
    "    print(f\"  Tweets:\")\n",
    "    for tweet, location, sentiment in zip(\n",
    "        info['details']['tweets'],  # Displaying first 5 tweets for brevity\n",
    "        info['details']['locations'],\n",
    "        info['details']['sentiments']\n",
    "    ):\n",
    "        print(f\"    - {tweet}\")\n",
    "        print(f\"    - {location}\")\n",
    "        print(f\"    - {sentiment}\")\n",
    "        print()\n",
    "\n",
    "# 3. Find missing values in 'tweet_location' and 'user_timezone'\n",
    "missing_tweet_location = tweets_data['tweet_location'].isnull().sum()\n",
    "missing_user_timezone = tweets_data['user_timezone'].isnull().sum()\n",
    "print(f\"Missing values in 'tweet_location': {missing_tweet_location}\")\n",
    "print(f\"Missing values in 'user_timezone': {missing_user_timezone}\")\n",
    "tweets_data_cleaned = tweets_data.dropna(subset=['tweet_location', 'user_timezone'])\n",
    "\n",
    "# 4. Parse 'tweet_created' as a date\n",
    "tweets_data_cleaned['tweet_created'] = pd.to_datetime(tweets_data_cleaned['tweet_created'], errors='coerce')\n",
    "print(f\"tweet_created type: {tweets_data_cleaned['tweet_created'].dtype}\")\n",
    "\n",
    "# 5. Find tweets from Philadelphia with variations in spelling\n",
    "philadelphia_variations = [\n",
    "    \"Philadelphia\", \"philadelphia\", \"Phila\", \"Philly\", \"philly\",\n",
    "    \"Phildelphia\", \"Phildelhia\", \"Philadephia\", \"Philadelpia\", \"Philadlephia\"\n",
    "]\n",
    "philadelphia_tweets = tweets_data_cleaned[\n",
    "    tweets_data_cleaned['tweet_location'].str.contains('|'.join(philadelphia_variations), na=False, case=False)\n",
    "]\n",
    "total_philadelphia_tweets = philadelphia_tweets.shape[0]\n",
    "unique_variations = philadelphia_tweets['tweet_location'].str.lower().unique()\n",
    "print(f\"Total Philadelphia tweets: {total_philadelphia_tweets}\")\n",
    "print(f\"Unique variations: {unique_variations}\")\n",
    "\n",
    "# 6. Create a subset with 'airline_sentiment_confidence' > 0.6\n",
    "confidence_subset = tweets_data_cleaned[tweets_data_cleaned['airline_sentiment_confidence'] > 0.6]\n",
    "subset_file_path = './airline_sentiment_confidence_above_0_6.csv'\n",
    "confidence_subset.to_csv(subset_file_path, index=False)\n",
    "print(f\"Subset saved to: {subset_file_path}, Total rows: {confidence_subset.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc2e57c-6b34-4544-bdb1-4be0a7f4db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 7701\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Display top-5 words for a sample user\u001b[39;00m\n\u001b[0;32m     31\u001b[0m sample_user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(user_top_words\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample User: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top-5 Words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_top_words[sample_user]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# # 2. For each airline, find the most active users and their details\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# most_active_users_per_airline = {}\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# for airline in tweets_data['airline'].unique():\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# confidence_subset.to_csv(subset_file_path, index=False)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# print(f\"Subset saved to: {subset_file_path}, Total rows: {confidence_subset.shape[0]}\")\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './Tweets.csv'\n",
    "tweets_data = pd.read_csv(file_path)\n",
    "\n",
    "# 1. Find number of unique users and compute top-5 words from their tweets using TF-IDF\n",
    "unique_users = tweets_data['name'].nunique()\n",
    "print(f\"Number of unique users: {unique_users}\")\n",
    "\n",
    "# Create a dictionary to store top-5 words for each user\n",
    "user_top_words = {}\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5)\n",
    "\n",
    "for user in tweets_data['name'].unique():\n",
    "    user_tweets = tweets_data[tweets_data['name'] == user]['text']\n",
    "    if not user_tweets.empty:\n",
    "        # Combine all tweets from the same user into one document\n",
    "        combined_tweets = \" \".join(user_tweets)\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([combined_tweets])  # Fit on combined tweets\n",
    "        top_words = tfidf_vectorizer.get_feature_names_out()\n",
    "        user_top_words[user] = top_words\n",
    "\n",
    "# Display top-5 words for a sample user\n",
    "sample_user = list(user_top_words.keys())[:10]\n",
    "print(f\"Sample User: {sample_user}, Top-5 Words: {user_top_words[sample_user]}\")\n",
    "\n",
    "# # 2. For each airline, find the most active users and their details\n",
    "# most_active_users_per_airline = {}\n",
    "# for airline in tweets_data['airline'].unique():\n",
    "#     airline_data = tweets_data[tweets_data['airline'] == airline]\n",
    "#     most_active_user = airline_data['name'].value_counts().idxmax()\n",
    "#     user_tweets = airline_data[airline_data['name'] == most_active_user]\n",
    "#     user_details = {\n",
    "#         \"tweets\": user_tweets['text'].tolist(),\n",
    "#         \"locations\": user_tweets['tweet_location'].tolist(),\n",
    "#         \"sentiments\": user_tweets['airline_sentiment'].tolist()\n",
    "#     }\n",
    "#     most_active_users_per_airline[airline] = {\n",
    "#         \"user\": most_active_user,\n",
    "#         \"details\": user_details\n",
    "#     }\n",
    "\n",
    "# # Display a sample result in a specific format\n",
    "# most_active_users_sample = {airline: details for airline, details in list(most_active_users_per_airline.items())[:3]}\n",
    "\n",
    "# # Print in the required format\n",
    "# for airline, info in most_active_users_sample.items():\n",
    "#     print(f\"Airline: {airline}\")\n",
    "#     print(f\"  Most Active User: {info['user']}\")\n",
    "#     print(f\"  Tweets:\")\n",
    "#     for tweet, location, sentiment in zip(\n",
    "#         info['details']['tweets'],  # Displaying first 5 tweets for brevity\n",
    "#         info['details']['locations'],\n",
    "#         info['details']['sentiments']\n",
    "#     ):\n",
    "#         print(f\"    - {tweet}\")\n",
    "#         print(f\"    - {location}\")\n",
    "#         print(f\"    - {sentiment}\")\n",
    "#         print()\n",
    "\n",
    "# # 3. Find missing values in 'tweet_location' and 'user_timezone'\n",
    "# missing_tweet_location = tweets_data['tweet_location'].isnull().sum()\n",
    "# missing_user_timezone = tweets_data['user_timezone'].isnull().sum()\n",
    "# print(f\"Missing values in 'tweet_location': {missing_tweet_location}\")\n",
    "# print(f\"Missing values in 'user_timezone': {missing_user_timezone}\")\n",
    "# tweets_data_cleaned = tweets_data.dropna(subset=['tweet_location', 'user_timezone'])\n",
    "\n",
    "# # 4. Parse 'tweet_created' as a date\n",
    "# tweets_data_cleaned['tweet_created'] = pd.to_datetime(tweets_data_cleaned['tweet_created'], errors='coerce')\n",
    "# print(f\"tweet_created type: {tweets_data_cleaned['tweet_created'].dtype}\")\n",
    "\n",
    "# # 5. Find tweets from Philadelphia with variations in spelling\n",
    "# philadelphia_variations = [\n",
    "#     \"Philadelphia\", \"philadelphia\", \"Phila\", \"Philly\", \"philly\",\n",
    "#     \"Phildelphia\", \"Phildelhia\", \"Philadephia\", \"Philadelpia\", \"Philadlephia\"\n",
    "# ]\n",
    "# philadelphia_tweets = tweets_data_cleaned[\n",
    "#     tweets_data_cleaned['tweet_location'].str.contains('|'.join(philadelphia_variations), na=False, case=False)\n",
    "# ]\n",
    "# total_philadelphia_tweets = philadelphia_tweets.shape[0]\n",
    "# unique_variations = philadelphia_tweets['tweet_location'].str.lower().unique()\n",
    "# print(f\"Total Philadelphia tweets: {total_philadelphia_tweets}\")\n",
    "# print(f\"Unique variations: {unique_variations}\")\n",
    "\n",
    "# # 6. Create a subset with 'airline_sentiment_confidence' > 0.6\n",
    "# confidence_subset = tweets_data_cleaned[tweets_data_cleaned['airline_sentiment_confidence'] > 0.6]\n",
    "# subset_file_path = './airline_sentiment_confidence_above_0_6_1206.csv'\n",
    "# confidence_subset.to_csv(subset_file_path, index=False)\n",
    "# print(f\"Subset saved to: {subset_file_path}, Total rows: {confidence_subset.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29f86ce-2a3a-4d78-8276-b263cfe439f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 7701\n",
      "\n",
      "Top-5 Words for First 5 Users:\n",
      "User: cairdin\n",
      "Top-5 Words: dhepburn, said, virginamerica\n",
      "\n",
      "User: jnardino\n",
      "Top-5 Words: bad, really, thing, va, virginamerica\n",
      "\n",
      "User: yvonnalynn\n",
      "Top-5 Words: didn, mean, need, today, trip\n",
      "\n",
      "User: cjmcginnis\n",
      "Top-5 Words: away, ear, fly, nearly, time\n",
      "\n",
      "User: pilot\n",
      "Top-5 Words: hats, https, men, missed, mwpg7grezp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './Tweets.csv'\n",
    "tweets_data = pd.read_csv(file_path)\n",
    "\n",
    "# Find number of unique users\n",
    "unique_users = tweets_data['name'].nunique()\n",
    "print(f\"Number of unique users: {unique_users}\")\n",
    "\n",
    "# Create a dictionary to store top-5 words for each user\n",
    "user_top_words = {}\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5)\n",
    "\n",
    "# Compute top-5 words for each unique user\n",
    "for user in tweets_data['name'].unique():\n",
    "    # Get all tweets from the current user\n",
    "    user_tweets = tweets_data[tweets_data['name'] == user]['text']\n",
    "    if not user_tweets.empty:\n",
    "        # Combine all tweets of the user into a single document\n",
    "        combined_tweets = \" \".join(user_tweets)\n",
    "        # Compute TF-IDF on the combined tweets\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([combined_tweets])\n",
    "        # Get top-5 words based on TF-IDF scores\n",
    "        top_words = tfidf_vectorizer.get_feature_names_out()\n",
    "        user_top_words[user] = top_words\n",
    "\n",
    "# Print top-5 words for the first 5 users\n",
    "print(\"\\nTop-5 Words for First 5 Users:\")\n",
    "for idx, (user, words) in enumerate(user_top_words.items()):\n",
    "    print(f\"User: {user}\")\n",
    "    print(f\"Top-5 Words: {', '.join(words)}\")\n",
    "    print()\n",
    "    if idx == 4:  # Stop after printing 5 users\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cd806-aae7-408f-bc6c-8146f3d1d77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
