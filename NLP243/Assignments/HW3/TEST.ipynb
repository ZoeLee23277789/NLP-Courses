{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407f7b1a-28e0-4170-9a69-5b2010a4fcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['sentence'],\n",
      "    num_rows: 42068\n",
      "}), Dataset({\n",
      "    features: ['sentence'],\n",
      "    num_rows: 3370\n",
      "}), Dataset({\n",
      "    features: ['sentence'],\n",
      "    num_rows: 3761\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "# 加載 Penn Treebank 資料集，允許執行自定義程式碼\n",
    "ptb = load_dataset('ptb_text_only', split=['train', 'validation', 'test'], trust_remote_code=True)\n",
    "\n",
    "# 查看資料集的結構\n",
    "print(ptb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10c6310-c8bd-4c5d-a88b-71fae202493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9616\n",
      "First 3 training examples (as word indices): [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 1, 1, 1], [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 30], [42, 29, 43, 44, 45, 29, 30, 46, 35, 47, 48, 49]]\n"
     ]
    }
   ],
   "source": [
    "# 1. 建立詞彙表\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    # 計算詞頻\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\w+', sentence.lower())  # 將句子轉換為小寫並用正則表達式分詞\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # 建立詞彙表，僅保留頻率大於等於 min_freq 的詞\n",
    "    vocab = {word: idx for idx, (word, count) in enumerate(counter.items(), start=2) if count >= min_freq}\n",
    "    \n",
    "    # 添加特殊符號 (e.g., PAD for padding, UNK for unknown tokens)\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# 2. 將句子格式化為詞彙索引\n",
    "def format_sentences(sentences, vocab):\n",
    "    formatted_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\w+', sentence.lower())\n",
    "        # 使用詞彙表將詞轉換為索引\n",
    "        indexed_sentence = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "        formatted_sentences.append(indexed_sentence)\n",
    "    return formatted_sentences\n",
    "\n",
    "# 將資料集中的句子提取出來\n",
    "train_sentences = [example['sentence'] for example in ptb[0]]\n",
    "validation_sentences = [example['sentence'] for example in ptb[1]]\n",
    "test_sentences = [example['sentence'] for example in ptb[2]]\n",
    "\n",
    "# 建立詞彙表 (基於訓練集)\n",
    "vocab = build_vocab(train_sentences)\n",
    "\n",
    "# 格式化句子\n",
    "train_data = format_sentences(train_sentences, vocab)\n",
    "validation_data = format_sentences(validation_sentences, vocab)\n",
    "test_data = format_sentences(test_sentences, vocab)\n",
    "\n",
    "# 確認處理後的數據\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"First 3 training examples (as word indices): {train_data[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289b1de5-e6f7-4817-941b-32f7e9e5e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始句子 1: aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "對應的詞彙索引: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 1, 1, 1]\n",
      "\n",
      "原始句子 2: pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "對應的詞彙索引: [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 30]\n",
      "\n",
      "原始句子 3: mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "對應的詞彙索引: [42, 29, 43, 44, 45, 29, 30, 46, 35, 47, 48, 49]\n",
      "\n",
      "原始句子 4: rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "對應的詞彙索引: [50, 29, 30, 31, 32, 51, 52, 44, 45, 53, 54, 55, 56, 57, 58, 38, 39, 40, 45, 59, 60, 61, 62]\n",
      "\n",
      "原始句子 5: a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n",
      "對應的詞彙索引: [38, 63, 45, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 38, 74, 75, 45, 76, 77, 78, 38, 49, 45, 79, 80, 67, 81, 82, 83, 30, 31, 84, 85, 86]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 打印前五筆資料的句子及其對應的詞彙索引\n",
    "for i in range(5):\n",
    "    sentence = train_sentences[i]\n",
    "    indexed_sentence = train_data[i]\n",
    "    print(f\"原始句子 {i+1}: {sentence}\")\n",
    "    print(f\"對應的詞彙索引: {indexed_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e777e1a1-f313-41a8-ae54-e8875cc8b800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: torch.Size([32, 98])\n",
      "First batch (as word indices): tensor([[8684, 8685, 3001,  ...,    0,    0,    0],\n",
      "        [ 285,  252,   42,  ...,    0,    0,    0],\n",
      "        [  38, 3130,  172,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  35, 9629, 2818,  ...,    0,    0,    0],\n",
      "        [1327,   90,  553,  ...,    0,    0,    0],\n",
      "        [  35,  192,  704,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 定義新的填充函數\n",
    "def collate_fn(batch):\n",
    "    # 將每個數據點的句子解包，因為 DataLoader 傳遞過來的是一個列表，列表中的每個元素是 TensorDataset 中的一個元組\n",
    "    sentences = [item[0] for item in batch]\n",
    "    # 使用 pad_sequence 將句子填充為相同長度\n",
    "    padded_batch = pad_sequence(sentences, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "    return padded_batch\n",
    "\n",
    "# 重新定義 DataLoader，這次使用修正的 collate_fn\n",
    "batch_size = 32\n",
    "\n",
    "# 使用 TensorDataset 包裝數據\n",
    "train_dataset = TensorDataset(torch.nn.utils.rnn.pad_sequence(train_tensors, batch_first=True, padding_value=vocab['<PAD>']))\n",
    "validation_dataset = TensorDataset(torch.nn.utils.rnn.pad_sequence(validation_tensors, batch_first=True, padding_value=vocab['<PAD>']))\n",
    "test_dataset = TensorDataset(torch.nn.utils.rnn.pad_sequence(test_tensors, batch_first=True, padding_value=vocab['<PAD>']))\n",
    "\n",
    "# 定義 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 打印一個批次來確認數據格式\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch size: {batch.shape}\")\n",
    "    print(f\"First batch (as word indices): {batch}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ee5591-cf7b-4a4a-a419-4f5aef520d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnvidia-smi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\memory.py:170\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 170\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3aa75c-7d12-4b97-837e-c1b6c6db8c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee093b66-d9d7-46d6-80ed-a047494bdcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(9616, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=9616, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8ba6861-0730-4aa1-9cfe-8fe23bf86ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of range index found: 9616\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Train data contains out of range indices!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m check_vocab_range(train_data, vocab_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain data contains out of range indices!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m check_vocab_range(validation_data, vocab_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation data contains out of range indices!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m check_vocab_range(test_data, vocab_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest data contains out of range indices!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Train data contains out of range indices!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 定義語言模型架構\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # 嵌入層\n",
    "        x = self.embedding(x)\n",
    "        # LSTM 層\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        # 全連接層\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # 初始化 LSTM 隱藏狀態和細胞狀態\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new_zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(device),\n",
    "                weight.new_zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(device))\n",
    "\n",
    "# 2. 定義模型超參數\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# 3. 建立模型、損失函數和優化器\n",
    "model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "\n",
    "# 移動模型到設備之前，檢查所有參數是否正常\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if torch.isnan(param).any():\n",
    "            raise ValueError(f\"Parameter {name} contains NaN values\")\n",
    "        if torch.isinf(param).any():\n",
    "            raise ValueError(f\"Parameter {name} contains infinite values\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 檢查詞彙索引範圍\n",
    "def check_vocab_range(data, vocab_size):\n",
    "    for sentence in data:\n",
    "        for idx in sentence:\n",
    "            if idx >= vocab_size or idx < 0:\n",
    "                print(f\"Out of range index found: {idx}\")\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "assert check_vocab_range(train_data, vocab_size), \"Train data contains out of range indices!\"\n",
    "assert check_vocab_range(validation_data, vocab_size), \"Validation data contains out of range indices!\"\n",
    "assert check_vocab_range(test_data, vocab_size), \"Test data contains out of range indices!\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # 確保批次數據和隱藏狀態位於正確設備上\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳播\n",
    "        hidden = tuple([each.to(device) for each in hidden])\n",
    "        output, hidden = model(batch, hidden)\n",
    "\n",
    "        # 調整 output 的形狀，使其適合 CrossEntropyLoss 的輸入\n",
    "        output = output.view(-1, vocab_size)\n",
    "        batch = batch.view(-1)\n",
    "\n",
    "        # 檢查 batch 是否在正確的範圍內且為 LongTensor\n",
    "        assert batch.dtype == torch.long, \"Batch tensor must be of type LongTensor\"\n",
    "        assert torch.max(batch) < vocab_size, \"Batch contains out-of-range indices\"\n",
    "\n",
    "        # 計算損失\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. 模型驗證\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(validation_loader):\n",
    "        # 確保批次數據和隱藏狀態位於正確設備上\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        hidden = tuple([each.to(device) for each in hidden])\n",
    "        output, hidden = model(batch, hidden)\n",
    "\n",
    "        # 調整 output 的形狀，使其適合 CrossEntropyLoss 的輸入\n",
    "        output = output.view(-1, vocab_size)\n",
    "        batch = batch.view(-1)\n",
    "\n",
    "        # 檢查 batch 是否在正確的範圍內且為 LongTensor\n",
    "        assert batch.dtype == torch.long, \"Batch tensor must be of type LongTensor\"\n",
    "        assert torch.max(batch) < vocab_size, \"Batch contains out-of-range indices\"\n",
    "\n",
    "        # 計算損失\n",
    "        loss = criterion(output, batch)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(validation_loader)\n",
    "print(f\"Validation Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "841d4f9b-5b2a-44ce-8055-37d45529ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of range index found in sentence 37973: 9616\n",
      "Sentence: [9616, 459, 49, 101, 81, 33, 1354, 37, 1105, 37, 30, 411, 112, 30, 45, 113, 814, 2272, 2273, 3488, 7008, 67, 740, 474, 67, 4661, 662, 98, 290, 38, 104, 2493, 3488, 5788]\n",
      "Train data contains out of range indices!\n",
      "Vocabulary size: 9616\n",
      "Sentence 0: aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "Indexed sentence: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 1, 1, 1]\n",
      "Sentence 1: pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "Indexed sentence: [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 30]\n",
      "Sentence 2: mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "Indexed sentence: [42, 29, 43, 44, 45, 29, 30, 46, 35, 47, 48, 49]\n",
      "Sentence 3: rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "Indexed sentence: [50, 29, 30, 31, 32, 51, 52, 44, 45, 53, 54, 55, 56, 57, 58, 38, 39, 40, 45, 59, 60, 61, 62]\n",
      "Sentence 4: a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n",
      "Indexed sentence: [38, 63, 45, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 38, 74, 75, 45, 76, 77, 78, 38, 49, 45, 79, 80, 67, 81, 82, 83, 30, 31, 84, 85, 86]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 加載 Penn Treebank 資料集\n",
    "ptb = load_dataset('ptb_text_only', split=['train', 'validation', 'test'], trust_remote_code=True)\n",
    "\n",
    "# 1. 建立詞彙表\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    # 計算詞頻\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\w+', sentence.lower())  # 將句子轉換為小寫並用正則表達式分詞\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # 添加特殊符號 (e.g., PAD for padding, UNK for unknown tokens)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # 建立詞彙表，僅保留頻率大於等於 min_freq 的詞，從索引 2 開始\n",
    "    for idx, (word, count) in enumerate(counter.items(), start=2):\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "# 2. 將句子格式化為詞彙索引\n",
    "def format_sentences(sentences, vocab):\n",
    "    formatted_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\w+', sentence.lower())\n",
    "        # 使用詞彙表將詞轉換為索引，如果詞不在詞彙表中，則使用 <UNK> 的索引\n",
    "        indexed_sentence = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "        formatted_sentences.append(indexed_sentence)\n",
    "    return formatted_sentences\n",
    "\n",
    "# 將資料集中的句子提取出來\n",
    "train_sentences = [example['sentence'] for example in ptb[0]]\n",
    "validation_sentences = [example['sentence'] for example in ptb[1]]\n",
    "test_sentences = [example['sentence'] for example in ptb[2]]\n",
    "\n",
    "# 建立詞彙表 (基於訓練集)\n",
    "vocab = build_vocab(train_sentences)\n",
    "\n",
    "# 格式化句子\n",
    "train_data = format_sentences(train_sentences, vocab)\n",
    "validation_data = format_sentences(validation_sentences, vocab)\n",
    "test_data = format_sentences(test_sentences, vocab)\n",
    "\n",
    "# 檢查詞彙索引範圍，並打印出錯誤的句子及其索引\n",
    "def check_vocab_range(data, vocab_size):\n",
    "    for sentence_idx, sentence in enumerate(data):\n",
    "        for idx in sentence:\n",
    "            if idx >= vocab_size or idx < 0:\n",
    "                # 打印出超出範圍的索引以及其對應的原始句子\n",
    "                print(f\"Out of range index found in sentence {sentence_idx}: {idx}\")\n",
    "                print(f\"Sentence: {data[sentence_idx]}\")\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# 確保詞彙表和數據的匹配\n",
    "try:\n",
    "    assert check_vocab_range(train_data, len(vocab)), \"Train data contains out of range indices!\"\n",
    "    assert check_vocab_range(validation_data, len(vocab)), \"Validation data contains out of range indices!\"\n",
    "    assert check_vocab_range(test_data, len(vocab)), \"Test data contains out of range indices!\"\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "\n",
    "# 打印詞彙表大小\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# 打印示例句子及其索引\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i}: {train_sentences[i]}\")\n",
    "    print(f\"Indexed sentence: {train_data[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb5b3a-d557-49e4-85d5-30c6d1b11949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
