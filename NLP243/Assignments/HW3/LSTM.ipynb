{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec9c2a1-c82d-40e1-a8f5-026d758062fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.5245, Validation Perplexity: 450.9675\n",
      "Epoch 2, Train Loss: 5.9175, Validation Perplexity: 331.0002\n",
      "Epoch 3, Train Loss: 5.6454, Validation Perplexity: 277.7363\n",
      "Epoch 4, Train Loss: 5.4629, Validation Perplexity: 247.3272\n",
      "Epoch 5, Train Loss: 5.3237, Validation Perplexity: 227.1906\n",
      "Epoch 6, Train Loss: 5.2104, Validation Perplexity: 213.3520\n",
      "Epoch 7, Train Loss: 5.1152, Validation Perplexity: 202.2631\n",
      "Epoch 8, Train Loss: 5.0331, Validation Perplexity: 194.3026\n",
      "Epoch 9, Train Loss: 4.9598, Validation Perplexity: 187.9996\n",
      "Epoch 10, Train Loss: 4.8942, Validation Perplexity: 182.9223\n",
      "Epoch 11, Train Loss: 4.8362, Validation Perplexity: 179.0538\n",
      "Epoch 12, Train Loss: 4.7829, Validation Perplexity: 175.5508\n",
      "Epoch 13, Train Loss: 4.7346, Validation Perplexity: 173.4530\n",
      "Epoch 14, Train Loss: 4.6902, Validation Perplexity: 171.4615\n",
      "Epoch 15, Train Loss: 4.6482, Validation Perplexity: 169.8248\n",
      "Epoch 16, Train Loss: 4.6086, Validation Perplexity: 168.6048\n",
      "Epoch 17, Train Loss: 4.5720, Validation Perplexity: 167.5580\n",
      "Epoch 18, Train Loss: 4.5381, Validation Perplexity: 167.3764\n",
      "Epoch 19, Train Loss: 4.5063, Validation Perplexity: 166.8300\n",
      "Epoch 20, Train Loss: 4.4749, Validation Perplexity: 166.2272\n",
      "Epoch 21, Train Loss: 4.4448, Validation Perplexity: 166.7726\n",
      "Epoch 22, Train Loss: 4.4167, Validation Perplexity: 166.2049\n",
      "Epoch 23, Train Loss: 4.3896, Validation Perplexity: 166.4010\n",
      "Epoch 24, Train Loss: 4.3632, Validation Perplexity: 166.4635\n",
      "Epoch 25, Train Loss: 4.3374, Validation Perplexity: 167.0562\n",
      "Epoch 26, Train Loss: 4.3132, Validation Perplexity: 167.8444\n",
      "Epoch 27, Train Loss: 4.2700, Validation Perplexity: 167.0138\n",
      "Early stopping triggered.\n",
      "Submission file 'LSTM_submission_output.csv' generated.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the dataset\n",
    "ptb = load_dataset('ptb_text_only', split=['train', 'validation', 'test'], trust_remote_code=True)\n",
    "\n",
    "# Tokenization and Vocabulary Building\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    counter = Counter()\n",
    "    for example in dataset:\n",
    "        tokens = tokenize(example['sentence'])\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "    vocab['<PAD>'] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(ptb[0])\n",
    "vocab_size = len(vocab)\n",
    "pad_token_idx = vocab['<PAD>']\n",
    "\n",
    "# Convert text to sequences of indices\n",
    "def encode_text(text, vocab):\n",
    "    return [vocab[word] for word in tokenize(text) if word in vocab]\n",
    "\n",
    "# Process each split\n",
    "train_data = [torch.tensor(encode_text(example['sentence'], vocab)) for example in ptb[0]]\n",
    "val_data = [torch.tensor(encode_text(example['sentence'], vocab)) for example in ptb[1]]\n",
    "test_data = [torch.tensor(encode_text(example['sentence'], vocab)) for example in ptb[2]]\n",
    "\n",
    "# DataLoader preparation\n",
    "def collate_batch(batch):\n",
    "    sequences = pad_sequence(batch, batch_first=True, padding_value=pad_token_idx)\n",
    "    return sequences[:, :-1], sequences[:, 1:]  # Inputs and targets\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_batch)  # Batch size 1 for sentence-level perplexity\n",
    "\n",
    "# Define the LSTM Language Model with dropout\n",
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, dropout=dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "# Model Initialization\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "model = LanguageModelLSTM(vocab_size, embedding_dim, hidden_dim, pad_token_idx, dropout_prob=0.5).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Calculate sentence-wise perplexities on the test set\n",
    "def evaluate_sentence_perplexities(model, dataloader, criterion, total_sentences):\n",
    "    model.eval()\n",
    "    sentence_perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, targets) in enumerate(dataloader):\n",
    "            if idx >= total_sentences:  # Stop after required sentences\n",
    "                break\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if inputs.size(1) == 0:\n",
    "                perplexity = -1  # Placeholder for empty sequence\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                perplexity = np.exp(loss.item())\n",
    "            sentence_perplexities.append((idx, perplexity))\n",
    "    return sentence_perplexities\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 50\n",
    "best_val_perplexity = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_perplexity = evaluate_model(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}\")\n",
    "    \n",
    "    # Update learning rate based on validation perplexity\n",
    "    scheduler.step(val_perplexity)\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_perplexity < best_val_perplexity:\n",
    "        best_val_perplexity = val_perplexity\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Evaluate on the test set and save sentence-wise perplexities\n",
    "total_sentences = 3761  # Expected number of test sentences\n",
    "test_perplexities = evaluate_sentence_perplexities(model, test_loader, criterion, total_sentences)\n",
    "\n",
    "# Save the perplexities to the required CSV submission file format\n",
    "with open(\"submission_output.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ID\", \"ppl\"])  # Header as per requirement\n",
    "    for idx, perplexity in test_perplexities:\n",
    "        writer.writerow([idx, perplexity])\n",
    "\n",
    "print(\"Submission file 'LSTM_submission_output.csv' generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52620e-c410-404b-832b-174f3f1f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN, LSTM, GRU, and Transformer models\n",
    "class LanguageModelRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        logits = self.fc(rnn_out)\n",
    "        return logits\n",
    "\n",
    "class LanguageModelGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        logits = self.fc(gru_out)\n",
    "        return logits\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embedding_dim))  # Support up to 512 tokens\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "# Model Initialization\n",
    "model_types = {\n",
    "    \"RNN\": LanguageModelRNN,\n",
    "    \"LSTM\": LanguageModelLSTM,\n",
    "    \"GRU\": LanguageModelGRU,\n",
    "    \"Transformer\": LanguageModelTransformer\n",
    "}\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout_prob = 0.5\n",
    "model_choice = \"Transformer\"  # Change to \"RNN\", \"LSTM\", or \"GRU\" to test other models\n",
    "\n",
    "if model_choice == \"Transformer\":\n",
    "    model = model_types[model_choice](vocab_size, embedding_dim, num_heads, num_layers, pad_token_idx, dropout_prob).to(device)\n",
    "else:\n",
    "    model = model_types[model_choice](vocab_size, embedding_dim, hidden_dim, pad_token_idx, dropout_prob).to(device)\n",
    "\n",
    "# Training and evaluation functions remain the same\n",
    "# ...\n",
    "\n",
    "# Start training the chosen model\n",
    "print(f\"Training {model_choice} model...\")\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_perplexity = evaluate_model(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_perplexity)\n",
    "    \n",
    "    if val_perplexity < best_val_perplexity:\n",
    "        best_val_perplexity = val_perplexity\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Evaluate and save test perplexities\n",
    "test_perplexities = evaluate_sentence_perplexities(model, test_loader, criterion, total_sentences)\n",
    "with open(f\"{model_choice}_submission_output.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ID\", \"ppl\"])\n",
    "    for idx, perplexity in test_perplexities:\n",
    "        writer.writerow([idx, perplexity])\n",
    "\n",
    "print(f\"Submission file '{model_choice}_submission_output.csv' generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e15ea5e-9ba5-4e33-a4b2-bfbbbea4ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 18:47:07,929] A new study created in memory with name: no-name-0231d9c9-3870-4a88-b1b1-7e2cec79978c\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\trial\\_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"embedding_dim\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 25, 'low': 50, 'high': 200}\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28784\\2520461970.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 18:49:27,079] Trial 0 finished with value: 219.2230314053985 and parameters: {'embedding_dim': 200, 'num_heads': 2, 'num_layers': 4, 'dropout_prob': 0.1, 'learning_rate': 0.00015025947000242787, 'batch_size': 32}. Best is trial 0 with value: 219.2230314053985.\n",
      "[W 2024-11-23 18:49:27,095] Trial 1 failed with parameters: {'embedding_dim': 175, 'num_heads': 4, 'num_layers': 2, 'dropout_prob': 0.5, 'learning_rate': 0.0016972383513934142, 'batch_size': 32} because of the following error: AssertionError('embed_dim must be divisible by num_heads').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28784\\2520461970.py\", line 117, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, model_type), n_trials=20)  # Adjust n_trials as needed\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28784\\2520461970.py\", line 74, in objective\n",
      "    model = LanguageModelTransformer(\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28784\\2520461970.py\", line 35, in __init__\n",
      "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dropout=dropout_prob)\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 590, in __init__\n",
      "    self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout,\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1020, in __init__\n",
      "    assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
      "AssertionError: embed_dim must be divisible by num_heads\n",
      "[W 2024-11-23 18:49:27,096] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing hyperparameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust n_trials as needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Save best trial and perplexity\u001b[39;00m\n\u001b[0;32m    120\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[6], line 117\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing hyperparameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Adjust n_trials as needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Save best trial and perplexity\u001b[39;00m\n\u001b[0;32m    120\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, model_type)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 74\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModelTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_prob\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m model_types[model_type]\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36mLanguageModelTransformer.__init__\u001b[1;34m(self, vocab_size, embedding_dim, num_heads, num_layers, pad_idx, dropout_prob)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim, padding_idx\u001b[38;5;241m=\u001b[39mpad_idx)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m, embedding_dim))  \u001b[38;5;66;03m# Support up to 512 tokens\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layer \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layer, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dim, vocab_size)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:590\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[0;32m    588\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1020\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m embed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[1;32m-> 1020\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim must be divisible by num_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define RNN, LSTM, GRU, and Transformer models\n",
    "class LanguageModelRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        logits = self.fc(rnn_out)\n",
    "        return logits\n",
    "\n",
    "class LanguageModelGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        logits = self.fc(gru_out)\n",
    "        return logits\n",
    "\n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embedding_dim))  # Support up to 512 tokens\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "# Model Initialization\n",
    "model_types = {\n",
    "    \"RNN\": LanguageModelRNN,\n",
    "    \"LSTM\": LanguageModelLSTM,\n",
    "    \"GRU\": LanguageModelGRU,\n",
    "    \"Transformer\": LanguageModelTransformer\n",
    "}\n",
    "model_results = {}\n",
    "\n",
    "\n",
    "def objective(trial, model_type):\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 50, 200, step=25)\n",
    "    \n",
    "    # For Transformer, ensure embedding_dim is divisible by num_heads\n",
    "    if model_type == \"Transformer\":\n",
    "        num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "        embedding_dim = trial.suggest_int(\"embedding_dim\", num_heads * 8, num_heads * 32, step=num_heads * 8)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 4, step=1)\n",
    "    else:\n",
    "        num_heads = None\n",
    "        num_layers = None\n",
    "\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256, step=32) if model_type != \"Transformer\" else None\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Initialize model\n",
    "    if model_type == \"Transformer\":\n",
    "        model = LanguageModelTransformer(\n",
    "            vocab_size, embedding_dim, num_heads, num_layers, pad_token_idx, dropout_prob\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model_class = model_types[model_type]\n",
    "        model = model_class(vocab_size, embedding_dim, hidden_dim, pad_token_idx, dropout_prob).to(device)\n",
    "    \n",
    "    # Update DataLoader with suggested batch size\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
    "\n",
    "    # Training for a few epochs (to save time during hyperparameter tuning)\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "    # Evaluate validation perplexity\n",
    "    val_perplexity = evaluate_model(model, val_loader, criterion)\n",
    "    return val_perplexity\n",
    "\n",
    "\n",
    "# Run Optuna for each model type\n",
    "for model_type in [\"RNN\", \"LSTM\", \"GRU\", \"Transformer\"]:\n",
    "    print(f\"Optimizing hyperparameters for {model_type}...\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_type), n_trials=20)  # Adjust n_trials as needed\n",
    "    \n",
    "    # Save best trial and perplexity\n",
    "    best_trial = study.best_trial\n",
    "    model_results[model_type] = {\n",
    "        \"best_params\": best_trial.params,\n",
    "        \"best_val_perplexity\": best_trial.value\n",
    "    }\n",
    "    print(f\"Best hyperparameters for {model_type}: {best_trial.params}\")\n",
    "    print(f\"Best validation perplexity for {model_type}: {best_trial.value}\")\n",
    "Run Optuna for each model type\n",
    "\n",
    "# Compare results and find the best model\n",
    "best_model = min(model_results, key=lambda x: model_results[x][\"best_val_perplexity\"])\n",
    "print(\"\\nSummary of results:\")\n",
    "for model_type, result in model_results.items():\n",
    "    print(f\"{model_type}: Best perplexity = {result['best_val_perplexity']}, Best params = {result['best_params']}\")\n",
    "print(f\"\\nBest model: {best_model} with perplexity {model_results[best_model]['best_val_perplexity']}\")\n",
    "\n",
    "# Train and evaluate the best model with its optimal hyperparameters\n",
    "best_params = model_results[best_model][\"best_params\"]\n",
    "if best_model == \"Transformer\":\n",
    "    model = LanguageModelTransformer(\n",
    "        vocab_size,\n",
    "        best_params[\"embedding_dim\"],\n",
    "        best_params[\"num_heads\"],\n",
    "        best_params[\"num_layers\"],\n",
    "        pad_token_idx,\n",
    "        best_params[\"dropout_prob\"]\n",
    "    ).to(device)\n",
    "else:\n",
    "    model_class = model_types[best_model]\n",
    "    model = model_class(\n",
    "        vocab_size,\n",
    "        best_params[\"embedding_dim\"],\n",
    "        best_params[\"hidden_dim\"],\n",
    "        pad_token_idx,\n",
    "        best_params[\"dropout_prob\"]\n",
    "    ).to(device)\n",
    "\n",
    "# Recreate loaders with optimal batch size\n",
    "train_loader = DataLoader(train_data, batch_size=best_params[\"batch_size\"], shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=best_params[\"batch_size\"], shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Train the best model\n",
    "optimizer = AdamW(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
    "epochs = 50  # Train for more epochs with optimal hyperparameters\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_perplexity = evaluate_model(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}\")\n",
    "\n",
    "print(f\"Final Validation Perplexity of Best Model ({best_model}): {val_perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d10fb-46be-427a-b7c7-f98a5f6b9a23",
   "metadata": {},
   "source": [
    "Best hyperparameters for RNN: {'embedding_dim': 150, 'hidden_dim': 128, 'dropout_prob': 0.5, 'learning_rate': 0.0016579694418827292, 'batch_size': 64}\n",
    "Best validation perplexity for RNN: 173.18619457496038\n",
    "Optimizing hyperparameters for LSTM...\n",
    "Best hyperparameters for LSTM: {'embedding_dim': 175, 'hidden_dim': 256, 'dropout_prob': 0.2, 'learning_rate': 0.00048762132730701416, 'batch_size': 16}\n",
    "Best validation perplexity for LSTM: 153.97874027835306\n",
    "Optimizing hyperparameters for GRU...\n",
    "Best hyperparameters for GRU: {'embedding_dim': 75, 'hidden_dim': 192, 'dropout_prob': 0.5, 'learning_rate': 0.001014428519610825, 'batch_size': 16}\n",
    "Best validation perplexity for GRU: 156.944322817553\n",
    "Optimizing hyperparameters for Transformer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b1b1ba-fd13-4c3c-86a8-b2dc5283164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 18:56:53,846] A new study created in memory with name: no-name-dfef2af9-4732-43dc-9aec-ec7b7fd779e0\n",
      "[I 2024-11-23 18:58:13,063] Trial 0 finished with value: 240.25912475585938 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 1, 'dropout_prob': 0.1, 'learning_rate': 0.00040652048652476926, 'batch_size': 16}. Best is trial 0 with value: 240.25912475585938.\n",
      "[I 2024-11-23 18:58:58,324] Trial 1 finished with value: 229.2498779296875 and parameters: {'num_heads': 4, 'embedding_dim': 96, 'num_layers': 1, 'dropout_prob': 0.5, 'learning_rate': 0.004776870580403055, 'batch_size': 32}. Best is trial 1 with value: 229.2498779296875.\n",
      "[I 2024-11-23 18:59:40,809] Trial 2 finished with value: 221.57119750976562 and parameters: {'num_heads': 4, 'embedding_dim': 96, 'num_layers': 1, 'dropout_prob': 0.1, 'learning_rate': 0.0026013361040733095, 'batch_size': 64}. Best is trial 2 with value: 221.57119750976562.\n",
      "[I 2024-11-23 19:01:44,482] Trial 3 finished with value: 217.09588623046875 and parameters: {'num_heads': 8, 'embedding_dim': 256, 'num_layers': 3, 'dropout_prob': 0.30000000000000004, 'learning_rate': 0.00030309817420584654, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:02:51,429] Trial 4 finished with value: 415.4204406738281 and parameters: {'num_heads': 4, 'embedding_dim': 32, 'num_layers': 4, 'dropout_prob': 0.2, 'learning_rate': 0.00014819216681701764, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:04:14,641] Trial 5 finished with value: 229.5494842529297 and parameters: {'num_heads': 6, 'embedding_dim': 96, 'num_layers': 1, 'dropout_prob': 0.30000000000000004, 'learning_rate': 0.0011453801157520844, 'batch_size': 16}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:07:10,016] Trial 6 finished with value: 249.7536163330078 and parameters: {'num_heads': 2, 'embedding_dim': 48, 'num_layers': 3, 'dropout_prob': 0.1, 'learning_rate': 0.0003182797795762156, 'batch_size': 16}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:08:41,721] Trial 7 finished with value: 302.2333068847656 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 3, 'dropout_prob': 0.2, 'learning_rate': 0.00013661058965366323, 'batch_size': 32}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:10:07,776] Trial 8 finished with value: 224.1119384765625 and parameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 1, 'dropout_prob': 0.2, 'learning_rate': 0.0003450860821962858, 'batch_size': 16}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:10:48,326] Trial 9 finished with value: 406.69879150390625 and parameters: {'num_heads': 2, 'embedding_dim': 48, 'num_layers': 1, 'dropout_prob': 0.4, 'learning_rate': 0.00011260288284890038, 'batch_size': 32}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:13:28,182] Trial 10 finished with value: 229.95297241210938 and parameters: {'num_heads': 8, 'embedding_dim': 256, 'num_layers': 4, 'dropout_prob': 0.4, 'learning_rate': 0.0012401242740615818, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:14:31,380] Trial 11 finished with value: 225.68051147460938 and parameters: {'num_heads': 4, 'embedding_dim': 128, 'num_layers': 2, 'dropout_prob': 0.30000000000000004, 'learning_rate': 0.0036543094433350892, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:15:47,153] Trial 12 finished with value: 759.1090087890625 and parameters: {'num_heads': 6, 'embedding_dim': 144, 'num_layers': 2, 'dropout_prob': 0.4, 'learning_rate': 0.00894981367293028, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:17:06,877] Trial 13 finished with value: 224.64700317382812 and parameters: {'num_heads': 4, 'embedding_dim': 128, 'num_layers': 3, 'dropout_prob': 0.1, 'learning_rate': 0.002620287681867652, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:18:32,929] Trial 14 finished with value: 218.5654296875 and parameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 2, 'dropout_prob': 0.30000000000000004, 'learning_rate': 0.0006286020306306355, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:20:17,639] Trial 15 finished with value: 221.40390014648438 and parameters: {'num_heads': 8, 'embedding_dim': 256, 'num_layers': 2, 'dropout_prob': 0.30000000000000004, 'learning_rate': 0.000633716324231258, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:22:07,163] Trial 16 finished with value: 226.79933166503906 and parameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 3, 'dropout_prob': 0.5, 'learning_rate': 0.0006346682051879843, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:23:35,009] Trial 17 finished with value: 241.45782470703125 and parameters: {'num_heads': 8, 'embedding_dim': 192, 'num_layers': 2, 'dropout_prob': 0.4, 'learning_rate': 0.00021698845255552508, 'batch_size': 64}. Best is trial 3 with value: 217.09588623046875.\n",
      "[I 2024-11-23 19:25:46,361] Trial 18 finished with value: 214.95179748535156 and parameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 4, 'dropout_prob': 0.2, 'learning_rate': 0.0007258591107050929, 'batch_size': 64}. Best is trial 18 with value: 214.95179748535156.\n",
      "[I 2024-11-23 19:28:37,876] Trial 19 finished with value: 769.8042602539062 and parameters: {'num_heads': 8, 'embedding_dim': 256, 'num_layers': 4, 'dropout_prob': 0.2, 'learning_rate': 0.0015503919858785966, 'batch_size': 32}. Best is trial 18 with value: 214.95179748535156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 4, 'dropout_prob': 0.2, 'learning_rate': 0.0007258591107050929, 'batch_size': 64}\n",
      "Best validation perplexity: 214.95179748535156\n",
      "LanguageModelTransformer(\n",
      "  (embedding): Embedding(9644, 192, padding_idx=9643)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=192, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=192, bias=True)\n",
      "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=192, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=192, bias=True)\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=192, out_features=9644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformer \n",
    "class LanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, pad_idx, dropout_prob=0.5):\n",
    "        super(LanguageModelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embedding_dim))  # 512 tokens\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "# \n",
    "def objective(trial):\n",
    "    # Transformer \n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", num_heads * 8, num_heads * 32, step=num_heads * 8)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4, step=1)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # \n",
    "    model = LanguageModelTransformer(\n",
    "        vocab_size, embedding_dim, num_heads, num_layers, pad_token_idx, dropout_prob\n",
    "    ).to(device)\n",
    "\n",
    "    #  DataLoader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #  epoch\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "# Optuna \n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)  #  n_trials \n",
    "\n",
    "# \n",
    "best_trial = study.best_trial\n",
    "print(\"Best hyperparameters:\", best_trial.params)\n",
    "print(\"Best validation perplexity:\", best_trial.value)\n",
    "\n",
    "# \n",
    "best_params = best_trial.params\n",
    "final_model = LanguageModelTransformer(\n",
    "    vocab_size,\n",
    "    best_params[\"embedding_dim\"],\n",
    "    best_params[\"num_heads\"],\n",
    "    best_params[\"num_layers\"],\n",
    "    pad_token_idx,\n",
    "    best_params[\"dropout_prob\"]\n",
    ").to(device)\n",
    "\n",
    "# \n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a19ad9-b37b-4a1d-959e-48007e4ad3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RNN on the test set...\n",
      "Average Loss: 9.2183\n",
      "Perplexity: 10079.7970\n",
      "RNN Overall Test Perplexity: 10079.80\n",
      "Evaluating LSTM on the test set...\n",
      "Average Loss: 9.1765\n",
      "Perplexity: 9667.7301\n",
      "LSTM Overall Test Perplexity: 9667.73\n",
      "Evaluating GRU on the test set...\n",
      "Average Loss: 9.1799\n",
      "Perplexity: 9700.6330\n",
      "GRU Overall Test Perplexity: 9700.63\n",
      "Evaluating Transformer on the test set...\n",
      "Average Loss: 9.3371\n",
      "Perplexity: 11351.5771\n",
      "Transformer Overall Test Perplexity: 11351.58\n",
      "\n",
      "Test Set Perplexity Results:\n",
      "RNN: 10079.80\n",
      "LSTM: 9667.73\n",
      "GRU: 9700.63\n",
      "Transformer: 11351.58\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# \n",
    "def calculate_test_perplexity(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Calculate overall perplexity using CrossEntropyLoss and exp.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Skip empty sequences\n",
    "            if inputs.size(1) == 0:\n",
    "                continue\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "            # Accumulate loss and token count\n",
    "            total_loss += loss.item() * targets.numel()\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    if total_tokens > 0:\n",
    "        average_loss = total_loss / total_tokens\n",
    "        perplexity = np.exp(average_loss)\n",
    "        print(f\"Average Loss: {average_loss:.4f}\")\n",
    "        print(f\"Perplexity: {perplexity:.4f}\")\n",
    "        return perplexity\n",
    "    else:\n",
    "        return float('inf')  # Handle edge case where there are no tokens\n",
    "\n",
    "#  DataLoader\n",
    "def create_test_loader(test_data, batch_size):\n",
    "    def collate_batch(batch):\n",
    "        sequences = pad_sequence(batch, batch_first=True, padding_value=pad_token_idx)\n",
    "        return sequences[:, :-1], sequences[:, 1:]  # Inputs and targets\n",
    "\n",
    "    return DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# \n",
    "best_params_dict = {\n",
    "    \"RNN\": {\n",
    "        \"embedding_dim\": 150,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"dropout_prob\": 0.5,\n",
    "        \"learning_rate\": 0.0016579694418827292,\n",
    "        \"batch_size\": 64,\n",
    "    },\n",
    "    \"LSTM\": {\n",
    "        \"embedding_dim\": 175,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout_prob\": 0.2,\n",
    "        \"learning_rate\": 0.00048762132730701416,\n",
    "        \"batch_size\": 16,\n",
    "    },\n",
    "    \"GRU\": {\n",
    "        \"embedding_dim\": 75,\n",
    "        \"hidden_dim\": 192,\n",
    "        \"dropout_prob\": 0.5,\n",
    "        \"learning_rate\": 0.001014428519610825,\n",
    "        \"batch_size\": 16,\n",
    "    },\n",
    "    \"Transformer\": {\n",
    "        \"num_heads\": 6,\n",
    "        \"embedding_dim\": 192,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout_prob\": 0.2,\n",
    "        \"learning_rate\": 0.0007258591107050929,\n",
    "        \"batch_size\": 64,\n",
    "    },\n",
    "}\n",
    "\n",
    "# \n",
    "model_classes = {\n",
    "    \"RNN\": LanguageModelRNN,\n",
    "    \"LSTM\": LanguageModelLSTM,\n",
    "    \"GRU\": LanguageModelGRU,\n",
    "    \"Transformer\": LanguageModelTransformer,\n",
    "}\n",
    "\n",
    "# \n",
    "test_results = {}\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
    "\n",
    "for model_name, params in best_params_dict.items():\n",
    "    print(f\"Evaluating {model_name} on the test set...\")\n",
    "\n",
    "    if model_name == \"Transformer\":\n",
    "        model = model_classes[model_name](\n",
    "            vocab_size,\n",
    "            params[\"embedding_dim\"],\n",
    "            params[\"num_heads\"],\n",
    "            params[\"num_layers\"],\n",
    "            pad_token_idx,\n",
    "            params[\"dropout_prob\"],\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = model_classes[model_name](\n",
    "            vocab_size,\n",
    "            params[\"embedding_dim\"],\n",
    "            params[\"hidden_dim\"],\n",
    "            pad_token_idx,\n",
    "            params[\"dropout_prob\"],\n",
    "        ).to(device)\n",
    "\n",
    "    #  DataLoader\n",
    "    test_loader = create_test_loader(test_data, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # \n",
    "    overall_perplexity = calculate_test_perplexity(model, test_loader, criterion)\n",
    "    test_results[model_name] = overall_perplexity\n",
    "    print(f\"{model_name} Overall Test Perplexity: {overall_perplexity:.2f}\")\n",
    "\n",
    "# \n",
    "print(\"\\nTest Set Perplexity Results:\")\n",
    "for model_name, perplexity in test_results.items():\n",
    "    print(f\"{model_name}: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518167c-e7ae-415b-bc0a-51691bdab33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
