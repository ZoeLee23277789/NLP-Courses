{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf805671-28a3-4d41-b796-d5e21e19ac7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的設備: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "\n",
    "# 檢查是否有 GPU 可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用的設備:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74600ba-ecbc-4428-89d4-9df5416f317d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                         utterances  \\\n",
      "0        1               who plays luke on star wars new hope   \n",
      "1        2                     show credits for the godfather   \n",
      "2        3             who was the main actor in the exorcist   \n",
      "3        4  find the female actress from the movie she 's ...   \n",
      "4        5                    who played dory on finding nemo   \n",
      "...    ...                                                ...   \n",
      "2307  2308               what was the revenue for toy story 3   \n",
      "2308  2309                                dark knight revenue   \n",
      "2309  2310               how much did the dark night generate   \n",
      "2310  2311                 can i see the lion king 's revenue   \n",
      "2311  2312        can i see what the lion king 's revenue was   \n",
      "\n",
      "                                      IOB Slot tags  \n",
      "0      O O B_char O B_movie I_movie I_movie I_movie  \n",
      "1                             O O O B_movie I_movie  \n",
      "2                       O O O O O O B_movie I_movie  \n",
      "3     O O O O O O O B_movie I_movie I_movie I_movie  \n",
      "4                      O O B_char O B_movie I_movie  \n",
      "...                                             ...  \n",
      "2307              O O O O O B_movie I_movie I_movie  \n",
      "2308                              B_movie I_movie O  \n",
      "2309                O O O B_movie I_movie I_movie O  \n",
      "2310              O O O B_movie I_movie I_movie O O  \n",
      "2311          O O O O B_movie I_movie I_movie O O O  \n",
      "\n",
      "[2312 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_file_path = r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Assignments\\HW2\\hw2_train.csv\"\n",
    "\n",
    "# Load the file\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e92355-6325-43de-8d69-a34856510ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用 CountVectorizer 將文本數據轉換為詞袋表示\n",
    "vectorizer = CountVectorizer(max_features=1000)  # 限制特徵數量為 1000\n",
    "X = vectorizer.fit_transform(train_df['utterances'])  # 假設 CSV 中有 'sentence' 列\n",
    "X = X.toarray()  # 將稀疏矩陣轉換為密集矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bc68a1-7f05-4390-83c2-b22a246adab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 假設我們有標籤列 'tags'，並進行標籤二值化\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(train_df['IOB Slot tags'].apply(lambda x: x.split()))  # 將標籤列轉換為列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6050ed03-8304-4c2d-888f-65e168d31657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 將數據分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f193c3-fc50-49c1-aab2-f1bd333ac119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 將數據轉換為 PyTorch 張量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbca54a-e729-4a4c-ba6d-3c2857f70c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 創建數據集和數據加載器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34054a86-109d-42a3-bc5a-aa015516009e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SlotTaggingModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SlotTaggingModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)  # 对输出进行softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = X_train.shape[1]  # 詞袋模型的特徵數量\n",
    "hidden_dim = 128  # 隱藏層的大小，可以根據需求調整\n",
    "output_dim = y_train.shape[1]  # 標籤的數量\n",
    "\n",
    "model = SlotTaggingModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.BCELoss()  # 二元交叉熵損失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c222ef66-d636-4f27-9052-af400796338e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 0.1000, Test Loss: 0.1125\n",
      "Epoch 2/500, Train Loss: 0.0982, Test Loss: 0.1122\n",
      "Epoch 3/500, Train Loss: 0.0970, Test Loss: 0.1113\n",
      "Epoch 4/500, Train Loss: 0.0959, Test Loss: 0.1113\n",
      "Epoch 5/500, Train Loss: 0.0951, Test Loss: 0.1113\n",
      "Epoch 6/500, Train Loss: 0.0942, Test Loss: 0.1115\n",
      "Epoch 7/500, Train Loss: 0.0936, Test Loss: 0.1114\n",
      "Epoch 8/500, Train Loss: 0.0930, Test Loss: 0.1112\n",
      "Epoch 9/500, Train Loss: 0.0925, Test Loss: 0.1114\n",
      "Epoch 10/500, Train Loss: 0.0921, Test Loss: 0.1115\n",
      "Epoch 11/500, Train Loss: 0.0916, Test Loss: 0.1118\n",
      "Epoch 12/500, Train Loss: 0.0913, Test Loss: 0.1120\n",
      "Epoch 13/500, Train Loss: 0.0911, Test Loss: 0.1122\n",
      "Epoch 14/500, Train Loss: 0.0907, Test Loss: 0.1125\n",
      "Epoch 15/500, Train Loss: 0.0905, Test Loss: 0.1127\n",
      "Epoch 16/500, Train Loss: 0.0903, Test Loss: 0.1128\n",
      "Epoch 17/500, Train Loss: 0.0901, Test Loss: 0.1133\n",
      "Epoch 18/500, Train Loss: 0.0899, Test Loss: 0.1135\n",
      "Epoch 19/500, Train Loss: 0.0898, Test Loss: 0.1139\n",
      "Epoch 20/500, Train Loss: 0.0896, Test Loss: 0.1147\n",
      "Epoch 21/500, Train Loss: 0.0894, Test Loss: 0.1141\n",
      "Epoch 22/500, Train Loss: 0.0893, Test Loss: 0.1147\n",
      "Epoch 23/500, Train Loss: 0.0892, Test Loss: 0.1143\n",
      "Epoch 24/500, Train Loss: 0.0891, Test Loss: 0.1151\n",
      "Epoch 25/500, Train Loss: 0.0890, Test Loss: 0.1156\n",
      "Epoch 26/500, Train Loss: 0.0890, Test Loss: 0.1159\n",
      "Epoch 27/500, Train Loss: 0.0888, Test Loss: 0.1164\n",
      "Epoch 28/500, Train Loss: 0.0888, Test Loss: 0.1164\n",
      "Epoch 29/500, Train Loss: 0.0887, Test Loss: 0.1163\n",
      "Epoch 30/500, Train Loss: 0.0886, Test Loss: 0.1170\n",
      "Epoch 31/500, Train Loss: 0.0885, Test Loss: 0.1170\n",
      "Epoch 32/500, Train Loss: 0.0884, Test Loss: 0.1174\n",
      "Epoch 33/500, Train Loss: 0.0884, Test Loss: 0.1172\n",
      "Epoch 34/500, Train Loss: 0.0884, Test Loss: 0.1173\n",
      "Epoch 35/500, Train Loss: 0.0883, Test Loss: 0.1176\n",
      "Epoch 36/500, Train Loss: 0.0882, Test Loss: 0.1181\n",
      "Epoch 37/500, Train Loss: 0.0882, Test Loss: 0.1187\n",
      "Epoch 38/500, Train Loss: 0.0882, Test Loss: 0.1185\n",
      "Epoch 39/500, Train Loss: 0.0882, Test Loss: 0.1190\n",
      "Epoch 40/500, Train Loss: 0.0881, Test Loss: 0.1187\n",
      "Epoch 41/500, Train Loss: 0.0881, Test Loss: 0.1197\n",
      "Epoch 42/500, Train Loss: 0.0880, Test Loss: 0.1191\n",
      "Epoch 43/500, Train Loss: 0.0880, Test Loss: 0.1198\n",
      "Epoch 44/500, Train Loss: 0.0879, Test Loss: 0.1203\n",
      "Epoch 45/500, Train Loss: 0.0879, Test Loss: 0.1210\n",
      "Epoch 46/500, Train Loss: 0.0879, Test Loss: 0.1203\n",
      "Epoch 47/500, Train Loss: 0.0878, Test Loss: 0.1203\n",
      "Epoch 48/500, Train Loss: 0.0878, Test Loss: 0.1204\n",
      "Epoch 49/500, Train Loss: 0.0878, Test Loss: 0.1217\n",
      "Epoch 50/500, Train Loss: 0.0877, Test Loss: 0.1208\n",
      "Epoch 51/500, Train Loss: 0.0878, Test Loss: 0.1216\n",
      "Epoch 52/500, Train Loss: 0.0877, Test Loss: 0.1216\n",
      "Epoch 53/500, Train Loss: 0.0877, Test Loss: 0.1221\n",
      "Epoch 54/500, Train Loss: 0.0877, Test Loss: 0.1223\n",
      "Epoch 55/500, Train Loss: 0.0876, Test Loss: 0.1220\n",
      "Epoch 56/500, Train Loss: 0.0876, Test Loss: 0.1221\n",
      "Epoch 57/500, Train Loss: 0.0876, Test Loss: 0.1228\n",
      "Epoch 58/500, Train Loss: 0.0876, Test Loss: 0.1225\n",
      "Epoch 59/500, Train Loss: 0.0877, Test Loss: 0.1224\n",
      "Epoch 60/500, Train Loss: 0.0876, Test Loss: 0.1231\n",
      "Epoch 61/500, Train Loss: 0.0876, Test Loss: 0.1240\n",
      "Epoch 62/500, Train Loss: 0.0876, Test Loss: 0.1237\n",
      "Epoch 63/500, Train Loss: 0.0876, Test Loss: 0.1242\n",
      "Epoch 64/500, Train Loss: 0.0875, Test Loss: 0.1239\n",
      "Epoch 65/500, Train Loss: 0.0874, Test Loss: 0.1241\n",
      "Epoch 66/500, Train Loss: 0.0874, Test Loss: 0.1239\n",
      "Epoch 67/500, Train Loss: 0.0875, Test Loss: 0.1250\n",
      "Epoch 68/500, Train Loss: 0.0875, Test Loss: 0.1244\n",
      "Epoch 69/500, Train Loss: 0.0875, Test Loss: 0.1250\n",
      "Epoch 70/500, Train Loss: 0.0874, Test Loss: 0.1246\n",
      "Epoch 71/500, Train Loss: 0.0874, Test Loss: 0.1250\n",
      "Epoch 72/500, Train Loss: 0.0874, Test Loss: 0.1254\n",
      "Epoch 73/500, Train Loss: 0.0874, Test Loss: 0.1258\n",
      "Epoch 74/500, Train Loss: 0.0875, Test Loss: 0.1251\n",
      "Epoch 75/500, Train Loss: 0.0874, Test Loss: 0.1258\n",
      "Epoch 76/500, Train Loss: 0.0874, Test Loss: 0.1257\n",
      "Epoch 77/500, Train Loss: 0.0873, Test Loss: 0.1261\n",
      "Epoch 78/500, Train Loss: 0.0873, Test Loss: 0.1270\n",
      "Epoch 79/500, Train Loss: 0.0873, Test Loss: 0.1264\n",
      "Epoch 80/500, Train Loss: 0.0874, Test Loss: 0.1268\n",
      "Epoch 81/500, Train Loss: 0.0873, Test Loss: 0.1273\n",
      "Epoch 82/500, Train Loss: 0.0874, Test Loss: 0.1265\n",
      "Epoch 83/500, Train Loss: 0.0873, Test Loss: 0.1264\n",
      "Epoch 84/500, Train Loss: 0.0874, Test Loss: 0.1273\n",
      "Epoch 85/500, Train Loss: 0.0872, Test Loss: 0.1271\n",
      "Epoch 86/500, Train Loss: 0.0873, Test Loss: 0.1279\n",
      "Epoch 87/500, Train Loss: 0.0873, Test Loss: 0.1277\n",
      "Epoch 88/500, Train Loss: 0.0873, Test Loss: 0.1277\n",
      "Epoch 89/500, Train Loss: 0.0872, Test Loss: 0.1278\n",
      "Epoch 90/500, Train Loss: 0.0873, Test Loss: 0.1280\n",
      "Epoch 91/500, Train Loss: 0.0872, Test Loss: 0.1283\n",
      "Epoch 92/500, Train Loss: 0.0872, Test Loss: 0.1283\n",
      "Epoch 93/500, Train Loss: 0.0872, Test Loss: 0.1283\n",
      "Epoch 94/500, Train Loss: 0.0871, Test Loss: 0.1288\n",
      "Epoch 95/500, Train Loss: 0.0872, Test Loss: 0.1286\n",
      "Epoch 96/500, Train Loss: 0.0873, Test Loss: 0.1285\n",
      "Epoch 97/500, Train Loss: 0.0872, Test Loss: 0.1290\n",
      "Epoch 98/500, Train Loss: 0.0872, Test Loss: 0.1295\n",
      "Epoch 99/500, Train Loss: 0.0872, Test Loss: 0.1294\n",
      "Epoch 100/500, Train Loss: 0.0872, Test Loss: 0.1292\n",
      "Epoch 101/500, Train Loss: 0.0872, Test Loss: 0.1295\n",
      "Epoch 102/500, Train Loss: 0.0871, Test Loss: 0.1294\n",
      "Epoch 103/500, Train Loss: 0.0872, Test Loss: 0.1301\n",
      "Epoch 104/500, Train Loss: 0.0871, Test Loss: 0.1294\n",
      "Epoch 105/500, Train Loss: 0.0871, Test Loss: 0.1298\n",
      "Epoch 106/500, Train Loss: 0.0872, Test Loss: 0.1303\n",
      "Epoch 107/500, Train Loss: 0.0872, Test Loss: 0.1301\n",
      "Epoch 108/500, Train Loss: 0.0871, Test Loss: 0.1300\n",
      "Epoch 109/500, Train Loss: 0.0872, Test Loss: 0.1302\n",
      "Epoch 110/500, Train Loss: 0.0871, Test Loss: 0.1306\n",
      "Epoch 111/500, Train Loss: 0.0871, Test Loss: 0.1309\n",
      "Epoch 112/500, Train Loss: 0.0872, Test Loss: 0.1309\n",
      "Epoch 113/500, Train Loss: 0.0872, Test Loss: 0.1307\n",
      "Epoch 114/500, Train Loss: 0.0872, Test Loss: 0.1308\n",
      "Epoch 115/500, Train Loss: 0.0872, Test Loss: 0.1311\n",
      "Epoch 116/500, Train Loss: 0.0872, Test Loss: 0.1314\n",
      "Epoch 117/500, Train Loss: 0.0871, Test Loss: 0.1310\n",
      "Epoch 118/500, Train Loss: 0.0871, Test Loss: 0.1313\n",
      "Epoch 119/500, Train Loss: 0.0871, Test Loss: 0.1316\n",
      "Epoch 120/500, Train Loss: 0.0870, Test Loss: 0.1312\n",
      "Epoch 121/500, Train Loss: 0.0870, Test Loss: 0.1315\n",
      "Epoch 122/500, Train Loss: 0.0870, Test Loss: 0.1314\n",
      "Epoch 123/500, Train Loss: 0.0870, Test Loss: 0.1322\n",
      "Epoch 124/500, Train Loss: 0.0871, Test Loss: 0.1324\n",
      "Epoch 125/500, Train Loss: 0.0871, Test Loss: 0.1319\n",
      "Epoch 126/500, Train Loss: 0.0871, Test Loss: 0.1325\n",
      "Epoch 127/500, Train Loss: 0.0871, Test Loss: 0.1320\n",
      "Epoch 128/500, Train Loss: 0.0871, Test Loss: 0.1324\n",
      "Epoch 129/500, Train Loss: 0.0871, Test Loss: 0.1322\n",
      "Epoch 130/500, Train Loss: 0.0872, Test Loss: 0.1328\n",
      "Epoch 131/500, Train Loss: 0.0870, Test Loss: 0.1333\n",
      "Epoch 132/500, Train Loss: 0.0871, Test Loss: 0.1331\n",
      "Epoch 133/500, Train Loss: 0.0871, Test Loss: 0.1334\n",
      "Epoch 134/500, Train Loss: 0.0871, Test Loss: 0.1332\n",
      "Epoch 135/500, Train Loss: 0.0870, Test Loss: 0.1325\n",
      "Epoch 136/500, Train Loss: 0.0871, Test Loss: 0.1334\n",
      "Epoch 137/500, Train Loss: 0.0871, Test Loss: 0.1332\n",
      "Epoch 138/500, Train Loss: 0.0870, Test Loss: 0.1329\n",
      "Epoch 139/500, Train Loss: 0.0870, Test Loss: 0.1336\n",
      "Epoch 140/500, Train Loss: 0.0870, Test Loss: 0.1338\n",
      "Epoch 141/500, Train Loss: 0.0870, Test Loss: 0.1335\n",
      "Epoch 142/500, Train Loss: 0.0871, Test Loss: 0.1333\n",
      "Epoch 143/500, Train Loss: 0.0871, Test Loss: 0.1335\n",
      "Epoch 144/500, Train Loss: 0.0871, Test Loss: 0.1339\n",
      "Epoch 145/500, Train Loss: 0.0871, Test Loss: 0.1336\n",
      "Epoch 146/500, Train Loss: 0.0871, Test Loss: 0.1338\n",
      "Epoch 147/500, Train Loss: 0.0871, Test Loss: 0.1343\n",
      "Epoch 148/500, Train Loss: 0.0870, Test Loss: 0.1341\n",
      "Epoch 149/500, Train Loss: 0.0870, Test Loss: 0.1335\n",
      "Epoch 150/500, Train Loss: 0.0869, Test Loss: 0.1341\n",
      "Epoch 151/500, Train Loss: 0.0870, Test Loss: 0.1341\n",
      "Epoch 152/500, Train Loss: 0.0870, Test Loss: 0.1338\n",
      "Epoch 153/500, Train Loss: 0.0870, Test Loss: 0.1347\n",
      "Epoch 154/500, Train Loss: 0.0870, Test Loss: 0.1350\n",
      "Epoch 155/500, Train Loss: 0.0870, Test Loss: 0.1352\n",
      "Epoch 156/500, Train Loss: 0.0871, Test Loss: 0.1352\n",
      "Epoch 157/500, Train Loss: 0.0871, Test Loss: 0.1348\n",
      "Epoch 158/500, Train Loss: 0.0870, Test Loss: 0.1352\n",
      "Epoch 159/500, Train Loss: 0.0870, Test Loss: 0.1355\n",
      "Epoch 160/500, Train Loss: 0.0870, Test Loss: 0.1344\n",
      "Epoch 161/500, Train Loss: 0.0871, Test Loss: 0.1350\n",
      "Epoch 162/500, Train Loss: 0.0871, Test Loss: 0.1347\n",
      "Epoch 163/500, Train Loss: 0.0871, Test Loss: 0.1346\n",
      "Epoch 164/500, Train Loss: 0.0871, Test Loss: 0.1353\n",
      "Epoch 165/500, Train Loss: 0.0870, Test Loss: 0.1346\n",
      "Epoch 166/500, Train Loss: 0.0870, Test Loss: 0.1349\n",
      "Epoch 167/500, Train Loss: 0.0870, Test Loss: 0.1347\n",
      "Epoch 168/500, Train Loss: 0.0870, Test Loss: 0.1356\n",
      "Epoch 169/500, Train Loss: 0.0870, Test Loss: 0.1347\n",
      "Epoch 170/500, Train Loss: 0.0870, Test Loss: 0.1361\n",
      "Epoch 171/500, Train Loss: 0.0870, Test Loss: 0.1354\n",
      "Epoch 172/500, Train Loss: 0.0870, Test Loss: 0.1358\n",
      "Epoch 173/500, Train Loss: 0.0870, Test Loss: 0.1359\n",
      "Epoch 174/500, Train Loss: 0.0870, Test Loss: 0.1356\n",
      "Epoch 175/500, Train Loss: 0.0870, Test Loss: 0.1357\n",
      "Epoch 176/500, Train Loss: 0.0870, Test Loss: 0.1356\n",
      "Epoch 177/500, Train Loss: 0.0869, Test Loss: 0.1362\n",
      "Epoch 178/500, Train Loss: 0.0870, Test Loss: 0.1357\n",
      "Epoch 179/500, Train Loss: 0.0870, Test Loss: 0.1364\n",
      "Epoch 180/500, Train Loss: 0.0870, Test Loss: 0.1355\n",
      "Epoch 181/500, Train Loss: 0.0870, Test Loss: 0.1360\n",
      "Epoch 182/500, Train Loss: 0.0870, Test Loss: 0.1366\n",
      "Epoch 183/500, Train Loss: 0.0870, Test Loss: 0.1359\n",
      "Epoch 184/500, Train Loss: 0.0870, Test Loss: 0.1359\n",
      "Epoch 185/500, Train Loss: 0.0870, Test Loss: 0.1363\n",
      "Epoch 186/500, Train Loss: 0.0870, Test Loss: 0.1371\n",
      "Epoch 187/500, Train Loss: 0.0869, Test Loss: 0.1361\n",
      "Epoch 188/500, Train Loss: 0.0869, Test Loss: 0.1366\n",
      "Epoch 189/500, Train Loss: 0.0870, Test Loss: 0.1371\n",
      "Epoch 190/500, Train Loss: 0.0870, Test Loss: 0.1369\n",
      "Epoch 191/500, Train Loss: 0.0870, Test Loss: 0.1361\n",
      "Epoch 192/500, Train Loss: 0.0871, Test Loss: 0.1364\n",
      "Epoch 193/500, Train Loss: 0.0870, Test Loss: 0.1369\n",
      "Epoch 194/500, Train Loss: 0.0869, Test Loss: 0.1367\n",
      "Epoch 195/500, Train Loss: 0.0869, Test Loss: 0.1372\n",
      "Epoch 196/500, Train Loss: 0.0869, Test Loss: 0.1365\n",
      "Epoch 197/500, Train Loss: 0.0869, Test Loss: 0.1370\n",
      "Epoch 198/500, Train Loss: 0.0869, Test Loss: 0.1369\n",
      "Epoch 199/500, Train Loss: 0.0869, Test Loss: 0.1370\n",
      "Epoch 200/500, Train Loss: 0.0869, Test Loss: 0.1376\n",
      "Epoch 201/500, Train Loss: 0.0870, Test Loss: 0.1374\n",
      "Epoch 202/500, Train Loss: 0.0870, Test Loss: 0.1372\n",
      "Epoch 203/500, Train Loss: 0.0870, Test Loss: 0.1372\n",
      "Epoch 204/500, Train Loss: 0.0870, Test Loss: 0.1368\n",
      "Epoch 205/500, Train Loss: 0.0870, Test Loss: 0.1373\n",
      "Epoch 206/500, Train Loss: 0.0869, Test Loss: 0.1378\n",
      "Epoch 207/500, Train Loss: 0.0869, Test Loss: 0.1371\n",
      "Epoch 208/500, Train Loss: 0.0870, Test Loss: 0.1380\n",
      "Epoch 209/500, Train Loss: 0.0869, Test Loss: 0.1376\n",
      "Epoch 210/500, Train Loss: 0.0870, Test Loss: 0.1375\n",
      "Epoch 211/500, Train Loss: 0.0869, Test Loss: 0.1378\n",
      "Epoch 212/500, Train Loss: 0.0869, Test Loss: 0.1382\n",
      "Epoch 213/500, Train Loss: 0.0869, Test Loss: 0.1372\n",
      "Epoch 214/500, Train Loss: 0.0870, Test Loss: 0.1378\n",
      "Epoch 215/500, Train Loss: 0.0870, Test Loss: 0.1379\n",
      "Epoch 216/500, Train Loss: 0.0870, Test Loss: 0.1379\n",
      "Epoch 217/500, Train Loss: 0.0870, Test Loss: 0.1376\n",
      "Epoch 218/500, Train Loss: 0.0870, Test Loss: 0.1383\n",
      "Epoch 219/500, Train Loss: 0.0869, Test Loss: 0.1372\n",
      "Epoch 220/500, Train Loss: 0.0870, Test Loss: 0.1382\n",
      "Epoch 221/500, Train Loss: 0.0869, Test Loss: 0.1370\n",
      "Epoch 222/500, Train Loss: 0.0870, Test Loss: 0.1377\n",
      "Epoch 223/500, Train Loss: 0.0869, Test Loss: 0.1371\n",
      "Epoch 224/500, Train Loss: 0.0869, Test Loss: 0.1381\n",
      "Epoch 225/500, Train Loss: 0.0869, Test Loss: 0.1378\n",
      "Epoch 226/500, Train Loss: 0.0869, Test Loss: 0.1377\n",
      "Epoch 227/500, Train Loss: 0.0868, Test Loss: 0.1380\n",
      "Epoch 228/500, Train Loss: 0.0869, Test Loss: 0.1384\n",
      "Epoch 229/500, Train Loss: 0.0869, Test Loss: 0.1385\n",
      "Epoch 230/500, Train Loss: 0.0869, Test Loss: 0.1388\n",
      "Epoch 231/500, Train Loss: 0.0869, Test Loss: 0.1381\n",
      "Epoch 232/500, Train Loss: 0.0869, Test Loss: 0.1379\n",
      "Epoch 233/500, Train Loss: 0.0869, Test Loss: 0.1385\n",
      "Epoch 234/500, Train Loss: 0.0869, Test Loss: 0.1380\n",
      "Epoch 235/500, Train Loss: 0.0870, Test Loss: 0.1383\n",
      "Epoch 236/500, Train Loss: 0.0869, Test Loss: 0.1385\n",
      "Epoch 237/500, Train Loss: 0.0870, Test Loss: 0.1379\n",
      "Epoch 238/500, Train Loss: 0.0869, Test Loss: 0.1381\n",
      "Epoch 239/500, Train Loss: 0.0869, Test Loss: 0.1388\n",
      "Epoch 240/500, Train Loss: 0.0869, Test Loss: 0.1382\n",
      "Epoch 241/500, Train Loss: 0.0870, Test Loss: 0.1384\n",
      "Epoch 242/500, Train Loss: 0.0869, Test Loss: 0.1384\n",
      "Epoch 243/500, Train Loss: 0.0869, Test Loss: 0.1384\n",
      "Epoch 244/500, Train Loss: 0.0869, Test Loss: 0.1390\n",
      "Epoch 245/500, Train Loss: 0.0869, Test Loss: 0.1383\n",
      "Epoch 246/500, Train Loss: 0.0869, Test Loss: 0.1386\n",
      "Epoch 247/500, Train Loss: 0.0868, Test Loss: 0.1387\n",
      "Epoch 248/500, Train Loss: 0.0869, Test Loss: 0.1388\n",
      "Epoch 249/500, Train Loss: 0.0868, Test Loss: 0.1390\n",
      "Epoch 250/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 251/500, Train Loss: 0.0869, Test Loss: 0.1392\n",
      "Epoch 252/500, Train Loss: 0.0869, Test Loss: 0.1395\n",
      "Epoch 253/500, Train Loss: 0.0869, Test Loss: 0.1389\n",
      "Epoch 254/500, Train Loss: 0.0870, Test Loss: 0.1399\n",
      "Epoch 255/500, Train Loss: 0.0870, Test Loss: 0.1386\n",
      "Epoch 256/500, Train Loss: 0.0870, Test Loss: 0.1386\n",
      "Epoch 257/500, Train Loss: 0.0870, Test Loss: 0.1389\n",
      "Epoch 258/500, Train Loss: 0.0870, Test Loss: 0.1392\n",
      "Epoch 259/500, Train Loss: 0.0869, Test Loss: 0.1386\n",
      "Epoch 260/500, Train Loss: 0.0868, Test Loss: 0.1390\n",
      "Epoch 261/500, Train Loss: 0.0868, Test Loss: 0.1384\n",
      "Epoch 262/500, Train Loss: 0.0868, Test Loss: 0.1393\n",
      "Epoch 263/500, Train Loss: 0.0868, Test Loss: 0.1397\n",
      "Epoch 264/500, Train Loss: 0.0869, Test Loss: 0.1390\n",
      "Epoch 265/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 266/500, Train Loss: 0.0869, Test Loss: 0.1390\n",
      "Epoch 267/500, Train Loss: 0.0868, Test Loss: 0.1399\n",
      "Epoch 268/500, Train Loss: 0.0868, Test Loss: 0.1396\n",
      "Epoch 269/500, Train Loss: 0.0869, Test Loss: 0.1399\n",
      "Epoch 270/500, Train Loss: 0.0869, Test Loss: 0.1392\n",
      "Epoch 271/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 272/500, Train Loss: 0.0868, Test Loss: 0.1391\n",
      "Epoch 273/500, Train Loss: 0.0868, Test Loss: 0.1391\n",
      "Epoch 274/500, Train Loss: 0.0869, Test Loss: 0.1395\n",
      "Epoch 275/500, Train Loss: 0.0868, Test Loss: 0.1394\n",
      "Epoch 276/500, Train Loss: 0.0868, Test Loss: 0.1394\n",
      "Epoch 277/500, Train Loss: 0.0869, Test Loss: 0.1399\n",
      "Epoch 278/500, Train Loss: 0.0869, Test Loss: 0.1395\n",
      "Epoch 279/500, Train Loss: 0.0869, Test Loss: 0.1397\n",
      "Epoch 280/500, Train Loss: 0.0868, Test Loss: 0.1396\n",
      "Epoch 281/500, Train Loss: 0.0869, Test Loss: 0.1394\n",
      "Epoch 282/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 283/500, Train Loss: 0.0869, Test Loss: 0.1397\n",
      "Epoch 284/500, Train Loss: 0.0868, Test Loss: 0.1396\n",
      "Epoch 285/500, Train Loss: 0.0869, Test Loss: 0.1397\n",
      "Epoch 286/500, Train Loss: 0.0869, Test Loss: 0.1398\n",
      "Epoch 287/500, Train Loss: 0.0869, Test Loss: 0.1404\n",
      "Epoch 288/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 289/500, Train Loss: 0.0869, Test Loss: 0.1400\n",
      "Epoch 290/500, Train Loss: 0.0869, Test Loss: 0.1393\n",
      "Epoch 291/500, Train Loss: 0.0870, Test Loss: 0.1397\n",
      "Epoch 292/500, Train Loss: 0.0869, Test Loss: 0.1397\n",
      "Epoch 293/500, Train Loss: 0.0869, Test Loss: 0.1398\n",
      "Epoch 294/500, Train Loss: 0.0869, Test Loss: 0.1397\n",
      "Epoch 295/500, Train Loss: 0.0869, Test Loss: 0.1394\n",
      "Epoch 296/500, Train Loss: 0.0868, Test Loss: 0.1395\n",
      "Epoch 297/500, Train Loss: 0.0869, Test Loss: 0.1400\n",
      "Epoch 298/500, Train Loss: 0.0869, Test Loss: 0.1398\n",
      "Epoch 299/500, Train Loss: 0.0868, Test Loss: 0.1401\n",
      "Epoch 300/500, Train Loss: 0.0869, Test Loss: 0.1394\n",
      "Epoch 301/500, Train Loss: 0.0869, Test Loss: 0.1396\n",
      "Epoch 302/500, Train Loss: 0.0869, Test Loss: 0.1392\n",
      "Epoch 303/500, Train Loss: 0.0868, Test Loss: 0.1396\n",
      "Epoch 304/500, Train Loss: 0.0868, Test Loss: 0.1403\n",
      "Epoch 305/500, Train Loss: 0.0869, Test Loss: 0.1398\n",
      "Epoch 306/500, Train Loss: 0.0868, Test Loss: 0.1398\n",
      "Epoch 307/500, Train Loss: 0.0868, Test Loss: 0.1402\n",
      "Epoch 308/500, Train Loss: 0.0869, Test Loss: 0.1400\n",
      "Epoch 309/500, Train Loss: 0.0868, Test Loss: 0.1407\n",
      "Epoch 310/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 311/500, Train Loss: 0.0868, Test Loss: 0.1401\n",
      "Epoch 312/500, Train Loss: 0.0868, Test Loss: 0.1403\n",
      "Epoch 313/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 314/500, Train Loss: 0.0868, Test Loss: 0.1407\n",
      "Epoch 315/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 316/500, Train Loss: 0.0868, Test Loss: 0.1402\n",
      "Epoch 317/500, Train Loss: 0.0869, Test Loss: 0.1404\n",
      "Epoch 318/500, Train Loss: 0.0869, Test Loss: 0.1405\n",
      "Epoch 319/500, Train Loss: 0.0869, Test Loss: 0.1399\n",
      "Epoch 320/500, Train Loss: 0.0869, Test Loss: 0.1406\n",
      "Epoch 321/500, Train Loss: 0.0869, Test Loss: 0.1402\n",
      "Epoch 322/500, Train Loss: 0.0869, Test Loss: 0.1406\n",
      "Epoch 323/500, Train Loss: 0.0869, Test Loss: 0.1400\n",
      "Epoch 324/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 325/500, Train Loss: 0.0868, Test Loss: 0.1400\n",
      "Epoch 326/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 327/500, Train Loss: 0.0868, Test Loss: 0.1397\n",
      "Epoch 328/500, Train Loss: 0.0868, Test Loss: 0.1405\n",
      "Epoch 329/500, Train Loss: 0.0869, Test Loss: 0.1399\n",
      "Epoch 330/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 331/500, Train Loss: 0.0869, Test Loss: 0.1405\n",
      "Epoch 332/500, Train Loss: 0.0869, Test Loss: 0.1410\n",
      "Epoch 333/500, Train Loss: 0.0868, Test Loss: 0.1399\n",
      "Epoch 334/500, Train Loss: 0.0868, Test Loss: 0.1402\n",
      "Epoch 335/500, Train Loss: 0.0868, Test Loss: 0.1401\n",
      "Epoch 336/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 337/500, Train Loss: 0.0868, Test Loss: 0.1402\n",
      "Epoch 338/500, Train Loss: 0.0868, Test Loss: 0.1404\n",
      "Epoch 339/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 340/500, Train Loss: 0.0868, Test Loss: 0.1409\n",
      "Epoch 341/500, Train Loss: 0.0868, Test Loss: 0.1402\n",
      "Epoch 342/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 343/500, Train Loss: 0.0868, Test Loss: 0.1406\n",
      "Epoch 344/500, Train Loss: 0.0869, Test Loss: 0.1410\n",
      "Epoch 345/500, Train Loss: 0.0868, Test Loss: 0.1405\n",
      "Epoch 346/500, Train Loss: 0.0869, Test Loss: 0.1410\n",
      "Epoch 347/500, Train Loss: 0.0869, Test Loss: 0.1402\n",
      "Epoch 348/500, Train Loss: 0.0868, Test Loss: 0.1406\n",
      "Epoch 349/500, Train Loss: 0.0868, Test Loss: 0.1406\n",
      "Epoch 350/500, Train Loss: 0.0869, Test Loss: 0.1408\n",
      "Epoch 351/500, Train Loss: 0.0869, Test Loss: 0.1404\n",
      "Epoch 352/500, Train Loss: 0.0868, Test Loss: 0.1409\n",
      "Epoch 353/500, Train Loss: 0.0868, Test Loss: 0.1407\n",
      "Epoch 354/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 355/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 356/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 357/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 358/500, Train Loss: 0.0869, Test Loss: 0.1412\n",
      "Epoch 359/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 360/500, Train Loss: 0.0869, Test Loss: 0.1405\n",
      "Epoch 361/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 362/500, Train Loss: 0.0869, Test Loss: 0.1409\n",
      "Epoch 363/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 364/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 365/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 366/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 367/500, Train Loss: 0.0868, Test Loss: 0.1417\n",
      "Epoch 368/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 369/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 370/500, Train Loss: 0.0869, Test Loss: 0.1416\n",
      "Epoch 371/500, Train Loss: 0.0869, Test Loss: 0.1409\n",
      "Epoch 372/500, Train Loss: 0.0868, Test Loss: 0.1405\n",
      "Epoch 373/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 374/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 375/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 376/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 377/500, Train Loss: 0.0869, Test Loss: 0.1416\n",
      "Epoch 378/500, Train Loss: 0.0869, Test Loss: 0.1411\n",
      "Epoch 379/500, Train Loss: 0.0869, Test Loss: 0.1405\n",
      "Epoch 380/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 381/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 382/500, Train Loss: 0.0868, Test Loss: 0.1409\n",
      "Epoch 383/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 384/500, Train Loss: 0.0869, Test Loss: 0.1407\n",
      "Epoch 385/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 386/500, Train Loss: 0.0867, Test Loss: 0.1410\n",
      "Epoch 387/500, Train Loss: 0.0867, Test Loss: 0.1411\n",
      "Epoch 388/500, Train Loss: 0.0868, Test Loss: 0.1412\n",
      "Epoch 389/500, Train Loss: 0.0867, Test Loss: 0.1413\n",
      "Epoch 390/500, Train Loss: 0.0869, Test Loss: 0.1417\n",
      "Epoch 391/500, Train Loss: 0.0869, Test Loss: 0.1411\n",
      "Epoch 392/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 393/500, Train Loss: 0.0869, Test Loss: 0.1414\n",
      "Epoch 394/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 395/500, Train Loss: 0.0869, Test Loss: 0.1409\n",
      "Epoch 396/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 397/500, Train Loss: 0.0869, Test Loss: 0.1414\n",
      "Epoch 398/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 399/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 400/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 401/500, Train Loss: 0.0868, Test Loss: 0.1408\n",
      "Epoch 402/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 403/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 404/500, Train Loss: 0.0868, Test Loss: 0.1410\n",
      "Epoch 405/500, Train Loss: 0.0868, Test Loss: 0.1412\n",
      "Epoch 406/500, Train Loss: 0.0868, Test Loss: 0.1409\n",
      "Epoch 407/500, Train Loss: 0.0868, Test Loss: 0.1415\n",
      "Epoch 408/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 409/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 410/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 411/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 412/500, Train Loss: 0.0867, Test Loss: 0.1415\n",
      "Epoch 413/500, Train Loss: 0.0868, Test Loss: 0.1415\n",
      "Epoch 414/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 415/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 416/500, Train Loss: 0.0867, Test Loss: 0.1413\n",
      "Epoch 417/500, Train Loss: 0.0869, Test Loss: 0.1412\n",
      "Epoch 418/500, Train Loss: 0.0869, Test Loss: 0.1415\n",
      "Epoch 419/500, Train Loss: 0.0868, Test Loss: 0.1415\n",
      "Epoch 420/500, Train Loss: 0.0869, Test Loss: 0.1416\n",
      "Epoch 421/500, Train Loss: 0.0868, Test Loss: 0.1412\n",
      "Epoch 422/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 423/500, Train Loss: 0.0868, Test Loss: 0.1412\n",
      "Epoch 424/500, Train Loss: 0.0869, Test Loss: 0.1420\n",
      "Epoch 425/500, Train Loss: 0.0868, Test Loss: 0.1411\n",
      "Epoch 426/500, Train Loss: 0.0868, Test Loss: 0.1409\n",
      "Epoch 427/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 428/500, Train Loss: 0.0868, Test Loss: 0.1415\n",
      "Epoch 429/500, Train Loss: 0.0867, Test Loss: 0.1414\n",
      "Epoch 430/500, Train Loss: 0.0867, Test Loss: 0.1415\n",
      "Epoch 431/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 432/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 433/500, Train Loss: 0.0868, Test Loss: 0.1415\n",
      "Epoch 434/500, Train Loss: 0.0867, Test Loss: 0.1416\n",
      "Epoch 435/500, Train Loss: 0.0867, Test Loss: 0.1418\n",
      "Epoch 436/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 437/500, Train Loss: 0.0868, Test Loss: 0.1418\n",
      "Epoch 438/500, Train Loss: 0.0868, Test Loss: 0.1418\n",
      "Epoch 439/500, Train Loss: 0.0867, Test Loss: 0.1418\n",
      "Epoch 440/500, Train Loss: 0.0867, Test Loss: 0.1419\n",
      "Epoch 441/500, Train Loss: 0.0868, Test Loss: 0.1421\n",
      "Epoch 442/500, Train Loss: 0.0868, Test Loss: 0.1421\n",
      "Epoch 443/500, Train Loss: 0.0869, Test Loss: 0.1417\n",
      "Epoch 444/500, Train Loss: 0.0869, Test Loss: 0.1419\n",
      "Epoch 445/500, Train Loss: 0.0868, Test Loss: 0.1412\n",
      "Epoch 446/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 447/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 448/500, Train Loss: 0.0867, Test Loss: 0.1417\n",
      "Epoch 449/500, Train Loss: 0.0868, Test Loss: 0.1417\n",
      "Epoch 450/500, Train Loss: 0.0868, Test Loss: 0.1417\n",
      "Epoch 451/500, Train Loss: 0.0867, Test Loss: 0.1424\n",
      "Epoch 452/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 453/500, Train Loss: 0.0868, Test Loss: 0.1420\n",
      "Epoch 454/500, Train Loss: 0.0867, Test Loss: 0.1422\n",
      "Epoch 455/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 456/500, Train Loss: 0.0868, Test Loss: 0.1418\n",
      "Epoch 457/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 458/500, Train Loss: 0.0868, Test Loss: 0.1422\n",
      "Epoch 459/500, Train Loss: 0.0869, Test Loss: 0.1412\n",
      "Epoch 460/500, Train Loss: 0.0868, Test Loss: 0.1413\n",
      "Epoch 461/500, Train Loss: 0.0868, Test Loss: 0.1417\n",
      "Epoch 462/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 463/500, Train Loss: 0.0869, Test Loss: 0.1418\n",
      "Epoch 464/500, Train Loss: 0.0868, Test Loss: 0.1420\n",
      "Epoch 465/500, Train Loss: 0.0869, Test Loss: 0.1418\n",
      "Epoch 466/500, Train Loss: 0.0869, Test Loss: 0.1420\n",
      "Epoch 467/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 468/500, Train Loss: 0.0868, Test Loss: 0.1414\n",
      "Epoch 469/500, Train Loss: 0.0867, Test Loss: 0.1416\n",
      "Epoch 470/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 471/500, Train Loss: 0.0867, Test Loss: 0.1413\n",
      "Epoch 472/500, Train Loss: 0.0868, Test Loss: 0.1416\n",
      "Epoch 473/500, Train Loss: 0.0868, Test Loss: 0.1421\n",
      "Epoch 474/500, Train Loss: 0.0868, Test Loss: 0.1417\n",
      "Epoch 475/500, Train Loss: 0.0867, Test Loss: 0.1421\n",
      "Epoch 476/500, Train Loss: 0.0868, Test Loss: 0.1425\n",
      "Epoch 477/500, Train Loss: 0.0867, Test Loss: 0.1422\n",
      "Epoch 478/500, Train Loss: 0.0867, Test Loss: 0.1423\n",
      "Epoch 479/500, Train Loss: 0.0867, Test Loss: 0.1420\n",
      "Epoch 480/500, Train Loss: 0.0868, Test Loss: 0.1423\n",
      "Epoch 481/500, Train Loss: 0.0867, Test Loss: 0.1420\n",
      "Epoch 482/500, Train Loss: 0.0868, Test Loss: 0.1420\n",
      "Epoch 483/500, Train Loss: 0.0868, Test Loss: 0.1426\n",
      "Epoch 484/500, Train Loss: 0.0868, Test Loss: 0.1425\n",
      "Epoch 485/500, Train Loss: 0.0868, Test Loss: 0.1426\n",
      "Epoch 486/500, Train Loss: 0.0868, Test Loss: 0.1420\n",
      "Epoch 487/500, Train Loss: 0.0868, Test Loss: 0.1423\n",
      "Epoch 488/500, Train Loss: 0.0867, Test Loss: 0.1423\n",
      "Epoch 489/500, Train Loss: 0.0867, Test Loss: 0.1418\n",
      "Epoch 490/500, Train Loss: 0.0867, Test Loss: 0.1423\n",
      "Epoch 491/500, Train Loss: 0.0867, Test Loss: 0.1427\n",
      "Epoch 492/500, Train Loss: 0.0868, Test Loss: 0.1424\n",
      "Epoch 493/500, Train Loss: 0.0868, Test Loss: 0.1425\n",
      "Epoch 494/500, Train Loss: 0.0868, Test Loss: 0.1424\n",
      "Epoch 495/500, Train Loss: 0.0867, Test Loss: 0.1426\n",
      "Epoch 496/500, Train Loss: 0.0868, Test Loss: 0.1430\n",
      "Epoch 497/500, Train Loss: 0.0868, Test Loss: 0.1422\n",
      "Epoch 498/500, Train Loss: 0.0868, Test Loss: 0.1425\n",
      "Epoch 499/500, Train Loss: 0.0868, Test Loss: 0.1419\n",
      "Epoch 500/500, Train Loss: 0.0868, Test Loss: 0.1423\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# 評估模型\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# 訓練和評估過程\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3b40c-1d93-49a8-8b56-4305a215955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83298789-d314-4594-bb8c-9b6f715b00cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
