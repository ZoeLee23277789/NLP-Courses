{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be431b8-9e82-4d09-9af3-6b074b033d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Usage: python run.py <train_data> <test_data> <output>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Main function to execute the program\n",
    "def main(train_data_path, test_data_path, output_path):\n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    \n",
    "    # # Initialize BERT Tokenizer and Model\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    # bert_model = BertModel.from_pretrained(\"bert-large-uncased\").to(device)\n",
    "    # 使用較小的 BERT 模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "    \n",
    "    # Define max length\n",
    "    MAX_LENGTH = 32\n",
    "\n",
    "    # Function to encode texts\n",
    "    def encode_texts(texts, tokenizer, bert_model, device, max_length=MAX_LENGTH):\n",
    "        inputs = tokenizer(\n",
    "            texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        outputs = bert_model(**inputs)\n",
    "        return outputs\n",
    "\n",
    "    # Extracting BERT embeddings for sentences\n",
    "    utterances = train_df['utterances'].tolist()\n",
    "    embeddings, sequence_lengths = encode_texts(utterances, tokenizer, bert_model, device)\n",
    "    \n",
    "    # Build a label mapping dictionary\n",
    "    unique_labels = set(label for tags in train_df['IOB Slot tags'] for label in tags.split())\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_index.items()}\n",
    "    \n",
    "    # max_length length of BERT output\n",
    "    labels = train_df['IOB Slot tags'].apply(lambda x: [label_to_index[label] for label in x.split()])\n",
    "    labels_padded = nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(label + [label_to_index[\"O\"]] * (MAX_LENGTH - len(label))) for label in labels], \n",
    "        batch_first=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings, labels_padded, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
    "    \n",
    "    # def Focal Loss \n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=1, gamma=2):\n",
    "            super(FocalLoss, self).__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "        def forward(self, inputs, targets):\n",
    "            ce_loss = self.ce_loss(inputs, targets)\n",
    "            p_t = torch.exp(-ce_loss)\n",
    "            focal_loss = self.alpha * ((1 - p_t) ** self.gamma) * ce_loss\n",
    "            return focal_loss.mean()\n",
    "    \n",
    "    # def GRU \n",
    "    class SlotTaggingModelGRU(nn.Module):\n",
    "        def __init__(self, bert_hidden_dim, hidden_dim=131, output_dim=None, dropout_prob=0.23450624849590243, num_layers=2):\n",
    "            super(SlotTaggingModelGRU, self).__init__()\n",
    "            self.gru = nn.GRU(\n",
    "                bert_hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_prob\n",
    "            )\n",
    "            self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            gru_out, _ = self.gru(x)\n",
    "            gru_out = self.layer_norm(gru_out)\n",
    "            gru_out = self.dropout(gru_out)\n",
    "            attn_weights = torch.softmax(self.attention(gru_out), dim=1)\n",
    "            gru_out = gru_out * attn_weights\n",
    "            output = self.fc(gru_out)\n",
    "            return output\n",
    "    \n",
    "    # init\n",
    "    bert_hidden_dim = embeddings.shape[2]\n",
    "    output_dim = len(label_to_index)\n",
    "    \n",
    "    model = SlotTaggingModelGRU(\n",
    "        bert_hidden_dim=bert_hidden_dim,\n",
    "        hidden_dim=131,\n",
    "        output_dim=output_dim,\n",
    "        dropout_prob=0.23450624849590243,\n",
    "        num_layers=2\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0009950695432002095, weight_decay=1e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    # train\n",
    "    def train_model(model, train_loader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x).view(-1, output_dim)\n",
    "            batch_y = batch_y.view(-1)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    # eval used seqeval\n",
    "    def evaluate_model(model, test_loader, criterion, device, idx_to_label):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x).view(-1, output_dim)\n",
    "                batch_y = batch_y.view(-1)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                total_loss += loss.item()\n",
    "    \n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                labels = batch_y.cpu().numpy()\n",
    "    \n",
    "                preds = preds.reshape(batch_x.size(0), -1)\n",
    "                labels = labels.reshape(batch_x.size(0), -1)\n",
    "    \n",
    "                for pred, label in zip(preds, labels):\n",
    "                    pred_tags = [idx_to_label[idx] for idx in pred if idx in idx_to_label]\n",
    "                    true_tags = [idx_to_label[idx] for idx in label if idx in idx_to_label]\n",
    "                    all_preds.append(pred_tags)\n",
    "                    all_labels.append(true_tags)\n",
    "    \n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "        return total_loss / len(test_loader), f1\n",
    "    \n",
    "    # train\n",
    "    num_epochs = 200  # 根據需要調整\n",
    "    train_losses, f1_scores = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, f1 = evaluate_model(model, test_loader, criterion, device, idx_to_label)\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        f1_scores.append(f1)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # plt Loss & F1 Score \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(range(num_epochs), train_losses, label=\"Train Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(range(num_epochs), f1_scores, label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # get the output\n",
    "    test_df = pd.read_csv('hw2_test.csv')\n",
    "    \n",
    "    def generate_submission_file(model, test_df, tokenizer, bert_model, idx_to_label, device, output_file=\"submission.csv\"):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for idx, row in test_df.iterrows():\n",
    "                utterance = row[\"utterances\"]\n",
    "                inputs = tokenizer(utterance, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "                embeddings = bert_model(**inputs).last_hidden_state\n",
    "                \n",
    "                outputs = model(embeddings)\n",
    "                \n",
    "                if outputs.dim() == 2:\n",
    "                    outputs = outputs.unsqueeze(0)\n",
    "    \n",
    "                pred_labels = torch.argmax(outputs, dim=2).squeeze().cpu().numpy()\n",
    "                pred_labels = [idx_to_label[label] for label in pred_labels[:len(inputs['input_ids'][0])]]\n",
    "    \n",
    "                tokens = tokenizer.tokenize(utterance)\n",
    "                final_labels = []\n",
    "                token_idx = 0\n",
    "    \n",
    "                for label in pred_labels:\n",
    "                    if token_idx >= len(tokens):\n",
    "                        break\n",
    "                    if tokens[token_idx].startswith(\"##\"):\n",
    "                        token_idx += 1\n",
    "                        continue\n",
    "                    final_labels.append(label if label != \"O\" else \"O\")\n",
    "                    token_idx += 1\n",
    "    \n",
    "                predictions.append(final_labels)\n",
    "    \n",
    "        submission_df = pd.DataFrame({\"ID\": test_df[\"ID\"], \"IOB Slot tags\": [\" \".join(tags) for tags in predictions]})\n",
    "        submission_df.to_csv(output_file, index=False)\n",
    "        print(f\"Submission file generated：{output_file}\")\n",
    "    \n",
    "    generate_submission_file(model, test_df, tokenizer, bert_model, idx_to_label, device)\n",
    "    with open(output_path, \"w\") as output_file:\n",
    "        output_file.write(\"Output results placeholder\")\n",
    "\n",
    "# Execute main function with command-line arguments\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: python run.py <train_data> <test_data> <output>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    train_data_path = sys.argv[1]\n",
    "    test_data_path = sys.argv[2]\n",
    "    output_path = sys.argv[3]\n",
    "    \n",
    "    main(train_data_path, test_data_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea6889-0b99-463f-97bc-79a27fcf031b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
