{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81cb8-f062-4a9b-930f-a5b3e5cf73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 20000/20000 [01:53<00:00, 176.59 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6950' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6950/45000 23:00:05 < 125:57:56, 0.08 it/s, Epoch 1.54/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.217517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, AdamW, EarlyStoppingCallback\n",
    "import re\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 載入 SpaCy NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:20000]', trust_remote_code=True)\n",
    "\n",
    "# 載入 mBART 模型和 Tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"  # 英文\n",
    "tokenizer.tgt_lang = \"zh_CN\"  # 簡體中文\n",
    "\n",
    "# 實體識別和標記的函數\n",
    "def mark_entities(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<<{ent.label_}:{ent.text}>>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "# 預處理函數\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    entities_list = []\n",
    "    for ex in examples[\"translation\"]:\n",
    "        marked_text, entities = mark_entities(ex[\"en\"])\n",
    "        target = ex[\"zh\"]\n",
    "        inputs.append(marked_text)\n",
    "        targets.append(target)\n",
    "        entities_list.append(entities)\n",
    "    \n",
    "    # Tokenize 輸入和標籤\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"entities\"] = entities_list\n",
    "    return model_inputs\n",
    "\n",
    "# 對數據集進行 Tokenize 和實體標記\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 訓練參數設置\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# 將數據集劃分為訓練集和驗證集\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# 自定義 Layer-wise Learning Rate Decay\n",
    "layer_decay = 0.8\n",
    "optimizer_grouped_parameters = []\n",
    "num_layers = len(model.model.encoder.layers)\n",
    "\n",
    "for i, layer in enumerate(model.model.encoder.layers):\n",
    "    lr = training_args.learning_rate * (layer_decay ** (num_layers - i - 1))\n",
    "    optimizer_grouped_parameters.append({\"params\": layer.parameters(), \"lr\": lr})\n",
    "optimizer_grouped_parameters.append({\"params\": model.model.shared.parameters(), \"lr\": training_args.learning_rate})\n",
    "\n",
    "# 初始化 optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)\n",
    "\n",
    "# 自定義 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# 提取並記錄訓練和驗證損失\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for log in trainer.state.log_history:\n",
    "    if 'loss' in log:\n",
    "        training_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        validation_losses.append(log['eval_loss'])\n",
    "\n",
    "# 確保訓練和驗證損失的長度一致\n",
    "min_length = min(len(training_losses), len(validation_losses))\n",
    "training_losses = training_losses[:min_length]\n",
    "validation_losses = validation_losses[:min_length]\n",
    "\n",
    "# 繪製損失曲線\n",
    "epochs = range(1, min_length + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(epochs, validation_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 後處理實體標記\n",
    "def postprocess_translation(translated_text, entities):\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker_pattern = re.escape(f\"<<{ent_label}:\") + r\"(.*?)>>\"\n",
    "        translated_text = re.sub(entity_marker_pattern, ent_text, translated_text, count=1)\n",
    "    \n",
    "    translated_text = re.sub(r\"\\b(DATE|LOC|PERCENT|CARDINAL):\", \"\", translated_text)\n",
    "    translated_text = re.sub(r\"\\(.*?\\)\", \"\", translated_text)  # 避免多餘標記出現\n",
    "    return translated_text\n",
    "\n",
    "# 使用訓練好的模型進行翻譯並還原實體\n",
    "def entity_aware_translate(input_text):\n",
    "    marked_text, entities = mark_entities(input_text)\n",
    "    inputs = tokenizer(marked_text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=256,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    final_translation = postprocess_translation(translated_text, entities)\n",
    "    return final_translation\n",
    "\n",
    "# 測試翻譯\n",
    "input_text = \"Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.\"\n",
    "final_translation = entity_aware_translate(input_text)\n",
    "print(\"Final Translation:\", final_translation)\n",
    "\n",
    "# 保存模型和 Tokenizer\n",
    "trainer.save_model(\"./mbart_finetuned\")\n",
    "tokenizer.save_pretrained(\"./mbart_finetuned\")\n",
    "\n",
    "# 驗證階段設置\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation', trust_remote_code=True)\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# 驗證函數\n",
    "def evaluate_model(val_dataset):\n",
    "    val_predictions = []\n",
    "    val_references = []\n",
    "    for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        reference_text = example[\"translation\"][\"zh\"]\n",
    "        \n",
    "        # 顯示原本的英文句子\n",
    "        print(\"Original English Text:\", input_text)\n",
    "        \n",
    "        final_translation = entity_aware_translate(input_text)\n",
    "        print(\"Reference Text:\", reference_text)\n",
    "        print(\"Translated Text:\", final_translation)\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        val_predictions.append(final_translation)\n",
    "        val_references.append([reference_text])\n",
    "    \n",
    "    # 計算 METEOR 和 BLEU 分數\n",
    "    val_meteor_score = meteor_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    val_bleu_score = bleu_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "    print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "\n",
    "# 執行驗證\n",
    "evaluate_model(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95c753-62d0-48d4-b988-89d0ec87566c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
