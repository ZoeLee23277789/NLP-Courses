{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bb6f7-fe71-4f66-95bb-f4a66d3cba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 加載 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2000]', trust_remote_code=True)\n",
    "\n",
    "# 使用自定義配置初始化 tokenizer 和模型\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "# 初始化 Tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<s>\", \"</s>\", \"<mask>\", \"<pad>\", \"<unk>\"]})\n",
    "\n",
    "# 設定 src_lang 和 tgt_lang\n",
    "tokenizer.src_lang = \"en_XX\"  # 英文\n",
    "tokenizer.tgt_lang = \"zh_CN\"  # 簡體中文\n",
    "\n",
    "# 初始化模型\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "model.config.max_length = 200\n",
    "model.config.num_beams = 5\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 配置訓練數據\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"zh\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize 數據集\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 設定訓練參數\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# 劃分訓練和驗證數據\n",
    "train_size = 0.9\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=1-train_size)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# 評估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# 保存模型和 Tokenizer\n",
    "trainer.save_model(\"./mbart_finetuned\")\n",
    "tokenizer.save_pretrained(\"./mbart_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2b5f71-7066-4f49-8f62-d44e8565605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:   0%|                                                               | 0/879 [00:00<?, ?it/s]C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1493: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Processing Validation Set: 100%|█████████████████████████████████████████████████████| 879/879 [28:36<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set BLEU score: {'bleu': 0.0, 'precisions': [0.01509009009009009, 0.002246559955068801, 0.0007062146892655367, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.7542473330699329, 'translation_length': 4440, 'reference_length': 2531}\n",
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|███████████████████████████████████████████████████████| 8549/8549 [3:59:36<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set BLEU score: {'bleu': 0.001762937007613813, 'precisions': [0.014441944429909476, 0.0021231422505307855, 0.0009088941787491885, 0.00034660042747386053], 'brevity_penalty': 1.0, 'length_ratio': 1.7534636205156435, 'translation_length': 38222, 'reference_length': 21798}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import torch\n",
    "import evaluate  # 使用 evaluate 庫來加載指標\n",
    "from tqdm import tqdm  # 加入進度條\n",
    "\n",
    "# 加載 IWSLT 2017 英中翻譯資料集的驗證和測試集\n",
    "# val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation', trust_remote_code=True)\n",
    "test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='test', trust_remote_code=True)\n",
    "\n",
    "# 加載微調後的模型和 tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"./mbart_finetuned\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"./mbart_finetuned\")\n",
    "\n",
    "# 設置源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# BLEU 評估指標\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# 翻譯函數\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# 計算 BLEU 分數的函數\n",
    "def compute_bleu(predictions, references):\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "# 在驗證集上進行翻譯並計算 BLEU 分數\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "val_predictions = []\n",
    "val_references = []\n",
    "for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):  # 加入進度條\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    translated_text = translate(input_text)\n",
    "    val_predictions.append(translated_text)\n",
    "    val_references.append([reference_text])  # BLEU 要求 reference 為列表\n",
    "\n",
    "val_bleu_score = compute_bleu(val_predictions, val_references)\n",
    "print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "\n",
    "# 在測試集上進行翻譯並計算 BLEU 分數\n",
    "print(\"Evaluating on Test Set...\")\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "for example in tqdm(test_dataset, desc=\"Processing Test Set\"):  # 加入進度條\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    translated_text = translate(input_text)\n",
    "    test_predictions.append(translated_text)\n",
    "    test_references.append([reference_text])  # BLEU 要求 reference 為列表\n",
    "\n",
    "test_bleu_score = compute_bleu(test_predictions, test_references)\n",
    "print(\"Test Set BLEU score:\", test_bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a48e59b-2129-45a0-8530-f9dd0503aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:   0%|                                                               | 0/879 [00:00<?, ?it/s]C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1493: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Processing Validation Set: 100%|███████████████████████████████████████████████████| 879/879 [1:02:05<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set BLEU score: {'bleu': 0.0, 'precisions': [0.013434579439252336, 0.0009396288466055908, 0.00028376844494892167, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.0292374555511654, 'translation_length': 5136, 'reference_length': 2531}\n",
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|███████████████████████████████████████████████████████| 8549/8549 [9:26:22<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set BLEU score: {'bleu': 0.0013290952461597158, 'precisions': [0.01223358716712477, 0.0018137678935178725, 0.0006515551592880903, 0.0002158428663932657], 'brevity_penalty': 1.0, 'length_ratio': 2.0362418570511056, 'translation_length': 44386, 'reference_length': 21798}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import spacy\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加載 IWSLT 2017 英中翻譯資料集的驗證和測試集\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation', trust_remote_code=True)\n",
    "test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='test', trust_remote_code=True)\n",
    "\n",
    "# 加載微調後的模型和 tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"./mbart_finetuned\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"./mbart_finetuned\")\n",
    "\n",
    "# 設置源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# 加載 SpaCy 英文 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# BLEU 評估指標\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# 步驟 1：實體標記和替換\n",
    "def mark_entities(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<{ent.label_}:{ent.text}>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "# 步驟 2：翻譯帶有實體標記的句子\n",
    "def translate_with_entities(text):\n",
    "    marked_text, entities = mark_entities(text)\n",
    "    inputs = tokenizer(marked_text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=80,\n",
    "        length_penalty=0.8,\n",
    "        num_beams=5,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text, entities\n",
    "\n",
    "# 步驟 3：後處理還原實體\n",
    "def postprocess_translation(translated_text, entities):\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker = f\"<{ent_label}:{ent_text}>\"\n",
    "        translated_text = translated_text.replace(entity_marker, ent_text)\n",
    "    return translated_text\n",
    "\n",
    "# 完整的 Entity-Aware 翻譯函數\n",
    "def entity_aware_translate(text):\n",
    "    translated_text, entities = translate_with_entities(text)\n",
    "    final_translation = postprocess_translation(translated_text, entities)\n",
    "    return final_translation\n",
    "\n",
    "# 評估 BLEU 分數\n",
    "def compute_bleu(predictions, references):\n",
    "    bleu_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = bleu_metric.compute()\n",
    "    return result\n",
    "\n",
    "# 在驗證集上進行翻譯並計算 BLEU 分數\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "val_predictions = []\n",
    "val_references = []\n",
    "for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    final_translation = entity_aware_translate(input_text)\n",
    "    val_predictions.append(final_translation)\n",
    "    val_references.append([reference_text])  # BLEU 要求 reference 為列表\n",
    "\n",
    "val_bleu_score = compute_bleu(val_predictions, val_references)\n",
    "print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "\n",
    "# 在測試集上進行翻譯並計算 BLEU 分數\n",
    "print(\"Evaluating on Test Set...\")\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "for example in tqdm(test_dataset, desc=\"Processing Test Set\"):\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    final_translation = entity_aware_translate(input_text)\n",
    "    test_predictions.append(final_translation)\n",
    "    test_references.append([reference_text])  # BLEU 要求 reference 為列表\n",
    "\n",
    "test_bleu_score = compute_bleu(test_predictions, test_references)\n",
    "print(\"Test Set BLEU score:\", test_bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e2a351-a4f5-4950-b0d9-e543b4616973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|█████████████████████████████████████████████████████████| 7.02k/7.02k [00:00<?, ?B/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set: 100%|███████████████████████████████████████████████████| 879/879 [1:53:55<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set METEOR score: {'meteor': 0.015482150565798002}\n",
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|███████████████████████████████████████████████████████| 8549/8549 [9:10:20<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set METEOR score: {'meteor': 0.022297144359408206}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import spacy\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加載 IWSLT 2017 英中翻譯資料集的驗證和測試集\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation', trust_remote_code=True)\n",
    "test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='test', trust_remote_code=True)\n",
    "\n",
    "# 加載微調後的模型和 tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"./mbart_finetuned\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"./mbart_finetuned\")\n",
    "\n",
    "# 設置源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# 加載 SpaCy 英文 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# METEOR 評估指標\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "# 步驟 1：實體標記和替換\n",
    "def mark_entities(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<{ent.label_}:{ent.text}>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "# 步驟 2：翻譯帶有實體標記的句子\n",
    "def translate_with_entities(text):\n",
    "    marked_text, entities = mark_entities(text)\n",
    "    inputs = tokenizer(marked_text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=80,\n",
    "        length_penalty=0.8,\n",
    "        num_beams=5,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text, entities\n",
    "\n",
    "# 步驟 3：後處理還原實體\n",
    "def postprocess_translation(translated_text, entities):\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker = f\"<{ent_label}:{ent_text}>\"\n",
    "        translated_text = translated_text.replace(entity_marker, ent_text)\n",
    "    return translated_text\n",
    "\n",
    "# 完整的 Entity-Aware 翻譯函數\n",
    "def entity_aware_translate(text):\n",
    "    translated_text, entities = translate_with_entities(text)\n",
    "    final_translation = postprocess_translation(translated_text, entities)\n",
    "    return final_translation\n",
    "\n",
    "# 評估 METEOR 分數\n",
    "def compute_meteor(predictions, references):\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "# 在驗證集上進行翻譯並計算 METEOR 分數\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "val_predictions = []\n",
    "val_references = []\n",
    "for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    final_translation = entity_aware_translate(input_text)\n",
    "    val_predictions.append(final_translation)\n",
    "    val_references.append([reference_text])  # METEOR 要求 reference 為列表\n",
    "\n",
    "val_meteor_score = compute_meteor(val_predictions, val_references)\n",
    "print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "\n",
    "# 在測試集上進行翻譯並計算 METEOR 分數\n",
    "print(\"Evaluating on Test Set...\")\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "for example in tqdm(test_dataset, desc=\"Processing Test Set\"):\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    final_translation = entity_aware_translate(input_text)\n",
    "    test_predictions.append(final_translation)\n",
    "    test_references.append([reference_text])  # METEOR 要求 reference 為列表\n",
    "\n",
    "test_meteor_score = compute_meteor(test_predictions, test_references)\n",
    "print(\"Test Set METEOR score:\", test_meteor_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d500bea-0241-4061-90b1-e8b87829f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 在指定範圍內選擇超參數\n",
    "    max_length = trial.suggest_int(\"max_length\", 50, 100)\n",
    "    length_penalty = trial.suggest_float(\"length_penalty\", 0.7, 1.0)\n",
    "    num_beams = trial.suggest_int(\"num_beams\", 3, 7)\n",
    "    \n",
    "    # 運行翻譯並評估\n",
    "    translated_texts = []\n",
    "    for example in val_dataset:\n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        translated_text, _ = translate_with_entities(\n",
    "            input_text,\n",
    "            max_length=max_length,\n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "        translated_texts.append(translated_text)\n",
    "\n",
    "    # 計算 METEOR 分數\n",
    "    meteor_score = compute_meteor(translated_texts, [ex[\"translation\"][\"zh\"] for ex in val_dataset])\n",
    "    return meteor_score\n",
    "\n",
    "# 使用 Optuna 優化\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# 獲取最佳參數\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best METEOR Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6937ee64-de4e-46ac-9547-1adb4b499e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:   0%|                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.\n",
      "\n",
      "Marked Text: <<DATE:Last year>> I showed these <<CARDINAL:two>> slides so that  demonstrate that <<LOC:the arctic ice cap>>,  which for most of <<DATE:the last three million years>>  has been the size of the lower <<CARDINAL:48>> states,  has shrunk by <<PERCENT:40 percent>>.\n",
      "Entities: [('Last year', 'DATE'), ('two', 'CARDINAL'), ('the arctic ice cap', 'LOC'), ('the last three million years', 'DATE'), ('48', 'CARDINAL'), ('40 percent', 'PERCENT')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  10%|█████▌                                                  | 1/10 [00:10<01:32, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 去年我展示了这些 slides,以证明,北极冰盖(LOC:the arctic ice cap),在过去的3百万年中大部分时间都是低级州的大小,已经减少了40%。\n",
      "Reference Text: 去年我给各位展示了两个 关于北极冰帽的演示 在过去三百万年中 其面积由相当于美国南方48州面积总和 缩减了40%\n",
      "Translated Text: 去年我展示了这些 slides,以证明,北极冰盖(LOC:the arctic ice cap),在过去的3百万年中大部分时间都是低级州的大小,已经减少了40%。\n",
      "==================================================\n",
      "Original Text: But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.\n",
      "\n",
      "Marked Text: But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.\n",
      "Entities: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  20%|███████████▏                                            | 2/10 [00:13<00:47,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 但这不足以说明该问题的严重性,因为它没有显示冰的厚度。\n",
      "Reference Text: 但这些没能完全说明这个问题的严重性 因为这没有表示出冰帽的厚度\n",
      "Translated Text: 但这不足以说明该问题的严重性,因为它没有显示冰的厚度。\n",
      "==================================================\n",
      "Original Text: The arctic ice cap is, in a sense,  the beating heart of the global climate system.\n",
      "\n",
      "Marked Text: The arctic ice cap is, in a sense,  the beating heart of the global climate system.\n",
      "Entities: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  30%|████████████████▊                                       | 3/10 [00:15<00:31,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 北极冰盖在某种意义上是全球气候体系的摇动之心。\n",
      "Reference Text: 感觉上，北极冰帽 就好象全球气候系统中跳动的心脏\n",
      "Translated Text: 北极冰盖在某种意义上是全球气候体系的摇动之心。\n",
      "==================================================\n",
      "Original Text: It expands in winter and contracts in summer.\n",
      "\n",
      "Marked Text: It expands in <<DATE:winter>> and contracts in <<DATE:summer>>.\n",
      "Entities: [('winter', 'DATE'), ('summer', 'DATE')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  40%|██████████████████████▍                                 | 4/10 [00:19<00:24,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 它在“DATE:winter”中扩展,在“DATE:summer”中收缩。\n",
      "Reference Text: 冬天心脏舒张，夏天心脏收缩\n",
      "Translated Text: 它在“DATE:winter”中扩展,在“DATE:summer”中收缩。\n",
      "==================================================\n",
      "Original Text: The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.\n",
      "\n",
      "Marked Text: The next slide I show you will be  a rapid fast-forward of what's happened over <<DATE:the last 25 years>>.\n",
      "Entities: [('the last 25 years', 'DATE')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  50%|████████████████████████████                            | 5/10 [00:22<00:19,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 下面的幻灯片将是一个快速的快速的向前发展,发生在“日期:过去25年”。\n",
      "Reference Text: 下面我要展示的是 在过去25年里的极剧变化\n",
      "Translated Text: 下面的幻灯片将是一个快速的快速的向前发展,发生在“日期:过去25年”。\n",
      "==================================================\n",
      "Original Text: The permanent ice is marked in red.\n",
      "\n",
      "Marked Text: The permanent ice is marked in red.\n",
      "Entities: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  60%|█████████████████████████████████▌                      | 6/10 [00:24<00:12,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 永久性冰被标记为红色。\n",
      "Reference Text: 红色的是永冻冰\n",
      "Translated Text: 永久性冰被标记为红色。\n",
      "==================================================\n",
      "Original Text: As you see, it expands to the dark blue --  that's the annual ice in winter,  and it contracts in summer.\n",
      "\n",
      "Marked Text: As you see, it expands to the dark blue --  that's the annual ice in <<DATE:winter>>,  and it contracts in <<DATE:summer>>.\n",
      "Entities: [('winter', 'DATE'), ('summer', 'DATE')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  70%|███████████████████████████████████████▏                | 7/10 [00:30<00:11,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 正如你所看到的,它扩展到深蓝色,那是每年的冰在“DATE:冬天”中,它在“DATE:夏天”中收缩。\n",
      "Reference Text: 你看，它正在变成深蓝色 这是每年冬天形成的年度冰 在夏天永冻冰收缩\n",
      "Translated Text: 正如你所看到的,它扩展到深蓝色,那是每年的冰在“DATE:冬天”中,它在“DATE:夏天”中收缩。\n",
      "==================================================\n",
      "Original Text: The so-called permanent ice, five years old or older,  you can see is almost like blood,  spilling out of the body here.\n",
      "\n",
      "Marked Text: The so-called permanent ice, <<DATE:five years old>> or older,  you can see is almost like blood,  spilling out of the body here.\n",
      "Entities: [('five years old', 'DATE')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  80%|████████████████████████████████████████████▊           | 8/10 [00:35<00:08,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 所谓的永久性冰(DATE:5岁)或更老的冰,你可以看到,几乎像血一样,从身体里流出。\n",
      "Reference Text: 所谓的“永冻”，是指形成五年或更久的冰 你看，这也像血液一样 输送到身体各部位\n",
      "Translated Text: 所谓的永久性冰(DATE:5岁)或更老的冰,你可以看到,几乎像血一样,从身体里流出。\n",
      "==================================================\n",
      "Original Text: In 25 years it's gone from this, to this.\n",
      "\n",
      "Marked Text: In <<DATE:25 years>> it's gone from this, to this.\n",
      "Entities: [('25 years', 'DATE')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  90%|██████████████████████████████████████████████████▍     | 9/10 [00:38<00:03,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 在“25年”里,它已经从这儿,变成这儿了。\n",
      "Reference Text: 在25年的时间里，它从这里，到了这里\n",
      "Translated Text: 在“25年”里,它已经从这儿,变成这儿了。\n",
      "==================================================\n",
      "Original Text: This is a problem because the warming  heats up the frozen ground around the Arctic Ocean,  where there is a massive amount of frozen carbon  which, when it thaws, is turned into methane by microbes.\n",
      "\n",
      "Marked Text: This is a problem because the warming  heats up the frozen ground around <<LOC:the Arctic Ocean>>,  where there is a massive amount of frozen carbon  which, when it thaws, is turned into methane by microbes.\n",
      "Entities: [('the Arctic Ocean', 'LOC')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set: 100%|███████████████████████████████████████████████████████| 10/10 [00:44<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: 这是一个问题,因为全球变暖加热了在“北极海洋”周围的冻土,那里有大量冰冻的碳,当它融化时,由微生物转化为甲烷。\n",
      "Reference Text: 值得注意的是 温室效应使得北冰洋周围的冻土层受热 而这里有大量被冻封的碳 解冻时，微生物降解碳形成甲烷\n",
      "Translated Text: 这是一个问题,因为全球变暖加热了在“北极海洋”周围的冻土,那里有大量冰冻的碳,当它融化时,由微生物转化为甲烷。\n",
      "==================================================\n",
      "Validation Set METEOR score: {'meteor': 0.006756756756756757}\n",
      "====================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import spacy\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 加載 IWSLT 2017 英中翻譯資料集的驗證和測試集\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation[:10]', trust_remote_code=True)\n",
    "# test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='test[:10]', trust_remote_code=True)\n",
    "\n",
    "# 加載微調後的模型和 tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"./mbart_finetuned\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"./mbart_finetuned\")\n",
    "# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "# model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 設置源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# 加載 SpaCy 英文 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# METEOR 評估指標\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "# 步驟 1：實體標記和替換\n",
    "def mark_entities(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<<{ent.label_}:{ent.text}>>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "\n",
    "# 步驟 2：翻譯帶有實體標記的句子，加入打印語句\n",
    "def translate_with_entities(text, max_length=80, length_penalty=1.2, num_beams=5):\n",
    "    marked_text, entities = mark_entities(text)\n",
    "    print(\"Original Text:\", text)\n",
    "    print(\"\")\n",
    "    print(\"Marked Text:\", marked_text)\n",
    "    print(\"Entities:\", entities)\n",
    "    print(\"\")\n",
    "    inputs = tokenizer(marked_text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        length_penalty=length_penalty,\n",
    "        num_beams=num_beams,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    print(\"Translated Text:\", translated_text)\n",
    "    # print(\"=\"*50)  # 分隔線便於區分不同句子的輸出\n",
    "\n",
    "    return translated_text, entities\n",
    "\n",
    "# 步驟 3：後處理還原實體\n",
    "def postprocess_translation(translated_text, entities):\n",
    "    for ent_text, ent_label in entities:\n",
    "        # 使用正則表達式來匹配標記，無論模型是否對標記進行了部分修改\n",
    "        entity_marker_pattern = re.escape(f\"<<{ent_label}:\") + r\"(.*?)>>\"\n",
    "        translated_text = re.sub(entity_marker_pattern, ent_text, translated_text)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 完整的 Entity-Aware 翻譯函數\n",
    "def entity_aware_translate(text):\n",
    "    translated_text, entities = translate_with_entities(text)\n",
    "    final_translation = postprocess_translation(translated_text, entities)\n",
    "    return final_translation\n",
    "\n",
    "\n",
    "# 評估 METEOR 分數\n",
    "def compute_meteor(predictions, references):\n",
    "    meteor_metric.add_batch(predictions=predictions, references=references)\n",
    "    result = meteor_metric.compute()\n",
    "    return result\n",
    "\n",
    "# 在驗證集上進行翻譯並計算 METEOR 分數\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "val_predictions = []\n",
    "val_references = []\n",
    "for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "    input_text = example[\"translation\"][\"en\"]\n",
    "    reference_text = example[\"translation\"][\"zh\"]\n",
    "    final_translation = entity_aware_translate(input_text)\n",
    "    print(\"Reference Text:\", reference_text)\n",
    "    print(\"Translated Text:\", final_translation)\n",
    "    print(\"=\" * 50)\n",
    "    val_predictions.append(final_translation)\n",
    "    val_references.append([reference_text])  # METEOR 要求 reference 為列表\n",
    "\n",
    "val_meteor_score = compute_meteor(val_predictions, val_references)\n",
    "print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "\n",
    "print(\"====================================================================\")\n",
    "# # 在測試集上進行翻譯並計算 METEOR 分數\n",
    "# print(\"Evaluating on Test Set...\")\n",
    "# test_predictions = []\n",
    "# test_references = []\n",
    "# for example in tqdm(test_dataset, desc=\"Processing Test Set\"):\n",
    "#     input_text = example[\"translation\"][\"en\"]\n",
    "#     reference_text = example[\"translation\"][\"zh\"]\n",
    "#     final_translation = entity_aware_translate(input_text)\n",
    "#     test_predictions.append(final_translation)\n",
    "#     test_references.append([reference_text])  # METEOR 要求 reference 為列表\n",
    "\n",
    "# test_meteor_score = compute_meteor(test_predictions, test_references)\n",
    "# print(\"Test Set METEOR score:\", test_meteor_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc19b86-66d2-4b24-8546-c6f7fa0bdbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
