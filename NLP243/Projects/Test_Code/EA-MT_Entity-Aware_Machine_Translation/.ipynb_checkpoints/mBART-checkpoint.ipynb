{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57dd9d8-cde1-41a1-a56b-04c7c67abc88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.7.3\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
      "     ---------------------------------------- 0.0/457.4 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/457.4 MB 640.0 kB/s eta 0:11:55\n",
      "     ---------------------------------------- 0.3/457.4 MB 2.6 MB/s eta 0:02:55\n",
      "     ---------------------------------------- 1.3/457.4 MB 9.3 MB/s eta 0:00:50\n",
      "     --------------------------------------- 4.1/457.4 MB 21.9 MB/s eta 0:00:21\n",
      "      -------------------------------------- 6.7/457.4 MB 28.5 MB/s eta 0:00:16\n",
      "      -------------------------------------- 8.0/457.4 MB 28.3 MB/s eta 0:00:16\n",
      "      ------------------------------------- 11.1/457.4 MB 50.4 MB/s eta 0:00:09\n",
      "     - ------------------------------------ 14.0/457.4 MB 54.7 MB/s eta 0:00:09\n",
      "     - ------------------------------------ 17.3/457.4 MB 59.5 MB/s eta 0:00:08\n",
      "     - ------------------------------------ 19.9/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     - ------------------------------------ 23.1/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 26.3/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 29.5/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     -- ----------------------------------- 31.9/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 34.8/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     --- ---------------------------------- 37.6/457.4 MB 65.2 MB/s eta 0:00:07\n",
      "     --- ---------------------------------- 42.0/457.4 MB 65.6 MB/s eta 0:00:07\n",
      "     --- ---------------------------------- 46.6/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     --- ---------------------------------- 47.4/457.4 MB 73.1 MB/s eta 0:00:06\n",
      "     ---- --------------------------------- 50.6/457.4 MB 54.7 MB/s eta 0:00:08\n",
      "     ---- --------------------------------- 53.4/457.4 MB 59.5 MB/s eta 0:00:07\n",
      "     ---- --------------------------------- 57.0/457.4 MB 59.5 MB/s eta 0:00:07\n",
      "     ----- -------------------------------- 60.5/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     ----- -------------------------------- 63.9/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     ----- -------------------------------- 67.6/457.4 MB 81.8 MB/s eta 0:00:05\n",
      "     ----- -------------------------------- 71.7/457.4 MB 81.8 MB/s eta 0:00:05\n",
      "     ------ ------------------------------- 74.5/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     ------ ------------------------------- 77.5/457.4 MB 81.8 MB/s eta 0:00:05\n",
      "     ------ ------------------------------- 81.4/457.4 MB 73.1 MB/s eta 0:00:06\n",
      "     ------ ------------------------------- 83.7/457.4 MB 65.6 MB/s eta 0:00:06\n",
      "     ------- ------------------------------ 86.9/457.4 MB 73.1 MB/s eta 0:00:06\n",
      "     ------- ------------------------------ 90.8/457.4 MB 65.6 MB/s eta 0:00:06\n",
      "     ------- ------------------------------ 94.4/457.4 MB 72.6 MB/s eta 0:00:06\n",
      "     -------- ----------------------------- 97.0/457.4 MB 72.6 MB/s eta 0:00:05\n",
      "     -------- ----------------------------- 99.8/457.4 MB 59.5 MB/s eta 0:00:07\n",
      "     -------- ---------------------------- 103.4/457.4 MB 72.6 MB/s eta 0:00:05\n",
      "     -------- ---------------------------- 107.2/457.4 MB 65.6 MB/s eta 0:00:06\n",
      "     -------- ---------------------------- 110.4/457.4 MB 73.1 MB/s eta 0:00:05\n",
      "     --------- --------------------------- 113.3/457.4 MB 65.6 MB/s eta 0:00:06\n",
      "     --------- --------------------------- 116.4/457.4 MB 72.6 MB/s eta 0:00:05\n",
      "     --------- --------------------------- 119.3/457.4 MB 65.6 MB/s eta 0:00:06\n",
      "     --------- --------------------------- 122.7/457.4 MB 65.2 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 125.8/457.4 MB 65.2 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 127.5/457.4 MB 59.8 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 130.2/457.4 MB 59.5 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 133.6/457.4 MB 59.5 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 136.7/457.4 MB 59.5 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 139.7/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     ----------- ------------------------- 143.4/457.4 MB 72.6 MB/s eta 0:00:05\n",
      "     ----------- ------------------------- 147.3/457.4 MB 72.6 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 150.5/457.4 MB 81.8 MB/s eta 0:00:04\n",
      "     ------------ ------------------------ 154.2/457.4 MB 73.1 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 156.4/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     ------------ ------------------------ 160.1/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     ------------- ----------------------- 163.1/457.4 MB 73.1 MB/s eta 0:00:05\n",
      "     ------------- ----------------------- 168.0/457.4 MB 72.6 MB/s eta 0:00:04\n",
      "     ------------- ----------------------- 172.4/457.4 MB 81.8 MB/s eta 0:00:04\n",
      "     -------------- ---------------------- 176.1/457.4 MB 81.8 MB/s eta 0:00:04\n",
      "     -------------- ---------------------- 178.8/457.4 MB 73.1 MB/s eta 0:00:04\n",
      "     -------------- ---------------------- 181.8/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     -------------- ---------------------- 184.6/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 187.5/457.4 MB 65.6 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 190.7/457.4 MB 65.2 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 194.9/457.4 MB 72.6 MB/s eta 0:00:04\n",
      "     --------------- --------------------- 197.1/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 200.2/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 203.4/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 206.8/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 209.4/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 212.7/457.4 MB 73.1 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 215.4/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 218.4/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 220.4/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 221.3/457.4 MB 50.4 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 222.0/457.4 MB 43.5 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 224.8/457.4 MB 43.7 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 227.6/457.4 MB 38.5 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 231.9/457.4 MB 54.7 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 235.0/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 237.2/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 240.1/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 243.4/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 246.2/457.4 MB 65.6 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 249.3/457.4 MB 65.2 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 252.1/457.4 MB 65.2 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 254.4/457.4 MB 59.8 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 256.8/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 260.0/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 262.2/457.4 MB 54.7 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 265.4/457.4 MB 54.4 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 267.7/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 270.5/457.4 MB 54.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 272.7/457.4 MB 54.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 275.3/457.4 MB 50.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 278.5/457.4 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 280.0/457.4 MB 54.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 282.8/457.4 MB 54.7 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 285.1/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 288.3/457.4 MB 50.4 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 292.8/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 295.2/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 297.2/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 300.2/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 302.7/457.4 MB 54.4 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 306.0/457.4 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 308.3/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 310.3/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 313.1/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 316.2/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 319.3/457.4 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 321.5/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 324.3/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 326.7/457.4 MB 54.4 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 329.4/457.4 MB 59.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 331.2/457.4 MB 54.4 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 334.9/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 337.2/457.4 MB 54.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 341.6/457.4 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 345.8/457.4 MB 72.6 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 350.6/457.4 MB 72.6 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 354.5/457.4 MB 72.6 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 355.5/457.4 MB 59.8 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 356.1/457.4 MB 50.4 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 358.6/457.4 MB 43.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 360.9/457.4 MB 43.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 363.3/457.4 MB 38.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 365.7/457.4 MB 43.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 368.0/457.4 MB 54.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 370.5/457.4 MB 50.1 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 373.7/457.4 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 375.9/457.4 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 378.2/457.4 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 381.8/457.4 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 384.8/457.4 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 387.5/457.4 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 391.1/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 393.9/457.4 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 396.7/457.4 MB 54.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 399.6/457.4 MB 54.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 402.7/457.4 MB 54.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 407.0/457.4 MB 72.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 409.0/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 409.7/457.4 MB 50.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 413.2/457.4 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 414.9/457.4 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 417.6/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 419.6/457.4 MB 50.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 421.8/457.4 MB 50.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 424.4/457.4 MB 50.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 427.5/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 428.6/457.4 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 430.1/457.4 MB 46.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 433.0/457.4 MB 43.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 436.4/457.4 MB 38.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 439.9/457.4 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 442.4/457.4 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  445.6/457.4 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  449.4/457.4 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  452.8/457.4 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  454.9/457.4 MB 50.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- 457.4/457.4 MB 13.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from en-core-web-trf==3.7.3) (3.7.2)\n",
      "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from en-core-web-trf==3.7.3)\n",
      "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.10.18)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.23.5)\n",
      "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
      "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
      "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
      "  Downloading curated_tokenizers-0.0.9-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.4.1)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.9.11)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
      "Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
      "   ---------------------------------------- 0.0/236.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/236.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 236.3/236.3 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading curated_tokenizers-0.0.9-cp38-cp38-win_amd64.whl (732 kB)\n",
      "   ---------------------------------------- 0.0/732.8 kB ? eta -:--:--\n",
      "   ----------------------------------- --- 675.8/732.8 kB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 732.8/732.8 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
      "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.7.3 spacy-curated-transformers-0.2.2\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f070689-6b58-4bb5-be4e-f01313d03275",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# mbart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e81cb8-f062-4a9b-930f-a5b3e5cf73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 2:36:55, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.319304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.235552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.284738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.494404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.863410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.396840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.110349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.014125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0: No log history available for this epoch.\n",
      "Epoch 2.0: Training Loss N/A, Validation Loss 10.319303512573242\n",
      "Epoch 3.0: Training Loss N/A, Validation Loss 9.235551834106445\n",
      "Epoch 4.0: Training Loss N/A, Validation Loss 8.284737586975098\n",
      "Epoch 5.0: Training Loss N/A, Validation Loss 7.494403839111328\n",
      "Epoch 6.0: Training Loss N/A, Validation Loss 6.863409519195557\n",
      "Epoch 7.0: Training Loss N/A, Validation Loss 6.3968400955200195\n",
      "Epoch 8.0: Training Loss N/A, Validation Loss 6.110349178314209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Processing Validation Set:   0%|                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.\n",
      "Reference Text: 去年我给各位展示了两个 关于北极冰帽的演示 在过去三百万年中 其面积由相当于美国南方48州面积总和 缩减了40%\n",
      "Translated Text: 这是说明北极的冰盖在最近的300万年中大部分时间都是下48个州的大小,已经缩小了40%。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  10%|█████▌                                                  | 1/10 [00:10<01:32, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.\n",
      "Reference Text: 但这些没能完全说明这个问题的严重性 因为这没有表示出冰帽的厚度\n",
      "Translated Text: 但这不足以说明这个问题的严重性,因为它不显示冰的厚度。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  20%|███████████▏                                            | 2/10 [00:16<01:03,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The arctic ice cap is, in a sense,  the beating heart of the global climate system.\n",
      "Reference Text: 感觉上，北极冰帽 就好象全球气候系统中跳动的心脏\n",
      "Translated Text: 北极冰盖是全球气候体系的心脏。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  30%|████████████████▊                                       | 3/10 [00:20<00:43,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: It expands in winter and contracts in summer.\n",
      "Reference Text: 冬天心脏舒张，夏天心脏收缩\n",
      "Translated Text: 是的,是的,是的,是的,是的,是的,是的。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  40%|██████████████████████▍                                 | 4/10 [00:27<00:37,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.\n",
      "Reference Text: 下面我要展示的是 在过去25年里的极剧变化\n",
      "Translated Text: 下面的幻灯片是过去25年发生的事的快速转动。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  50%|████████████████████████████                            | 5/10 [00:32<00:29,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The permanent ice is marked in red.\n",
      "Reference Text: 红色的是永冻冰\n",
      "Translated Text: 永久的冰是红色的。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  60%|█████████████████████████████████▌                      | 6/10 [00:34<00:19,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: As you see, it expands to the dark blue --  that's the annual ice in winter,  and it contracts in summer.\n",
      "Reference Text: 你看，它正在变成深蓝色 这是每年冬天形成的年度冰 在夏天永冻冰收缩\n",
      "Translated Text: 正如你所看到的那样,它扩展到深蓝色,那是冬天的冰,夏天的冰收缩。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  70%|███████████████████████████████████████▏                | 7/10 [00:42<00:16,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The so-called permanent ice, five years old or older,  you can see is almost like blood,  spilling out of the body here.\n",
      "Reference Text: 所谓的“永冻”，是指形成五年或更久的冰 你看，这也像血液一样 输送到身体各部位\n",
      "Translated Text: 这里所谓的永久性冰(五岁或更老的)几乎像血一样,从身体里流出。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  80%|████████████████████████████████████████████▊           | 8/10 [00:49<00:12,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: In 25 years it's gone from this, to this.\n",
      "Reference Text: 在25年的时间里，它从这里，到了这里\n",
      "Translated Text: 在过去的25年里,它已经从这里变为这里了。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  90%|██████████████████████████████████████████████████▍     | 9/10 [00:54<00:05,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: This is a problem because the warming  heats up the frozen ground around the Arctic Ocean,  where there is a massive amount of frozen carbon  which, when it thaws, is turned into methane by microbes.\n",
      "Reference Text: 值得注意的是 温室效应使得北冰洋周围的冻土层受热 而这里有大量被冻封的碳 解冻时，微生物降解碳形成甲烷\n",
      "Translated Text: 这是一个问题,因为加热会加热冰冻的地区,那里有大量冰冻的碳,当它融化时,由微生物变为甲烷。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set: 100%|███████████████████████████████████████████████████████| 10/10 [01:05<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set METEOR score: {'meteor': 0.00847457627118644}\n",
      "Validation Set BLEU score: {'bleu': 0.0, 'precisions': [0.02, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.0, 'translation_length': 50, 'reference_length': 25}\n",
      "Validation Set ROUGE score: {'rouge1': 0.27999999999999997, 'rouge2': 0.06666666666666667, 'rougeL': 0.27999999999999997, 'rougeLsum': 0.27999999999999997}\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.17 seconds, 59.38 sentences/sec\n",
      "BERTScore - Precision: 0.745878279209137, Recall: 0.7331073880195618, F1: 0.7390149831771851\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, AdamW, EarlyStoppingCallback, TrainerCallback\n",
    "import torch\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 載入 SpaCy NER 模型\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:200]', trust_remote_code=True)\n",
    "\n",
    "# 載入 mBART 模型和 Tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"zh_CN\"\n",
    "\n",
    "# 實體識別和標記的函數\n",
    "def mark_entities(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<<{ent.label_}:{ent.text}>>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "# 預處理函數\n",
    "def preprocess_function(examples):\n",
    "    inputs, targets, entities_list = [], [], []\n",
    "    for ex in examples[\"translation\"]:\n",
    "        marked_text, entities = mark_entities(ex[\"en\"])\n",
    "        inputs.append(marked_text)\n",
    "        targets.append(ex[\"zh\"])\n",
    "        entities_list.append(entities)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"entities\"] = entities_list\n",
    "    return model_inputs\n",
    "\n",
    "# 對數據集進行 Tokenize 和實體標記\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 訓練參數設置\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# 自定義 callback 以顯示每個 epoch 的損失\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # 確保 log_history 非空，避免 IndexError\n",
    "        if state.log_history:\n",
    "            last_log = state.log_history[-1]\n",
    "            training_loss = last_log.get(\"loss\", \"N/A\")\n",
    "            eval_loss = last_log.get(\"eval_loss\", \"N/A\")\n",
    "            print(f\"Epoch {state.epoch}: Training Loss {training_loss}, Validation Loss {eval_loss}\")\n",
    "        else:\n",
    "            print(f\"Epoch {state.epoch}: No log history available for this epoch.\")\n",
    "\n",
    "# 將數據集劃分為訓練集和驗證集\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# 自定義 Layer-wise Learning Rate Decay\n",
    "layer_decay = 0.8\n",
    "optimizer_grouped_parameters = []\n",
    "num_layers = len(model.model.encoder.layers)\n",
    "for i, layer in enumerate(model.model.encoder.layers):\n",
    "    lr = training_args.learning_rate * (layer_decay ** (num_layers - i - 1))\n",
    "    optimizer_grouped_parameters.append({\"params\": layer.parameters(), \"lr\": lr})\n",
    "optimizer_grouped_parameters.append({\"params\": model.model.shared.parameters(), \"lr\": training_args.learning_rate})\n",
    "\n",
    "# 初始化 optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)\n",
    "\n",
    "# 自定義 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), LogCallback()],\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# 後處理實體標記\n",
    "def postprocess_translation(translated_text, entities):\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker_pattern = re.escape(f\"<<{ent_label}:\") + r\"(.*?)>>\"\n",
    "        translated_text = re.sub(entity_marker_pattern, ent_text, translated_text, count=1)\n",
    "    return translated_text\n",
    "\n",
    "# 使用訓練好的模型進行翻譯並還原實體\n",
    "def entity_aware_translate(input_text):\n",
    "    marked_text, entities = mark_entities(input_text)\n",
    "    inputs = tokenizer(marked_text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=256,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return postprocess_translation(translated_text, entities)\n",
    "\n",
    "# 驗證指標設置\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# 評估函數\n",
    "def evaluate_model(val_dataset):\n",
    "    val_predictions, val_references = [], []\n",
    "    for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        reference_text = example[\"translation\"][\"zh\"]\n",
    "\n",
    "        final_translation = entity_aware_translate(input_text)\n",
    "        val_predictions.append(final_translation)\n",
    "        val_references.append([reference_text])\n",
    "\n",
    "        print(\"Original English Text:\", input_text)\n",
    "        print(\"Reference Text:\", reference_text)\n",
    "        print(\"Translated Text:\", final_translation)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    # 計算各項評分\n",
    "    val_meteor_score = meteor_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    val_bleu_score = bleu_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    val_rouge_score = rouge_metric.compute(predictions=val_predictions, references=val_references)\n",
    "\n",
    "    print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "    print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "    print(\"Validation Set ROUGE score:\", val_rouge_score)\n",
    "\n",
    "    # BERTScore 計算\n",
    "    P, R, F1 = bert_score(val_predictions, [ref[0] for ref in val_references], lang=\"zh\", verbose=True)\n",
    "    print(f\"BERTScore - Precision: {P.mean().item()}, Recall: {R.mean().item()}, F1: {F1.mean().item()}\")\n",
    "\n",
    "# 執行評估\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation[:10]', trust_remote_code=True)\n",
    "evaluate_model(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9837e3-c826-4ba3-a1a2-0655c5f92927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from datasets import load_dataset\n",
    "# from transformers import MBartForConditionalGeneration, MBart50Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, AdamW, EarlyStoppingCallback, TrainerCallback\n",
    "# import torch\n",
    "# import re\n",
    "# import warnings\n",
    "# from tqdm import tqdm\n",
    "# import evaluate\n",
    "# from bert_score import score as bert_score\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # 設定設備\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # 載入 SpaCy NER 模型\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# # 載入 IWSLT 2017 英中翻譯資料集\n",
    "# dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2]', trust_remote_code=True)\n",
    "\n",
    "# # 載入 mBART 模型和 Tokenizer\n",
    "# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "# model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # 設定源語言和目標語言\n",
    "# tokenizer.src_lang = \"en_XX\"\n",
    "# tokenizer.tgt_lang = \"zh_CN\"\n",
    "\n",
    "# # 實體識別和標記的函數\n",
    "# def mark_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     modified_text = text\n",
    "#     entities = []\n",
    "#     for ent in doc.ents:\n",
    "#         # 使用新的實體標記格式\n",
    "#         entity_marker = f\"<ENTITY type=\\\"{ent.label_}\\\">{ent.text}</ENTITY>\"\n",
    "#         modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "#         entities.append((ent.text, ent.label_))\n",
    "#     return modified_text, entities\n",
    "\n",
    "# # 預處理函數\n",
    "# def preprocess_function(examples):\n",
    "#     inputs, targets, entities_list = [], [], []\n",
    "#     for ex in examples[\"translation\"]:\n",
    "#         marked_text, entities = mark_entities(ex[\"en\"])\n",
    "#         inputs.append(marked_text)\n",
    "#         targets.append(ex[\"zh\"])\n",
    "#         entities_list.append(entities)\n",
    "    \n",
    "#     model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "#     labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     model_inputs[\"entities\"] = entities_list\n",
    "#     return model_inputs\n",
    "\n",
    "# # 對數據集進行 Tokenize 和實體標記\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# # 訓練參數設置\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./mbart_finetuned\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=1,\n",
    "#     predict_with_generate=True,\n",
    "#     logging_dir='./logs',\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "# # 自定義 callback 以顯示每個 epoch 的 Training 和 Validation 損失\n",
    "# class LogCallback(TrainerCallback):\n",
    "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
    "#         if state.log_history:\n",
    "#             last_log = state.log_history[-1]\n",
    "#             training_loss = last_log.get(\"loss\", \"N/A\")\n",
    "#             eval_loss = last_log.get(\"eval_loss\", \"N/A\")\n",
    "#             print(f\"Epoch {state.epoch}: Training Loss {training_loss}, Validation Loss {eval_loss}\")\n",
    "#         else:\n",
    "#             print(f\"Epoch {state.epoch}: No log history available for this epoch.\")\n",
    "\n",
    "# # 將數據集劃分為訓練集和驗證集\n",
    "# train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset = train_test_split[\"train\"]\n",
    "# eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# # 自定義 Layer-wise Learning Rate Decay\n",
    "# layer_decay = 0.8\n",
    "# optimizer_grouped_parameters = []\n",
    "# num_layers = len(model.model.encoder.layers)\n",
    "# for i, layer in enumerate(model.model.encoder.layers):\n",
    "#     lr = training_args.learning_rate * (layer_decay ** (num_layers - i - 1))\n",
    "#     optimizer_grouped_parameters.append({\"params\": layer.parameters(), \"lr\": lr})\n",
    "# optimizer_grouped_parameters.append({\"params\": model.model.shared.parameters(), \"lr\": training_args.learning_rate})\n",
    "\n",
    "# # 初始化 optimizer\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)\n",
    "\n",
    "# # 自定義 Trainer\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     optimizers=(optimizer, None),\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3), LogCallback()],\n",
    "# )\n",
    "\n",
    "# # 開始訓練\n",
    "# trainer.train()\n",
    "\n",
    "# # 後處理實體標記\n",
    "# def postprocess_translation(translated_text, entities):\n",
    "#     for ent_text, ent_label in entities:\n",
    "#         entity_marker_pattern = re.escape(f\"<ENTITY type=\\\"{ent_label}\\\">\") + r\"(.*?)</ENTITY>\"\n",
    "#         translated_text = re.sub(entity_marker_pattern, ent_text, translated_text, count=1)\n",
    "#     return translated_text\n",
    "\n",
    "# # 使用訓練好的模型進行翻譯並還原實體\n",
    "# def entity_aware_translate(input_text):\n",
    "#     marked_text, entities = mark_entities(input_text)\n",
    "#     inputs = tokenizer(marked_text, return_tensors=\"pt\").to(device)\n",
    "#     translated_tokens = model.generate(\n",
    "#         inputs[\"input_ids\"],\n",
    "#         max_length=256,\n",
    "#         forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "#     )\n",
    "#     translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "#     return postprocess_translation(translated_text, entities)\n",
    "\n",
    "# # 驗證指標設置\n",
    "# meteor_metric = evaluate.load(\"meteor\")\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "# rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# # 評估函數\n",
    "# def evaluate_model(val_dataset):\n",
    "#     val_predictions, val_references = [], []\n",
    "#     for example in tqdm(val_dataset, desc=\"Processing Validation Set\"):\n",
    "#         input_text = example[\"translation\"][\"en\"]\n",
    "#         reference_text = example[\"translation\"][\"zh\"]\n",
    "\n",
    "#         final_translation = entity_aware_translate(input_text)\n",
    "#         val_predictions.append(final_translation)\n",
    "#         val_references.append([reference_text])\n",
    "\n",
    "#         print(\"Original English Text:\", input_text)\n",
    "#         print(\"Reference Text:\", reference_text)\n",
    "#         print(\"Translated Text:\", final_translation)\n",
    "#         print(\"=\" * 50)\n",
    "\n",
    "#     # 計算各項評分\n",
    "#     val_meteor_score = meteor_metric.compute(predictions=val_predictions, references=val_references)\n",
    "#     val_bleu_score = bleu_metric.compute(predictions=val_predictions, references=val_references)\n",
    "#     val_rouge_score = rouge_metric.compute(predictions=val_predictions, references=val_references)\n",
    "\n",
    "#     print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "#     print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "#     print(\"Validation Set ROUGE score:\", val_rouge_score)\n",
    "\n",
    "#     # BERTScore 計算\n",
    "#     P, R, F1 = bert_score(val_predictions, [ref[0] for ref in val_references], lang=\"zh\", verbose=True)\n",
    "#     print(f\"BERTScore - Precision: {P.mean().item()}, Recall: {R.mean().item()}, F1: {F1.mean().item()}\")\n",
    "\n",
    "# # 執行評估\n",
    "# val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation[:10]', trust_remote_code=True)\n",
    "# evaluate_model(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1820c-4656-4283-8212-4cdece3787b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f95c753-62d0-48d4-b988-89d0ec87566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  10%|█████▌                                                  | 1/10 [00:01<00:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.\n",
      "Reference Text: 去年我给各位展示了两个 关于北极冰帽的演示 在过去三百万年中 其面积由相当于美国南方48州面积总和 缩减了40%\n",
      "Translated Text: 去年我展示了这两张幻灯片,以证明北极冰盖,在过去的3百万年中大部分时间都是48个较低的州的大小,已经缩小了40%。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  20%|███████████▏                                            | 2/10 [00:01<00:06,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.\n",
      "Reference Text: 但这些没能完全说明这个问题的严重性 因为这没有表示出冰帽的厚度\n",
      "Translated Text: 但这不足以说明该问题的严重性,因为它没有显示冰的厚度。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  30%|████████████████▊                                       | 3/10 [00:02<00:04,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The arctic ice cap is, in a sense,  the beating heart of the global climate system.\n",
      "Reference Text: 感觉上，北极冰帽 就好象全球气候系统中跳动的心脏\n",
      "Translated Text: 北极冰盖在某种意义上是全球气候体系的心脏。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  40%|██████████████████████▍                                 | 4/10 [00:02<00:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: It expands in winter and contracts in summer.\n",
      "Reference Text: 冬天心脏舒张，夏天心脏收缩\n",
      "Translated Text: 它在冬天扩张,在夏天收缩。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  50%|████████████████████████████                            | 5/10 [00:03<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.\n",
      "Reference Text: 下面我要展示的是 在过去25年里的极剧变化\n",
      "Translated Text: 下面的幻灯片将是过去25年发生的快速转动。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  60%|█████████████████████████████████▌                      | 6/10 [00:03<00:01,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The permanent ice is marked in red.\n",
      "Reference Text: 红色的是永冻冰\n",
      "Translated Text: 永久性冰被标记为红色。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  70%|███████████████████████████████████████▏                | 7/10 [00:04<00:01,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: As you see, it expands to the dark blue --  that's the annual ice in winter,  and it contracts in summer.\n",
      "Reference Text: 你看，它正在变成深蓝色 这是每年冬天形成的年度冰 在夏天永冻冰收缩\n",
      "Translated Text: 正如你看到的,它扩展到深蓝色,那是每年冬天的冰,而夏天它收缩。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  80%|████████████████████████████████████████████▊           | 8/10 [00:04<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: The so-called permanent ice, five years old or older,  you can see is almost like blood,  spilling out of the body here.\n",
      "Reference Text: 所谓的“永冻”，是指形成五年或更久的冰 你看，这也像血液一样 输送到身体各部位\n",
      "Translated Text: 所谓的永久性冰,五岁或以上,你可以看到几乎像血一样,从身体里流出。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set:  90%|██████████████████████████████████████████████████▍     | 9/10 [00:05<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: In 25 years it's gone from this, to this.\n",
      "Reference Text: 在25年的时间里，它从这里，到了这里\n",
      "Translated Text: 25年后,它从这里变为这里。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Set: 100%|███████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Text: This is a problem because the warming  heats up the frozen ground around the Arctic Ocean,  where there is a massive amount of frozen carbon  which, when it thaws, is turned into methane by microbes.\n",
      "Reference Text: 值得注意的是 温室效应使得北冰洋周围的冻土层受热 而这里有大量被冻封的碳 解冻时，微生物降解碳形成甲烷\n",
      "Translated Text: 这是一个问题,因为加热加热了北极大洋周围的冻土,那里有大量的冰冻碳,当它融化时,由微生物转化为甲烷。\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set METEOR score: {'meteor': 0.007936507936507938}\n",
      "Validation Set BLEU score: {'bleu': 0.0, 'precisions': [0.022727272727272728, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.76, 'translation_length': 44, 'reference_length': 25}\n",
      "Validation Set ROUGE score: {'rouge1': 0.27999999999999997, 'rouge2': 0.06666666666666667, 'rougeL': 0.27999999999999997, 'rougeLsum': 0.27999999999999997}\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 96.23 sentences/sec\n",
      "BERTScore - Precision: 0.766173243522644, Recall: 0.7631586790084839, F1: 0.76434725522995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 載入 SpaCy NER 模型\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='validation[:10]', trust_remote_code=True)\n",
    "\n",
    "# 載入預訓練的 mBART 模型和 Tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"  # 英文\n",
    "tokenizer.tgt_lang = \"zh_CN\"  # 簡體中文\n",
    "\n",
    "# 使用預訓練模型進行翻譯\n",
    "def translate_text(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]  # 強制翻譯成中文\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# 評估模型的表現\n",
    "def evaluate_model(dataset):\n",
    "    val_predictions = []\n",
    "    val_references = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Processing Validation Set\"):\n",
    "        input_text = example[\"translation\"][\"en\"]\n",
    "        reference_text = example[\"translation\"][\"zh\"]\n",
    "        \n",
    "        # 進行翻譯\n",
    "        final_translation = translate_text(input_text)\n",
    "        val_predictions.append(final_translation)\n",
    "        val_references.append([reference_text])\n",
    "        \n",
    "        # 顯示翻譯結果\n",
    "        print(\"Original English Text:\", input_text)\n",
    "        print(\"Reference Text:\", reference_text)\n",
    "        print(\"Translated Text:\", final_translation)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    # 計算 METEOR, BLEU 和 ROUGE 分數\n",
    "    meteor_metric = evaluate.load(\"meteor\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    val_meteor_score = meteor_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    val_bleu_score = bleu_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    val_rouge_score = rouge_metric.compute(predictions=val_predictions, references=val_references)\n",
    "    \n",
    "    print(\"Validation Set METEOR score:\", val_meteor_score)\n",
    "    print(\"Validation Set BLEU score:\", val_bleu_score)\n",
    "    print(\"Validation Set ROUGE score:\", val_rouge_score)\n",
    "\n",
    "    # 計算 BERTScore\n",
    "    P, R, F1 = bert_score(val_predictions, [ref[0] for ref in val_references], lang=\"zh\", verbose=True)\n",
    "    print(f\"BERTScore - Precision: {P.mean().item()}, Recall: {R.mean().item()}, F1: {F1.mean().item()}\")\n",
    "\n",
    "# 執行評估\n",
    "evaluate_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f2920-fa8b-4d8a-8b3a-2c881b415585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
