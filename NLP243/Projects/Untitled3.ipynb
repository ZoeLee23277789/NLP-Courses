{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ff5a0c-60fc-4fb1-90c1-c7ae011a0440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing English NER: 100%|██████████████████████████████████████████████████████| 2000/2000 [09:41<00:00,  3.44it/s]\n",
      "Processing Chinese NER: 100%|██████████████████████████████████████████████████████| 2000/2000 [09:58<00:00,  3.34it/s]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\USER\\\\Downloads\\\\NLP-Courses\\\\NLP243\\\\Projects\\\\Test_iwslt2017_ner_tagged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m\n\u001b[0;32m     29\u001b[0m ner_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: english_sentences,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish NER Tagged\u001b[39m\u001b[38;5;124m\"\u001b[39m: english_ner_results,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChinese Sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: chinese_sentences,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChinese NER Tagged\u001b[39m\u001b[38;5;124m\"\u001b[39m: chinese_ner_results\n\u001b[0;32m     34\u001b[0m })\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 保存 NER 標記結果到 CSV 文件\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mner_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUSER\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNLP-Courses\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNLP243\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTest_iwslt2017_ner_tagged.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 定義函數以用指定格式替換實體\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_with_tags\u001b[39m(tagged_words):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\USER\\\\Downloads\\\\NLP-Courses\\\\NLP243\\\\Projects\\\\Test_iwslt2017_ner_tagged.csv'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Hugging Face 的多語言 NER pipeline，載入 XLM-R 模型\n",
    "ner_pipeline = pipeline(\"ner\", model=\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "\n",
    "# 使用 datasets 套件載入 IWSLT 2017 英中翻譯資料集，僅取前 2000 筆數據\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2000]', trust_remote_code=True)\n",
    "\n",
    "# 提取中英文句子\n",
    "english_sentences = [example['translation']['en'] for example in dataset]\n",
    "chinese_sentences = [example['translation']['zh'] for example in dataset]\n",
    "\n",
    "# 進行英文命名實體識別\n",
    "english_ner_results = []\n",
    "for sentence in tqdm(english_sentences, desc=\"Processing English NER\"):\n",
    "    tagged_words = ner_pipeline(sentence)\n",
    "    english_ner_results.append(tagged_words)\n",
    "\n",
    "# 進行中文命名實體識別\n",
    "chinese_ner_results = []\n",
    "for sentence in tqdm(chinese_sentences, desc=\"Processing Chinese NER\"):\n",
    "    tagged_words = ner_pipeline(sentence)\n",
    "    chinese_ner_results.append(tagged_words)\n",
    "\n",
    "# 將結果轉換為 DataFrame 便於檢視\n",
    "ner_df = pd.DataFrame({\n",
    "    \"English Sentence\": english_sentences,\n",
    "    \"English NER Tagged\": english_ner_results,\n",
    "    \"Chinese Sentence\": chinese_sentences,\n",
    "    \"Chinese NER Tagged\": chinese_ner_results\n",
    "})\n",
    "\n",
    "# 保存 NER 標記結果到 CSV 文件\n",
    "ner_df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Test_tagged.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53ad5a5-10b0-480d-b17d-528190217f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 NER 標記結果到 CSV 文件\n",
    "ner_df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Test_tagged.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca390584-75bd-40d8-a581-9c45679a20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tagged CSV file for processing entity merging\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the uploaded tagged file\n",
    "file_path = './Test_tagged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to merge segmented entities based on their type\n",
    "# Update the merge function for Chinese to only merge entities with the same tag and strictly consecutive start:end positions\n",
    "def merge_entities_for_chinese(entities):\n",
    "    merged_entities = []\n",
    "    temp_entity = \"\"\n",
    "    temp_tag = None\n",
    "    temp_score = 1.0  # Start with a high confidence score for the entity being merged\n",
    "    temp_end = None  # Track the end position of the last added word\n",
    "\n",
    "    for entity in entities:\n",
    "        word, tag, score, start, end = entity['word'].replace(\"▁\", \"\"), entity['entity'], entity['score'], entity['start'], entity['end']\n",
    "\n",
    "        # Check if we should start a new entity based on tag or strictly consecutive positions (start == temp_end)\n",
    "        if temp_tag is None or temp_tag != tag or (temp_end is not None and temp_end != start):\n",
    "            # If we are starting a new entity, tag changes, or start position is not strictly consecutive, store the previous entity\n",
    "            if temp_entity:\n",
    "                merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "            # Start a new entity\n",
    "            temp_entity = word\n",
    "            temp_tag = tag\n",
    "            temp_score = score\n",
    "            temp_end = end\n",
    "        else:\n",
    "            # Continue the current entity if the tag is the same and positions are strictly consecutive\n",
    "            temp_entity += word\n",
    "            temp_score = min(temp_score, score)  # Track the minimum score as the overall confidence\n",
    "            temp_end = end  # Update end position for consecutive check\n",
    "\n",
    "    # Add the final entity if any\n",
    "    if temp_entity:\n",
    "        merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Apply the updated merging function for English and the new function for Chinese\n",
    "merged_english_entities = []\n",
    "merged_chinese_entities = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    english_entities = eval(row['English NER Tagged']) if row['English NER Tagged'] != \"[]\" else []\n",
    "    chinese_entities = eval(row['Chinese NER Tagged']) if row['Chinese NER Tagged'] != \"[]\" else []\n",
    "\n",
    "    # Apply merge function with space for English entities, and strict consecutive merge for Chinese entities\n",
    "    merged_english_entities.append(merge_entities_with_space_for_english(english_entities))\n",
    "    merged_chinese_entities.append(merge_entities_for_chinese(chinese_entities))\n",
    "\n",
    "# Add merged entities to the DataFrame\n",
    "df['Merged English NER Tagged'] = merged_english_entities\n",
    "df['Merged Chinese NER Tagged'] = merged_chinese_entities\n",
    "\n",
    "# Display the modified DataFrame with merged entities\n",
    "df[['English Sentence', 'Merged English NER Tagged', 'Chinese Sentence', 'Merged Chinese NER Tagged']].head()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# 保存替換後的 NER 標記結果到 CSV 文件\n",
    "df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Entity.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802d1186-c879-472a-8f50-884ca8f4567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest tagged CSV file for processing\n",
    "file_path = './Test_tagged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to merge segmented entitie# Update the merge function to only combine entities with the same tag and consecutive start:end positions\n",
    "# Update the merge function to handle cases where consecutive entities in English require a space between them\n",
    "def merge_entities_with_space_for_english(entities):\n",
    "    merged_entities = []\n",
    "    temp_entity = \"\"\n",
    "    temp_tag = None\n",
    "    temp_score = 1.0  # Start with a high confidence score for the entity being merged\n",
    "    temp_end = None  # Track the end position of the last added word\n",
    "\n",
    "    for entity in entities:\n",
    "        word, tag, score, start, end = entity['word'].replace(\"▁\", \"\"), entity['entity'], entity['score'], entity['start'], entity['end']\n",
    "\n",
    "        # Check if we should start a new entity based on tag or position (end + 1 == start for consecutive check)\n",
    "        if temp_tag is None or temp_tag != tag or (temp_end is not None and temp_end + 1 < start):\n",
    "            # If we are starting a new entity, tag changes, or start position is not consecutive, store the previous entity\n",
    "            if temp_entity:\n",
    "                merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "            # Start a new entity\n",
    "            temp_entity = word\n",
    "            temp_tag = tag\n",
    "            temp_score = score\n",
    "            temp_end = end\n",
    "        else:\n",
    "            # Continue the current entity if the tag is the same\n",
    "            # Add a space if there's a gap between the previous end and current start\n",
    "            if temp_end + 1 == start:\n",
    "                temp_entity += \" \" + word\n",
    "            else:\n",
    "                temp_entity += word\n",
    "            temp_score = min(temp_score, score)  # Track the minimum score as the overall confidence\n",
    "            temp_end = end  # Update end position for consecutive check\n",
    "\n",
    "    # Add the final entity if any\n",
    "    if temp_entity:\n",
    "        merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Apply the updated merging function only on the English NER tagged results\n",
    "merged_english_entities = []\n",
    "merged_chinese_entities = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    english_entities = eval(row['English NER Tagged']) if row['English NER Tagged'] != \"[]\" else []\n",
    "    chinese_entities = eval(row['Chinese NER Tagged']) if row['Chinese NER Tagged'] != \"[]\" else []\n",
    "\n",
    "    # Apply merge function with space for English entities, and regular merge for Chinese entities\n",
    "    merged_english_entities.append(merge_entities_with_space_for_english(english_entities))\n",
    "    merged_chinese_entities.append(merge_entities_by_tag_and_consecutive_extended(chinese_entities))\n",
    "\n",
    "# Add merged entities to the DataFrame\n",
    "df['Merged English NER Tagged'] = merged_english_entities\n",
    "df['Merged Chinese NER Tagged'] = merged_chinese_entities\n",
    "\n",
    "# Display the modified DataFrame with merged entities\n",
    "df[['English Sentence', 'Merged English NER Tagged', 'Chinese Sentence', 'Merged Chinese NER Tagged']].head()\n",
    "\n",
    "\n",
    "# 保存替換後的 NER 標記結果到 CSV 文件\n",
    "df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Entity.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d6b7677-72bb-4a94-9f40-3e71446c6951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實體翻譯詞典:\n",
      "英文實體: Tipper (I-PER)  ->  中文實體: Tipper (I-PER)\n",
      "英文實體: Shoney's (I-ORG)  ->  中文實體: Shoney' (I-ORG)\n",
      "英文實體: Ashraf (I-PER)  ->  中文實體: Ashraf (I-PER)\n",
      "英文實體: CA (I-ORG)  ->  中文實體: CA (I-ORG)\n",
      "英文實體: AG (I-ORG)  ->  中文實體: AG (I-ORG)\n",
      "英文實體: DC8 (I-MISC)  ->  中文實體: DC8 (I-MISC)\n",
      "英文實體: SR71 (I-MISC)  ->  中文實體: SR71 (I-MISC)\n",
      "英文實體: IBM (I-ORG)  ->  中文實體: IBM (I-ORG)\n",
      "英文實體: Mac (I-MISC)  ->  中文實體: Mac (I-MISC)\n",
      "英文實體: TED (I-ORG)  ->  中文實體: TED (I-ORG)\n",
      "英文實體: DOS (I-MISC)  ->  中文實體: DOS (I-MISC)\n",
      "英文實體: Photoshop (I-MISC)  ->  中文實體: Photoshop (I-MISC)\n",
      "英文實體: Windows (I-MISC)  ->  中文實體: Windows (I-MISC)\n",
      "英文實體: Windows PC (I-MISC)  ->  中文實體: Windows (I-MISC)\n",
      "英文實體: Palm (I-ORG)  ->  中文實體: Palm (I-ORG)\n",
      "英文實體: Word (I-MISC)  ->  中文實體: Word (I-MISC)\n",
      "英文實體: Office (I-MISC)  ->  中文實體: Office (I-MISC)\n",
      "英文實體: Woz (I-PER)  ->  中文實體: Woz (I-PER)\n",
      "英文實體: iPod (I-MISC)  ->  中文實體: iPod (I-MISC)\n",
      "英文實體: Sonos (I-ORG)  ->  中文實體: Sonos (I-ORG)\n",
      "英文實體: CA (I-LOC)  ->  中文實體: CA (I-LOC)\n",
      "英文實體: JS (I-PER)  ->  中文實體: JS (I-PER)\n",
      "英文實體: iBOT (I-MISC)  ->  中文實體: BOT (I-MISC)\n",
      "英文實體: AARP (I-ORG)  ->  中文實體: AARP (I-ORG)\n",
      "英文實體: HIV (I-MISC)  ->  中文實體: HIV (I-MISC)\n",
      "英文實體: Safeway (I-ORG)  ->  中文實體: Safeway (I-ORG)\n",
      "英文實體: GSI (I-MISC)  ->  中文實體: GSI (I-MISC)\n",
      "英文實體: Ai (I-PER)  ->  中文實體: Ai (I-PER)\n",
      "英文實體: MyoD (I-MISC)  ->  中文實體: MyoD (I-MISC)\n",
      "英文實體: Kyu (I-PER)  ->  中文實體: Kyu (I-PER)\n",
      "英文實體: Herbie (I-PER)  ->  中文實體: Herbie (I-PER)\n",
      "英文實體: Chris (I-PER)  ->  中文實體: Chris (I-PER)\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "# 假設您已經加載了包含 'Merged English NER Tagged' 和 'Merged Chinese NER Tagged' 的 DataFrame df\n",
    "# 使用以下代碼來建立配對字典\n",
    "\n",
    "# 構建實體翻譯詞典\n",
    "entity_dict = {}\n",
    "\n",
    "# 遍歷每一行數據\n",
    "for i, row in df.iterrows():\n",
    "    english_entities = row[\"Merged English NER Tagged\"]\n",
    "    chinese_entities = row[\"Merged Chinese NER Tagged\"]\n",
    "    \n",
    "    # 匹配英文和中文實體\n",
    "    for en_entity in english_entities:\n",
    "        for zh_entity in chinese_entities:\n",
    "            # 確保標籤相同才配對\n",
    "            if en_entity['tag'] == zh_entity['tag']:\n",
    "                # 計算名稱的相似度，若超過閾值則加入詞典\n",
    "                similarity = fuzz.ratio(en_entity['word'], zh_entity['word'])\n",
    "                if similarity > 80:  # 設定相似度閾值\n",
    "                    entity_dict[(en_entity['word'], en_entity['tag'])] = (zh_entity['word'], zh_entity['tag'])\n",
    "\n",
    "# 顯示結果\n",
    "print(\"實體翻譯詞典:\")\n",
    "for (en_word, en_tag), (zh_word, zh_tag) in entity_dict.items():\n",
    "    print(f\"英文實體: {en_word} ({en_tag})  ->  中文實體: {zh_word} ({zh_tag})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ec5d3f-1ea5-4e11-8709-e90652a497cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實體翻譯詞典:\n",
      "英文實體: Tipper (I-PER)  ->  中文實體: Tipper (I-PER)\n",
      "英文實體: Ford Taurus (I-MISC)  ->  中文實體: 福特Taurus (I-MISC)\n",
      "英文實體: Shoney's (I-ORG)  ->  中文實體: ney's (I-ORG)\n",
      "英文實體: G-V (I-MISC)  ->  中文實體: G-5 (I-MISC)\n",
      "英文實體: TED (I-ORG)  ->  中文實體: TED (I-ORG)\n",
      "英文實體: Bill Joy (I-PER)  ->  中文實體: Bill (I-PER)\n",
      "英文實體: Ashraf (I-PER)  ->  中文實體: Ashraf (I-PER)\n",
      "英文實體: Ashraf Ghani (I-PER)  ->  中文實體: Ashraf (I-PER)\n",
      "英文實體: CA (I-ORG)  ->  中文實體: CA (I-ORG)\n",
      "英文實體: AG (I-ORG)  ->  中文實體: AG (I-ORG)\n",
      "英文實體: DC8 (I-MISC)  ->  中文實體: DC8 (I-MISC)\n",
      "英文實體: DARPA (I-MISC)  ->  中文實體: DARPA网 (I-ORG)\n",
      "英文實體: SR71 (I-MISC)  ->  中文實體: SR71 (I-MISC)\n",
      "英文實體: IBM (I-ORG)  ->  中文實體: IBM (I-ORG)\n",
      "英文實體: Mac (I-MISC)  ->  中文實體: Mac (I-MISC)\n",
      "英文實體: DOS (I-MISC)  ->  中文實體: DOS (I-MISC)\n",
      "英文實體: Photoshop (I-MISC)  ->  中文實體: Photoshop (I-MISC)\n",
      "英文實體: Microsoft Word (I-MISC)  ->  中文實體: Microsoft (I-MISC)\n",
      "英文實體: Microsoft Write (I-MISC)  ->  中文實體: Microsoft (I-MISC)\n",
      "英文實體: Windows 2000. (I-MISC)  ->  中文實體: Windows (I-MISC)\n",
      "英文實體: Windows (I-MISC)  ->  中文實體: Windows (I-MISC)\n",
      "英文實體: Windows PC (I-MISC)  ->  中文實體: Windows (I-MISC)\n",
      "英文實體: Palm (I-PER)  ->  中文實體: Palm (I-ORG)\n",
      "英文實體: Palm (I-ORG)  ->  中文實體: Palm (I-ORG)\n",
      "英文實體: Jeff Hawkins (I-PER)  ->  中文實體: Hawkins (I-PER)\n",
      "英文實體: Palm Pilot (I-MISC)  ->  中文實體: Pilot (I-MISC)\n",
      "英文實體: Word (I-MISC)  ->  中文實體: Word (I-MISC)\n",
      "英文實體: Office (I-MISC)  ->  中文實體: Office (I-MISC)\n",
      "英文實體: MacWorld Expo (I-MISC)  ->  中文實體: MacWorld (I-MISC)\n",
      "英文實體: Woz (I-PER)  ->  中文實體: Woz (I-PER)\n",
      "英文實體: iPod (I-MISC)  ->  中文實體: iPod (I-MISC)\n",
      "英文實體: Sonos (I-ORG)  ->  中文實體: Sonos (I-ORG)\n",
      "英文實體: Chris Anderson (I-PER)  ->  中文實體: Anderson (I-PER)\n",
      "英文實體: CA (I-LOC)  ->  中文實體: CA (I-LOC)\n",
      "英文實體: PS 234 (I-MISC)  ->  中文實體: 234 (I-MISC)\n",
      "英文實體: JS (I-PER)  ->  中文實體: JS (I-PER)\n",
      "英文實體: iBOT (I-MISC)  ->  中文實體: BOT (I-MISC)\n",
      "英文實體: AARP (I-ORG)  ->  中文實體: AARP (I-ORG)\n",
      "英文實體: HIV (I-MISC)  ->  中文實體: HIV (I-MISC)\n",
      "英文實體: Safeway (I-ORG)  ->  中文實體: Safeway (I-ORG)\n",
      "英文實體: GSI (I-MISC)  ->  中文實體: GSI (I-MISC)\n",
      "英文實體: SI (I-MISC)  ->  中文實體: GSI (I-MISC)\n",
      "英文實體: Ai (I-PER)  ->  中文實體: Ai (I-PER)\n",
      "英文實體: MyoD (I-MISC)  ->  中文實體: MyoD (I-MISC)\n",
      "英文實體: Kyu (I-PER)  ->  中文實體: Kyu (I-PER)\n",
      "英文實體: Herbie (I-PER)  ->  中文實體: Herbie (I-PER)\n",
      "英文實體: Chris (I-PER)  ->  中文實體: Chris (I-PER)\n",
      "英文實體: Whitney Biennial (I-MISC)  ->  中文實體: Biennial (I-MISC)\n",
      "英文實體: Bitforms Gallery (I-ORG)  ->  中文實體: Bitforms (I-ORG)\n",
      "英文實體: Mary Anne (I-PER)  ->  中文實體: Anne (I-PER)\n"
     ]
    }
   ],
   "source": [
    "# 構建實體翻譯詞典\n",
    "entity_dict = {}\n",
    "\n",
    "# 遍歷每一行數據\n",
    "for i, row in df.iterrows():\n",
    "    english_entities = row[\"Merged English NER Tagged\"]\n",
    "    chinese_entities = row[\"Merged Chinese NER Tagged\"]\n",
    "    \n",
    "    # 匹配英文和中文實體\n",
    "    for en_idx, en_entity in enumerate(english_entities):\n",
    "        en_word, en_tag = en_entity['word'], en_entity['tag']\n",
    "        for zh_idx, zh_entity in enumerate(chinese_entities):\n",
    "            zh_word, zh_tag = zh_entity['word'], zh_entity['tag']\n",
    "            \n",
    "            # 根據標籤相似性和位置相似性進行配對\n",
    "            if en_tag == zh_tag or abs(en_idx - zh_idx) <= 1:  # 放寬標籤限制\n",
    "                similarity = fuzz.ratio(en_word, zh_word)\n",
    "                if similarity > 60:  # 設定相似度閾值，可以根據需要調整\n",
    "                    entity_dict[(en_word, en_tag)] = (zh_word, zh_tag)\n",
    "\n",
    "# 顯示結果\n",
    "print(\"實體翻譯詞典:\")\n",
    "for (en_word, en_tag), (zh_word, zh_tag) in entity_dict.items():\n",
    "    print(f\"英文實體: {en_word} ({en_tag})  ->  中文實體: {zh_word} ({zh_tag})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04d5b289-a47c-49a9-b3e9-0976f4e74700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm  # 新增進度條\n",
    "\n",
    "# 1. 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2000]', trust_remote_code=True)\n",
    "\n",
    "# 2. 載入 NER 模型（以英文的 Spacy 模型為例）\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 3. 設置翻譯模型 (使用 MarianMT 模型從英文翻譯到中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 用於儲存結果的列表\n",
    "results = []\n",
    "\n",
    "# 4. 遍歷每個句子進行 NER 識別並翻譯實體，使用 tqdm 顯示進度\n",
    "for example in tqdm(dataset, desc=\"Processing sentences\"):\n",
    "    english_text = example['translation']['en']\n",
    "    chinese_text = example['translation']['zh']\n",
    "    \n",
    "    # NER 識別\n",
    "    doc = nlp(english_text)\n",
    "    translated_entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # 翻譯識別到的實體\n",
    "        translated = model.generate(**tokenizer(ent.text, return_tensors=\"pt\", padding=True))\n",
    "        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 儲存原始實體、翻譯實體和實體類型\n",
    "        translated_entities.append({\n",
    "            \"original_entity\": ent.text,\n",
    "            \"translated_entity\": translated_text,\n",
    "            \"entity_type\": ent.label_\n",
    "        })\n",
    "    \n",
    "    # 儲存結果（原始英文句子，中文翻譯，識別實體及其翻譯）\n",
    "    results.append({\n",
    "        \"english_text\": english_text,\n",
    "        \"chinese_text\": chinese_text,\n",
    "        \"entities\": translated_entities\n",
    "    })\n",
    "\n",
    "# 5. 顯示結果示例\n",
    "for result in results[:5]:  # 僅顯示前五個結果\n",
    "    print(\"英文句子:\", result[\"english_text\"])\n",
    "    print(\"中文句子:\", result[\"chinese_text\"])\n",
    "    print(\"識別到的實體與翻譯:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  原始實體: {entity['original_entity']}, 翻譯: {entity['translated_entity']}, 類別: {entity['entity_type']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41dd002-debf-4df7-b25c-cf1824305dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER 及翻譯結果已保存到 'ner_translations.csv'\n"
     ]
    }
   ],
   "source": [
    "# 5. 將結果存入 CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ner_translations.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"NER 及翻譯結果已保存到 'ner_translations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83d691d1-d472-4ec5-88f0-f2c93978e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'NllbTokenizer'. \n",
      "The class this function is called from is 'MarianTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# # 設置翻譯模型\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model_name = \"Helsinki-NLP/opus-mt-en-zh\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 使用 NLLB 模型和 Tokenizer\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/nllb-200-distilled-600M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2028\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2025\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2026\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2260\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:147\u001b[0m, in \u001b[0;36mMarianTokenizer.__init__\u001b[1;34m(self, source_spm, target_spm, vocab, target_vocab_file, source_lang, target_lang, unk_token, eos_token, pad_token, model_max_length, sp_model_kwargs, separate_vocabs, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    131\u001b[0m     source_spm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m sp_model_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sp_model_kwargs\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_spm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot find spm source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_spm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparate_vocabs \u001b[38;5;241m=\u001b[39m separate_vocabs\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m load_json(vocab)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:1042\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[1;32m-> 1042\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:683\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[1;34m(cls, args, init)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 683\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:667\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    665\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[0;32m    670\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:200]', trust_remote_code=True)\n",
    "\n",
    "# 載入 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 設置翻譯模型\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# 儲存結果的列表\n",
    "results = []\n",
    "\n",
    "# 遍歷每個句子進行 NER 識別並翻譯實體，加上 tqdm 進度條\n",
    "for example in tqdm(dataset, desc=\"Processing sentences\"):\n",
    "    english_text = example['translation']['en']\n",
    "    chinese_text = example['translation']['zh']\n",
    "    \n",
    "    # NER 識別\n",
    "    doc = nlp(english_text)\n",
    "    translated_entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # 翻譯識別到的實體，並限制生成的字數避免重複\n",
    "        translated = model.generate(**tokenizer(ent.text, return_tensors=\"pt\", padding=True), max_length=5)\n",
    "        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 去除重複字元\n",
    "        translated_text = \" \".join(dict.fromkeys(translated_text.split()))\n",
    "        \n",
    "        # 儲存原始實體、翻譯實體和實體類型\n",
    "        translated_entities.append({\n",
    "            \"original_entity\": ent.text,\n",
    "            \"translated_entity\": translated_text,\n",
    "            \"entity_type\": ent.label_\n",
    "        })\n",
    "    \n",
    "    # 儲存結果\n",
    "    results.append({\n",
    "        \"english_text\": english_text,\n",
    "        \"chinese_text\": chinese_text,\n",
    "        \"entities\": translated_entities\n",
    "    })\n",
    "\n",
    "# 顯示結果示例\n",
    "for result in results[:5]:  # 僅顯示前五個結果\n",
    "    print(\"英文句子:\", result[\"english_text\"])\n",
    "    print(\"中文句子:\", result[\"chinese_text\"])\n",
    "    print(\"識別到的實體與翻譯:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  原始實體: {entity['original_entity']}, 翻譯: {entity['translated_entity']}, 類別: {entity['entity_type']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da170d52-f0ac-453b-ae58-21aa1f79af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER 及翻譯結果已保存到 'ner_translations.csv'\n"
     ]
    }
   ],
   "source": [
    "# 5. 將結果存入 CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ner_translations.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"NER 及翻譯結果已保存到 'ner_translations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "820e4074-01a6-4e14-922f-9cfe2bfd02a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████████| 200/200 [01:42<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文句子: Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "中文句子: 非常谢谢，克里斯。的确非常荣幸 能有第二次站在这个台上的机会，我真是非常感激。\n",
      "識別到的實體與翻譯:\n",
      "  原始實體: Chris, 翻譯: 克里, 類別: PERSON\n",
      "\n",
      "\n",
      "英文句子: I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "中文句子: 这个会议真是让我感到惊叹不已，我还要谢谢你们留下的 关于我上次演讲的精彩评论\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n",
      "英文句子: And I say that sincerely, partly because  I need that.  Put yourselves in my position.\n",
      "中文句子: 我是非常真诚的，部分原因是因为----我的确非常需要！ 你设身处地为我想想！\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n",
      "英文句子: I flew on Air Force Two for eight years.\n",
      "中文句子: 我坐了8年的空军二号。\n",
      "識別到的實體與翻譯:\n",
      "  原始實體: Air Force Two, 翻譯: 航空队, 類別: PRODUCT\n",
      "  原始實體: eight years, 翻譯: 八年, 類別: DATE\n",
      "\n",
      "\n",
      "英文句子: Now I have to take off my shoes or boots to get on an airplane!\n",
      "中文句子: 不过现在上飞机前我则要脱掉我的鞋子\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:200]', trust_remote_code=True)\n",
    "\n",
    "# 載入 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 使用 NLLB 模型進行翻譯\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 儲存結果的列表\n",
    "results = []\n",
    "\n",
    "# 遍歷每個句子進行 NER 識別並翻譯實體，加上 tqdm 進度條\n",
    "for example in tqdm(dataset, desc=\"Processing sentences\"):\n",
    "    english_text = example['translation']['en']\n",
    "    chinese_text = example['translation']['zh']\n",
    "    \n",
    "    # NER 識別\n",
    "    doc = nlp(english_text)\n",
    "    translated_entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # 翻譯識別到的實體，並限制生成的字數避免重複\n",
    "        inputs = tokenizer(ent.text, return_tensors=\"pt\")\n",
    "        translated_tokens = model.generate(inputs[\"input_ids\"], max_length=5, forced_bos_token_id=tokenizer.lang_code_to_id[\"zho_Hans\"])\n",
    "        translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 去除重複字元\n",
    "        translated_text = \" \".join(dict.fromkeys(translated_text.split()))\n",
    "        \n",
    "        # 儲存原始實體、翻譯實體和實體類型\n",
    "        translated_entities.append({\n",
    "            \"original_entity\": ent.text,\n",
    "            \"translated_entity\": translated_text,\n",
    "            \"entity_type\": ent.label_\n",
    "        })\n",
    "    \n",
    "    # 儲存結果\n",
    "    results.append({\n",
    "        \"english_text\": english_text,\n",
    "        \"chinese_text\": chinese_text,\n",
    "        \"entities\": translated_entities\n",
    "    })\n",
    "\n",
    "# 顯示結果示例\n",
    "for result in results[:5]:  # 僅顯示前五個結果\n",
    "    print(\"英文句子:\", result[\"english_text\"])\n",
    "    print(\"中文句子:\", result[\"chinese_text\"])\n",
    "    print(\"識別到的實體與翻譯:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  原始實體: {entity['original_entity']}, 翻譯: {entity['translated_entity']}, 類別: {entity['entity_type']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88036012-2781-4179-823f-6ead51592c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████████| 200/200 [03:16<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文句子: Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "中文句子: 非常谢谢，克里斯。的确非常荣幸 能有第二次站在这个台上的机会，我真是非常感激。\n",
      "識別到的實體與翻譯:\n",
      "  原始實體: Chris, 翻譯: 克里斯, 類別: PERSON\n",
      "\n",
      "\n",
      "英文句子: I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "中文句子: 这个会议真是让我感到惊叹不已，我还要谢谢你们留下的 关于我上次演讲的精彩评论\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n",
      "英文句子: And I say that sincerely, partly because  I need that.  Put yourselves in my position.\n",
      "中文句子: 我是非常真诚的，部分原因是因为----我的确非常需要！ 你设身处地为我想想！\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n",
      "英文句子: I flew on Air Force Two for eight years.\n",
      "中文句子: 我坐了8年的空军二号。\n",
      "識別到的實體與翻譯:\n",
      "  原始實體: Air Force Two, 翻譯: 空军二号, 類別: PRODUCT\n",
      "  原始實體: eight years, 翻譯: 八年, 類別: DATE\n",
      "\n",
      "\n",
      "英文句子: Now I have to take off my shoes or boots to get on an airplane!\n",
      "中文句子: 不过现在上飞机前我则要脱掉我的鞋子\n",
      "識別到的實體與翻譯:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:200]', trust_remote_code=True)\n",
    "\n",
    "# 載入 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 設置 mBART 翻譯模型\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "# 儲存結果的列表\n",
    "results = []\n",
    "\n",
    "# 遍歷每個句子進行 NER 識別並翻譯實體，加上 tqdm 進度條\n",
    "for example in tqdm(dataset, desc=\"Processing sentences\"):\n",
    "    english_text = example['translation']['en']\n",
    "    chinese_text = example['translation']['zh']\n",
    "    \n",
    "    # NER 識別\n",
    "    doc = nlp(english_text)\n",
    "    translated_entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # 翻譯識別到的實體，並限制生成的字數避免重複\n",
    "        inputs = tokenizer(ent.text, return_tensors=\"pt\")\n",
    "        translated_tokens = model.generate(inputs[\"input_ids\"], max_length=10, forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"])\n",
    "        translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 去除重複字元\n",
    "        translated_text = \" \".join(dict.fromkeys(translated_text.split()))\n",
    "        \n",
    "        # 儲存原始實體、翻譯實體和實體類型\n",
    "        translated_entities.append({\n",
    "            \"original_entity\": ent.text,\n",
    "            \"translated_entity\": translated_text,\n",
    "            \"entity_type\": ent.label_\n",
    "        })\n",
    "    \n",
    "    # 儲存結果\n",
    "    results.append({\n",
    "        \"english_text\": english_text,\n",
    "        \"chinese_text\": chinese_text,\n",
    "        \"entities\": translated_entities\n",
    "    })\n",
    "\n",
    "# 顯示結果示例\n",
    "for result in results[:5]:  # 僅顯示前五個結果\n",
    "    print(\"英文句子:\", result[\"english_text\"])\n",
    "    print(\"中文句子:\", result[\"chinese_text\"])\n",
    "    print(\"識別到的實體與翻譯:\")\n",
    "    for entity in result[\"entities\"]:\n",
    "        print(f\"  原始實體: {entity['original_entity']}, 翻譯: {entity['translated_entity']}, 類別: {entity['entity_type']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34361c12-97f9-4b66-98d4-3033baa637da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER 及翻譯結果已保存到 'ner_translations.csv'\n"
     ]
    }
   ],
   "source": [
    "# 5. 將結果存入 CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ner_translations.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"NER 及翻譯結果已保存到 'ner_translations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347711ba-a0ee-4b2b-aed1-952c494fbf24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
