{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ff5a0c-60fc-4fb1-90c1-c7ae011a0440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing English NER: 100%|██████████████████████████████████████████████████████| 2000/2000 [09:41<00:00,  3.44it/s]\n",
      "Processing Chinese NER: 100%|██████████████████████████████████████████████████████| 2000/2000 [09:58<00:00,  3.34it/s]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\USER\\\\Downloads\\\\NLP-Courses\\\\NLP243\\\\Projects\\\\Test_iwslt2017_ner_tagged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m\n\u001b[0;32m     29\u001b[0m ner_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: english_sentences,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish NER Tagged\u001b[39m\u001b[38;5;124m\"\u001b[39m: english_ner_results,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChinese Sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: chinese_sentences,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChinese NER Tagged\u001b[39m\u001b[38;5;124m\"\u001b[39m: chinese_ner_results\n\u001b[0;32m     34\u001b[0m })\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 保存 NER 標記結果到 CSV 文件\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mner_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUSER\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNLP-Courses\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNLP243\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTest_iwslt2017_ner_tagged.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 定義函數以用指定格式替換實體\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_with_tags\u001b[39m(tagged_words):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\USER\\\\Downloads\\\\NLP-Courses\\\\NLP243\\\\Projects\\\\Test_iwslt2017_ner_tagged.csv'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Hugging Face 的多語言 NER pipeline，載入 XLM-R 模型\n",
    "ner_pipeline = pipeline(\"ner\", model=\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "\n",
    "# 使用 datasets 套件載入 IWSLT 2017 英中翻譯資料集，僅取前 2000 筆數據\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2000]', trust_remote_code=True)\n",
    "\n",
    "# 提取中英文句子\n",
    "english_sentences = [example['translation']['en'] for example in dataset]\n",
    "chinese_sentences = [example['translation']['zh'] for example in dataset]\n",
    "\n",
    "# 進行英文命名實體識別\n",
    "english_ner_results = []\n",
    "for sentence in tqdm(english_sentences, desc=\"Processing English NER\"):\n",
    "    tagged_words = ner_pipeline(sentence)\n",
    "    english_ner_results.append(tagged_words)\n",
    "\n",
    "# 進行中文命名實體識別\n",
    "chinese_ner_results = []\n",
    "for sentence in tqdm(chinese_sentences, desc=\"Processing Chinese NER\"):\n",
    "    tagged_words = ner_pipeline(sentence)\n",
    "    chinese_ner_results.append(tagged_words)\n",
    "\n",
    "# 將結果轉換為 DataFrame 便於檢視\n",
    "ner_df = pd.DataFrame({\n",
    "    \"English Sentence\": english_sentences,\n",
    "    \"English NER Tagged\": english_ner_results,\n",
    "    \"Chinese Sentence\": chinese_sentences,\n",
    "    \"Chinese NER Tagged\": chinese_ner_results\n",
    "})\n",
    "\n",
    "# 保存 NER 標記結果到 CSV 文件\n",
    "ner_df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Test_tagged.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53ad5a5-10b0-480d-b17d-528190217f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 NER 標記結果到 CSV 文件\n",
    "ner_df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Test_tagged.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca390584-75bd-40d8-a581-9c45679a20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tagged CSV file for processing entity merging\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the uploaded tagged file\n",
    "file_path = './Test_tagged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to merge segmented entities based on their type\n",
    "# Update the merge function for Chinese to only merge entities with the same tag and strictly consecutive start:end positions\n",
    "def merge_entities_for_chinese(entities):\n",
    "    merged_entities = []\n",
    "    temp_entity = \"\"\n",
    "    temp_tag = None\n",
    "    temp_score = 1.0  # Start with a high confidence score for the entity being merged\n",
    "    temp_end = None  # Track the end position of the last added word\n",
    "\n",
    "    for entity in entities:\n",
    "        word, tag, score, start, end = entity['word'].replace(\"▁\", \"\"), entity['entity'], entity['score'], entity['start'], entity['end']\n",
    "\n",
    "        # Check if we should start a new entity based on tag or strictly consecutive positions (start == temp_end)\n",
    "        if temp_tag is None or temp_tag != tag or (temp_end is not None and temp_end != start):\n",
    "            # If we are starting a new entity, tag changes, or start position is not strictly consecutive, store the previous entity\n",
    "            if temp_entity:\n",
    "                merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "            # Start a new entity\n",
    "            temp_entity = word\n",
    "            temp_tag = tag\n",
    "            temp_score = score\n",
    "            temp_end = end\n",
    "        else:\n",
    "            # Continue the current entity if the tag is the same and positions are strictly consecutive\n",
    "            temp_entity += word\n",
    "            temp_score = min(temp_score, score)  # Track the minimum score as the overall confidence\n",
    "            temp_end = end  # Update end position for consecutive check\n",
    "\n",
    "    # Add the final entity if any\n",
    "    if temp_entity:\n",
    "        merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Apply the updated merging function for English and the new function for Chinese\n",
    "merged_english_entities = []\n",
    "merged_chinese_entities = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    english_entities = eval(row['English NER Tagged']) if row['English NER Tagged'] != \"[]\" else []\n",
    "    chinese_entities = eval(row['Chinese NER Tagged']) if row['Chinese NER Tagged'] != \"[]\" else []\n",
    "\n",
    "    # Apply merge function with space for English entities, and strict consecutive merge for Chinese entities\n",
    "    merged_english_entities.append(merge_entities_with_space_for_english(english_entities))\n",
    "    merged_chinese_entities.append(merge_entities_for_chinese(chinese_entities))\n",
    "\n",
    "# Add merged entities to the DataFrame\n",
    "df['Merged English NER Tagged'] = merged_english_entities\n",
    "df['Merged Chinese NER Tagged'] = merged_chinese_entities\n",
    "\n",
    "# Display the modified DataFrame with merged entities\n",
    "df[['English Sentence', 'Merged English NER Tagged', 'Chinese Sentence', 'Merged Chinese NER Tagged']].head()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# 保存替換後的 NER 標記結果到 CSV 文件\n",
    "df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Entity.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802d1186-c879-472a-8f50-884ca8f4567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest tagged CSV file for processing\n",
    "file_path = './Test_tagged.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to merge segmented entitie# Update the merge function to only combine entities with the same tag and consecutive start:end positions\n",
    "# Update the merge function to handle cases where consecutive entities in English require a space between them\n",
    "def merge_entities_with_space_for_english(entities):\n",
    "    merged_entities = []\n",
    "    temp_entity = \"\"\n",
    "    temp_tag = None\n",
    "    temp_score = 1.0  # Start with a high confidence score for the entity being merged\n",
    "    temp_end = None  # Track the end position of the last added word\n",
    "\n",
    "    for entity in entities:\n",
    "        word, tag, score, start, end = entity['word'].replace(\"▁\", \"\"), entity['entity'], entity['score'], entity['start'], entity['end']\n",
    "\n",
    "        # Check if we should start a new entity based on tag or position (end + 1 == start for consecutive check)\n",
    "        if temp_tag is None or temp_tag != tag or (temp_end is not None and temp_end + 1 < start):\n",
    "            # If we are starting a new entity, tag changes, or start position is not consecutive, store the previous entity\n",
    "            if temp_entity:\n",
    "                merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "            # Start a new entity\n",
    "            temp_entity = word\n",
    "            temp_tag = tag\n",
    "            temp_score = score\n",
    "            temp_end = end\n",
    "        else:\n",
    "            # Continue the current entity if the tag is the same\n",
    "            # Add a space if there's a gap between the previous end and current start\n",
    "            if temp_end + 1 == start:\n",
    "                temp_entity += \" \" + word\n",
    "            else:\n",
    "                temp_entity += word\n",
    "            temp_score = min(temp_score, score)  # Track the minimum score as the overall confidence\n",
    "            temp_end = end  # Update end position for consecutive check\n",
    "\n",
    "    # Add the final entity if any\n",
    "    if temp_entity:\n",
    "        merged_entities.append({\"word\": temp_entity, \"tag\": temp_tag, \"score\": temp_score})\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Apply the updated merging function only on the English NER tagged results\n",
    "merged_english_entities = []\n",
    "merged_chinese_entities = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    english_entities = eval(row['English NER Tagged']) if row['English NER Tagged'] != \"[]\" else []\n",
    "    chinese_entities = eval(row['Chinese NER Tagged']) if row['Chinese NER Tagged'] != \"[]\" else []\n",
    "\n",
    "    # Apply merge function with space for English entities, and regular merge for Chinese entities\n",
    "    merged_english_entities.append(merge_entities_with_space_for_english(english_entities))\n",
    "    merged_chinese_entities.append(merge_entities_by_tag_and_consecutive_extended(chinese_entities))\n",
    "\n",
    "# Add merged entities to the DataFrame\n",
    "df['Merged English NER Tagged'] = merged_english_entities\n",
    "df['Merged Chinese NER Tagged'] = merged_chinese_entities\n",
    "\n",
    "# Display the modified DataFrame with merged entities\n",
    "df[['English Sentence', 'Merged English NER Tagged', 'Chinese Sentence', 'Merged Chinese NER Tagged']].head()\n",
    "\n",
    "\n",
    "# 保存替換後的 NER 標記結果到 CSV 文件\n",
    "df.to_csv(r\"C:\\Users\\USER\\Downloads\\NLP-Courses\\NLP243\\Projects\\Entity.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b7677-72bb-4a94-9f40-3e71446c6951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
