{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch spacy tqdm evaluate bert-score nltk datasets OpenHowNet jieba\n",
        "\n",
        "!python -m spacy download en_core_web_trf\n",
        "import spacy\n",
        "from datasets import load_dataset\n",
        "from transformers import MBartForConditionalGeneration, MBart50Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
        "import torch\n",
        "import warnings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import jieba\n",
        "import json\n",
        "from bert_score import score\n",
        "import OpenHowNet\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# 加载 OpenHowNet 資源\n",
        "print(\"Checking OpenHowNet resources...\")\n",
        "OpenHowNet.download()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 設定設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 載入 SpaCy NER 模型\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# 載入 IWSLT 2017 英中翻譯資料集\n",
        "train_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train', trust_remote_code=True)\n",
        "test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='test', trust_remote_code=True)\n",
        "\n",
        "# 載入 mBART 模型和 Tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# 設定源語言和目標語言\n",
        "tokenizer.src_lang = \"en_XX\"\n",
        "tokenizer.tgt_lang = \"zh_CN\"\n",
        "\n",
        "# 實體識別和標記的函數\n",
        "def mark_entities(text):\n",
        "    doc = nlp(text)\n",
        "    modified_text = text\n",
        "    for ent in doc.ents:\n",
        "        entity_marker = f\"<ENTITY type=\\\"{ent.label_}\\\">{ent.text}</ENTITY>\"\n",
        "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
        "    return modified_text\n",
        "\n",
        "# 預處理函數\n",
        "def preprocess_function(examples):\n",
        "    inputs, targets = [], []\n",
        "    for ex in examples[\"translation\"]:\n",
        "        marked_text = mark_entities(ex[\"en\"])\n",
        "        inputs.append(marked_text)\n",
        "        targets.append(ex[\"zh\"])\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# 預處理數據集\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 設定輸出目錄\n",
        "output_dir = \"/content/mbart_finetuned\"\n",
        "if os.path.exists(output_dir):\n",
        "    if os.path.isfile(output_dir):\n",
        "        os.remove(output_dir)\n",
        "    else:\n",
        "        import shutil\n",
        "        shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "# 自定義回調函數來顯示 Training Loss\n",
        "class CustomLogCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if \"loss\" in logs:\n",
        "            print(f\"Step {state.global_step}, Training Loss: {logs['loss']:.4f}\")\n",
        "        if \"eval_loss\" in logs:\n",
        "            print(f\"Step {state.global_step}, Validation Loss: {logs['eval_loss']:.4f}\")\n",
        "\n",
        "# 訓練參數設置\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='/content/logs',\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 初始化 Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), CustomLogCallback()],\n",
        ")\n",
        "\n",
        "# 開始訓練\n",
        "print(\"Training started...\")\n",
        "trainer.train()\n",
        "\n",
        "# 儲存模型\n",
        "final_model_dir = \"/content/final_model\"\n",
        "trainer.save_model(final_model_dir)\n",
        "tokenizer.save_pretrained(final_model_dir)\n",
        "print(f\"Model saved to {final_model_dir}\")\n",
        "\n",
        "# 加載同義詞集合\n",
        "hownet_dict = OpenHowNet.HowNetDict()\n",
        "hownet_dict.initialize_similarity_calculation()\n",
        "\n",
        "# 加載哈工大詞林\n",
        "def load_cilin(file_path):\n",
        "    synonym_groups = []\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if \"=\" in line:\n",
        "                words = line.strip().split('=')[1].split()\n",
        "                synonym_groups.append(set(words))\n",
        "    return synonym_groups\n",
        "\n",
        "cilin_path = \"/content/cilin.txt\"\n",
        "synonym_groups = load_cilin(cilin_path)\n",
        "\n",
        "# 更新评估函数\n",
        "def evaluate_translations(model, tokenizer, dataset, output_path, synonym_groups, num_translations=5):\n",
        "    print(\"Starting evaluation...\")\n",
        "    translated_results = []\n",
        "    total_meteor = 0\n",
        "    total_bleu = 0\n",
        "    total_bert_p = 0\n",
        "    total_bert_r = 0\n",
        "    total_bert_f1 = 0\n",
        "    num_sentences = 0\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        input_text = example[\"translation\"][\"en\"]\n",
        "        reference_text = jieba.lcut(example[\"translation\"][\"zh\"])  # 分词后的参考翻译\n",
        "\n",
        "        # 模型生成多个翻译\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=256,\n",
        "            num_return_sequences=num_translations,\n",
        "            num_beams=num_translations,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        translations = [jieba.lcut(tokenizer.decode(output, skip_special_tokens=True)) for output in outputs]\n",
        "\n",
        "        # 计算 METEOR 分数\n",
        "        meteor_scores = [\n",
        "            calculate_meteor(translation, reference_text, synonym_groups)[\"METEOR\"]\n",
        "            for translation in translations\n",
        "        ]\n",
        "\n",
        "        # 找到最佳翻译（基于 METEOR）\n",
        "        best_translation_idx = max(range(len(meteor_scores)), key=lambda idx: meteor_scores[idx])\n",
        "        best_translation = translations[best_translation_idx]\n",
        "        best_meteor_score = meteor_scores[best_translation_idx]\n",
        "\n",
        "        # 转换最佳翻译为字符串形式以计算 BLEU 和 BERTScore\n",
        "        best_translation_str = \"\".join(best_translation)\n",
        "        reference_text_str = \"\".join(reference_text)\n",
        "\n",
        "        # 计算 BLEU 分数\n",
        "        bleu_score = sentence_bleu([reference_text], best_translation)\n",
        "        total_bleu += bleu_score\n",
        "\n",
        "        # 计算 BERTScore\n",
        "        bert_p, bert_r, bert_f1 = score(\n",
        "            cands=[best_translation_str],\n",
        "            refs=[reference_text_str],\n",
        "            lang=\"zh\",\n",
        "            verbose=False\n",
        "        )\n",
        "        best_bert_p = bert_p.mean().item()\n",
        "        best_bert_r = bert_r.mean().item()\n",
        "        best_bert_f1 = bert_f1.mean().item()\n",
        "\n",
        "        # 累积分数\n",
        "        total_meteor += best_meteor_score\n",
        "        total_bert_p += best_bert_p\n",
        "        total_bert_r += best_bert_r\n",
        "        total_bert_f1 += best_bert_f1\n",
        "        num_sentences += 1\n",
        "\n",
        "        translated_results.append({\n",
        "            \"Original Text\": input_text,\n",
        "            \"Reference Text\": reference_text_str,\n",
        "            \"All Translations\": [\"\".join(translation) for translation in translations],\n",
        "            \"Best Translation\": best_translation_str,\n",
        "            \"BLEU Score\": bleu_score,\n",
        "            \"METEOR Score\": best_meteor_score,\n",
        "            \"BERTScore Precision\": best_bert_p,\n",
        "            \"BERTScore Recall\": best_bert_r,\n",
        "            \"BERTScore F1\": best_bert_f1\n",
        "        })\n",
        "\n",
        "    # 保存结果\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(translated_results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # 输出总体分数\n",
        "    print(f\"Overall BLEU: {total_bleu / num_sentences:.4f}\")\n",
        "    print(f\"Overall METEOR: {total_meteor / num_sentences:.4f}\")\n",
        "    print(f\"Overall BERTScore Precision: {total_bert_p / num_sentences:.4f}\")\n",
        "    print(f\"Overall BERTScore Recall: {total_bert_r / num_sentences:.4f}\")\n",
        "    print(f\"Overall BERTScore F1: {total_bert_f1 / num_sentences:.4f}\")\n",
        "    print(f\"Evaluation results saved to {output_path}\")\n",
        "\n",
        "# 執行評估\n",
        "evaluate_translations(model, tokenizer, test_dataset, \"/content/translated_results.json\", synonym_groups, num_translations=1)\n"
      ],
      "metadata": {
        "id": "eIUunNQou_3B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}