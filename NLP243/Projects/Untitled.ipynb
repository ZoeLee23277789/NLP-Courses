{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc69e9e-9722-4039-b7e3-a144453a6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "# 載入 IWSLT 2017 英中翻譯資料集（取前 2000 筆）\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-zh', split='train[:2000]', trust_remote_code=True)\n",
    "\n",
    "# 1. 載入英文 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 載入 mBART 翻譯模型（微調後的 mBART 作為基礎翻譯模型）\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"zh_CN\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b138d7be-958f-464c-9a01-2d4730675e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_entity_aware(text):\n",
    "    # 使用 NER 模型來識別句中的實體\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # 將實體標記加入句子中\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entity_marker = f\"<{ent.label_}:{ent.text}>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "\n",
    "    return modified_text, entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01605b07-93f2-4931-9a1a-834057b09def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_aware_translate(text):\n",
    "    # 1. 對文本進行實體識別和標記\n",
    "    processed_text, entities = preprocess_entity_aware(text)\n",
    "\n",
    "    # 2. 進行翻譯\n",
    "    inputs = tokenizer(processed_text, return_tensors=\"pt\")\n",
    "    translated_tokens = model.generate(inputs[\"input_ids\"])\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # 3. 後處理：將實體替換回原來的翻譯結果中\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker = f\"<{ent_label}:{ent_text}>\"\n",
    "        if entity_marker in translated_text:\n",
    "            # 保留實體一致性\n",
    "            translated_text = translated_text.replace(entity_marker, ent_text)\n",
    "\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95518ff1-610e-437d-ac34-05c3726ed241",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (MBartForConditionalGeneration) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'MBartForCausalLM', 'MBartForConditionalGeneration'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)):  \u001b[38;5;66;03m# 測試前10個句子\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m     translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mentity_aware_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_text)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, translated_text)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mentity_aware_translate\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 2. 進行翻譯\u001b[39;00m\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(processed_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m translated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 3. 後處理：將實體替換回原來的翻譯結果中\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1505\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m     generation_config = self.generation_config\n\u001b[0;32m   1503\u001b[0m     using_model_generation_config = True\n\u001b[1;32m-> 1505\u001b[0m # `torch.compile` can't compile `copy.deepcopy`, arguments in `kwargs` that are part of `generation_config`\n\u001b[0;32m   1506\u001b[0m # will mutate the object with `.update`. As such, passing these arguments through `kwargs` is disabled -- an\n\u001b[0;32m   1507\u001b[0m # exception will be raised in `_validate_model_kwargs`\n\u001b[0;32m   1508\u001b[0m if not is_torchdynamo_compiling():\n\u001b[0;32m   1509\u001b[0m     generation_config = copy.deepcopy(generation_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1287\u001b[0m, in \u001b[0;36m_validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     are_equal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, attr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(assistant_model\u001b[38;5;241m.\u001b[39mconfig, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes_to_check\n\u001b[0;32m   1284\u001b[0m     )\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_equal:\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m-> 1287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe main model and the assistant don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have compatible encoder-dependent input shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1289\u001b[0m         )\n\u001b[0;32m   1291\u001b[0m doc_reference \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m )\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config()\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m==\u001b[39m assistant_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config()\u001b[38;5;241m.\u001b[39mvocab_size:\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (MBartForConditionalGeneration) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'MBartForCausalLM', 'MBartForConditionalGeneration'}"
     ]
    }
   ],
   "source": [
    "# 測試模型\n",
    "for example in dataset.select(range(10)):  # 測試前10個句子\n",
    "    input_text = example['translation']['en']\n",
    "    translated_text = entity_aware_translate(input_text)\n",
    "    \n",
    "    print(\"Original text:\", input_text)\n",
    "    print(\"Translated text:\", translated_text)\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8600f0-8e13-4ba3-92a7-1bd44430d98f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (MBartForConditionalGeneration) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'MBartForCausalLM', 'MBartForConditionalGeneration'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 測試\u001b[39;00m\n\u001b[0;32m     52\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMicrosoft was founded in 1975 by Bill Gates and Paul Allen.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 53\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mentity_aware_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_text)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, translated_text)\n",
      "Cell \u001b[1;32mIn[24], line 36\u001b[0m, in \u001b[0;36mentity_aware_translate\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(processed_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 2. 使用 generate 方法進行翻譯，確保設置了目標語言\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m translated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzh_CN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 指定目標語言為中文\u001b[39;49;00m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 3. 後處理：將實體替換回原來的翻譯結果中\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1505\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m     generation_config = self.generation_config\n\u001b[0;32m   1503\u001b[0m     using_model_generation_config = True\n\u001b[1;32m-> 1505\u001b[0m # `torch.compile` can't compile `copy.deepcopy`, arguments in `kwargs` that are part of `generation_config`\n\u001b[0;32m   1506\u001b[0m # will mutate the object with `.update`. As such, passing these arguments through `kwargs` is disabled -- an\n\u001b[0;32m   1507\u001b[0m # exception will be raised in `_validate_model_kwargs`\n\u001b[0;32m   1508\u001b[0m if not is_torchdynamo_compiling():\n\u001b[0;32m   1509\u001b[0m     generation_config = copy.deepcopy(generation_config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1287\u001b[0m, in \u001b[0;36m_validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     are_equal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, attr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(assistant_model\u001b[38;5;241m.\u001b[39mconfig, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes_to_check\n\u001b[0;32m   1284\u001b[0m     )\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_equal:\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m-> 1287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe main model and the assistant don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have compatible encoder-dependent input shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1289\u001b[0m         )\n\u001b[0;32m   1291\u001b[0m doc_reference \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m )\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config()\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m==\u001b[39m assistant_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config()\u001b[38;5;241m.\u001b[39mvocab_size:\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (MBartForConditionalGeneration) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'MBartForCausalLM', 'MBartForConditionalGeneration'}"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "# 載入 NER 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 載入 mBART 模型\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 設定源語言和目標語言\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# 預處理實體標記的函數\n",
    "def preprocess_entity_aware(text):\n",
    "    doc = nlp(text)\n",
    "    modified_text = text\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        # 在實體周圍加上 < 和 > 符號，並保留其標記\n",
    "        entity_marker = f\"<{ent.label_}:{ent.text}>\"\n",
    "        modified_text = modified_text.replace(ent.text, entity_marker)\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return modified_text, entities\n",
    "\n",
    "# 使用 forward() 方法進行翻譯的實體識別和翻譯函數\n",
    "def entity_aware_translate(text):\n",
    "    # 1. 預處理：對文本進行實體識別和標記\n",
    "    processed_text, entities = preprocess_entity_aware(text)\n",
    "    inputs = tokenizer(processed_text, return_tensors=\"pt\")\n",
    "\n",
    "    # 2. 使用 generate 方法進行翻譯，確保設置了目標語言\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]  # 指定目標語言為中文\n",
    "    )\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # 3. 後處理：將實體替換回原來的翻譯結果中\n",
    "    for ent_text, ent_label in entities:\n",
    "        entity_marker = f\"<{ent_label}:{ent_text}>\"\n",
    "        if entity_marker in translated_text:\n",
    "            translated_text = translated_text.replace(entity_marker, ent_text)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# 測試\n",
    "input_text = \"Microsoft was founded in 1975 by Bill Gates and Paul Allen.\"\n",
    "translated_text = entity_aware_translate(input_text)\n",
    "print(\"Original text:\", input_text)\n",
    "print(\"Translated text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc5c941-3a7d-499b-803f-9b3759eb3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1382, in _get_module\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py\", line 26, in <module>\n",
      "    from ...cache_utils import Cache, DynamicCache, StaticCache\n",
      "ImportError: cannot import name 'StaticCache' from 'transformers.cache_utils' (C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\cache_utils.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_44312\\3803337787.py\", line 1, in <module>\n",
      "    from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
      "  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1373, in __getattr__\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1372, in __getattr__\n",
      "    \"\"\"\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1384, in _get_module\n",
      "    FLAX_IMPORT_ERROR = \"\"\"\n",
      "RuntimeError: Failed to import transformers.models.gpt_neo.modeling_gpt_neo because of the following error (look up to see its traceback):\n",
      "cannot import name 'StaticCache' from 'transformers.cache_utils' (C:\\Users\\USER\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\cache_utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\USER\\AppData\\Roaming\\Python\\Python38\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e85a8a-e506-4f78-9a7b-3e804e441c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
